<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Hibari System Administrator’s Guide DRAFT - IN PROGRESS</title><link rel="stylesheet" href="./docbook-xsl.css" type="text/css" /><meta name="generator" content="DocBook XSL Stylesheets V1.75.2" /></head><body><div xml:lang="en" class="article" title="Hibari System Administrator’s Guide DRAFT - IN PROGRESS" lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="id374247"></a>Hibari System Administrator’s Guide  <span class="strong"><strong>DRAFT - IN PROGRESS</strong></span></h2></div><div><div class="revhistory"><table border="1" width="100%" summary="Revision history"><tr><th align="left" valign="top" colspan="2"><b>Revision History</b></th></tr><tr><td align="left">Revision 0.5.1</td><td align="left">2011/01/07</td></tr></table></div></div></div><hr /></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#_introduction">1. Introduction</a></span></dt><dd><dl><dt><span class="section"><a href="#_the_problem">1.1. The Problem</a></span></dt><dt><span class="section"><a href="#_key_value_store">1.2. Key-Value Store</a></span></dt><dt><span class="section"><a href="#hibari-origins">1.3. Hibari’s Origins</a></span></dt><dt><span class="section"><a href="#_summary_of_hibari_8217_s_main_features">1.4. Summary of Hibari’s Main Features</a></span></dt><dt><span class="section"><a href="#acid-base-hibari">1.5. The "ACID vs. BASE" Spectrum and Hibari</a></span></dt><dt><span class="section"><a href="#cap-theorem-and-hibari">1.6. The CAP Theorem and Hibari</a></span></dt><dt><span class="section"><a href="#_copyright_notices">1.7. Copyright Notices</a></span></dt></dl></dd><dt><span class="section"><a href="#_hibari_8217_s_main_features_in_broad_detail">2. Hibari’s Main Features in Broad Detail</a></span></dt><dd><dl><dt><span class="section"><a href="#_distributed_system">2.1. Distributed system</a></span></dt><dt><span class="section"><a href="#_scalable_system">2.2. Scalable system</a></span></dt><dt><span class="section"><a href="#_durable_updates">2.3. Durable updates</a></span></dt><dt><span class="section"><a href="#_consistent_updates">2.4. Consistent updates</a></span></dt><dt><span class="section"><a href="#lockless-client-api">2.5. Lockless client API</a></span></dt><dt><span class="section"><a href="#_high_availability">2.6. High availability</a></span></dt><dt><span class="section"><a href="#_multiple_client_protocols">2.7. Multiple Client Protocols</a></span></dt><dt><span class="section"><a href="#overview-high-performance">2.8. High performance</a></span></dt><dt><span class="section"><a href="#_automatic_repair">2.9. Automatic repair</a></span></dt><dt><span class="section"><a href="#_dynamic_configuration">2.10. Dynamic configuration</a></span></dt><dt><span class="section"><a href="#_data_rebalancing">2.11. Data rebalancing</a></span></dt><dt><span class="section"><a href="#_heterogeneous_hardware_support">2.12. Heterogeneous hardware support</a></span></dt><dt><span class="section"><a href="#_micro_transactions">2.13. Micro-Transactions</a></span></dt><dt><span class="section"><a href="#per-table-config-perf-options">2.14. Per-table configurable performance options</a></span></dt></dl></dd><dt><span class="section"><a href="#_getting_started_with_hibari_incomplete">3. Getting Started with Hibari (INCOMPLETE)</a></span></dt><dd><dl><dt><span class="section"><a href="#_starting_hibari_for_the_first_time_incomplete">3.1. Starting Hibari for the first time (INCOMPLETE)</a></span></dt><dt><span class="section"><a href="#_an_introductory_example_incomplete">3.2. An Introductory Example (INCOMPLETE)</a></span></dt></dl></dd><dt><span class="section"><a href="#_building_a_hibari_database">4. Building A Hibari Database</a></span></dt><dd><dl><dt><span class="section"><a href="#_defining_a_schema">4.1. Defining a Schema</a></span></dt><dt><span class="section"><a href="#hibari-data-model">4.2. The Hibari Data Model</a></span></dt><dt><span class="section"><a href="#_hibari_8217_s_client_operations">4.3. Hibari’s Client Operations</a></span></dt><dt><span class="section"><a href="#_indexes">4.4. Indexes</a></span></dt><dt><span class="section"><a href="#creating-new-tables">4.5. Creating New Tables</a></span></dt></dl></dd><dt><span class="section"><a href="#hibari-architecture">5. Hibari Architecture</a></span></dt><dd><dl><dt><span class="section"><a href="#_bricks_physical_and_logical">5.1. Bricks, Physical and Logical</a></span></dt><dt><span class="section"><a href="#write-ahead-logs">5.2. Write-Ahead Logs</a></span></dt><dt><span class="section"><a href="#chains">5.3. Chains</a></span></dt><dt><span class="section"><a href="#_tables">5.4. Tables</a></span></dt><dt><span class="section"><a href="#micro-transactions">5.5. Micro-Transactions</a></span></dt><dt><span class="section"><a href="#_distribution_workload_partitioning_and_fault_tolerance">5.6. Distribution: Workload Partitioning and Fault Tolerance</a></span></dt></dl></dd><dt><span class="section"><a href="#admin-server-app">6. The Admin Server Application</a></span></dt><dd><dl><dt><span class="section"><a href="#_admin_server_active_standby_implementation">6.1. Admin Server Active/Standby Implementation</a></span></dt><dt><span class="section"><a href="#bootstrap-bricks">6.2. Admin Server’s Private State: the Bootstrap Bricks</a></span></dt><dt><span class="section"><a href="#_admin_server_crash_and_restart">6.3. Admin Server Crash and Restart</a></span></dt><dt><span class="section"><a href="#admin-server-and-network-partition">6.4. Admin Server and Network Partition</a></span></dt><dt><span class="section"><a href="#_admin_server_network_partition_and_client_access">6.5. Admin Server, Network Partition, and Client Access</a></span></dt></dl></dd><dt><span class="section"><a href="#_hibari_system_information_configuration_files_etc">7. Hibari System Information: Configuration Files, Etc.</a></span></dt><dd><dl><dt><span class="section"><a href="#_literal_central_conf_literal_file_syntax_and_usage">7.1. <code class="literal">central.conf</code> File Syntax and Usage</a></span></dt><dt><span class="section"><a href="#central-conf-parameters">7.2. Parameters in the <code class="literal">central.conf</code> File</a></span></dt><dt><span class="section"><a href="#_admin_server_configuration">7.3. Admin Server Configuration</a></span></dt><dt><span class="section"><a href="#_configuration_not_stored_in_editable_config_files">7.4. Configuration Not Stored in Editable Config Files</a></span></dt></dl></dd><dt><span class="section"><a href="#_the_life_of_a_logical_brick">8. The Life of a (Logical) Brick</a></span></dt><dd><dl><dt><span class="section"><a href="#brick-lifecycle-fsm">8.1. Brick Lifecycle Finite State Machine</a></span></dt><dt><span class="section"><a href="#chain-lifecycle-fsm">8.2. Chain Lifecycle Finite State Machine</a></span></dt><dt><span class="section"><a href="#brick-roles">8.3. Brick “Roles” Within A Chain</a></span></dt><dt><span class="section"><a href="#brick-init">8.4. Brick Initialization</a></span></dt><dt><span class="section"><a href="#chain-repair">8.5. Chain Repair</a></span></dt><dt><span class="section"><a href="#checkpoints">8.6. Brick Checkpoint Operations</a></span></dt><dt><span class="section"><a href="#scavenger">8.7. The Scavenger</a></span></dt></dl></dd><dt><span class="section"><a href="#_dynamic_cluster_reconfiguration">9. Dynamic Cluster Reconfiguration</a></span></dt><dd><dl><dt><span class="section"><a href="#add-table">9.1. Adding a Table</a></span></dt><dt><span class="section"><a href="#remove-table">9.2. Removing a Table</a></span></dt><dt><span class="section"><a href="#chain-length-change">9.3. Changing Chain Length (Changing Replication Factor)</a></span></dt><dt><span class="section"><a href="#chain-migration">9.4. Chain Migration: Rebalancing Data Across Chains</a></span></dt><dt><span class="section"><a href="#adding-removing-client-nodes">9.5. Adding/Removing Client Nodes</a></span></dt></dl></dd><dt><span class="section"><a href="#partition-detector">10. The Partition Detector Application</a></span></dt><dd><dl><dt><span class="section"><a href="#_partition_detector_heartbeats">10.1. Partition Detector Heartbeats</a></span></dt><dt><span class="section"><a href="#_partition_detector_8217_s_tiebreaker">10.2. Partition Detector’s Tiebreaker</a></span></dt></dl></dd><dt><span class="section"><a href="#_backup_and_disaster_recovery">11. Backup and Disaster Recovery</a></span></dt><dd><dl><dt><span class="section"><a href="#_backup_and_recovery_software">11.1. Backup and Recovery Software</a></span></dt><dt><span class="section"><a href="#_disaster_recovery_via_remote_data_centers">11.2. Disaster Recovery via Remote Data Centers</a></span></dt></dl></dd><dt><span class="section"><a href="#_hibari_application_logging">12. Hibari Application Logging</a></span></dt><dd><dl><dt><span class="section"><a href="#_format_of_the_hibari_application_log">12.1. Format of the Hibari Application Log</a></span></dt><dt><span class="section"><a href="#_application_log_example">12.2. Application Log Example</a></span></dt></dl></dd><dt><span class="section"><a href="#_hardware_and_software_considerations">13. Hardware and Software Considerations</a></span></dt><dd><dl><dt><span class="section"><a href="#_notes_on_brick_hardware">13.1. Notes on Brick Hardware</a></span></dt><dt><span class="section"><a href="#_notes_on_networking">13.2. Notes on Networking</a></span></dt><dt><span class="section"><a href="#_notes_on_operating_system">13.3. Notes on Operating System</a></span></dt><dt><span class="section"><a href="#_notes_on_supporting_software">13.4. Notes on Supporting Software</a></span></dt><dt><span class="section"><a href="#_notes_on_hibari_configuration">13.5. Notes on Hibari Configuration</a></span></dt><dt><span class="section"><a href="#_notes_on_monitoring_a_hibari_cluster">13.6. Notes on Monitoring a Hibari Cluster</a></span></dt></dl></dd></dl></div><div class="section" title="1. Introduction"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_introduction"></a>1. Introduction</h2></div></div></div><div class="caution" title="Caution" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Caution"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Caution]" src="./images/icons/caution.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>This document is under re-construction - beware!</p></td></tr></table></div><div class="section" title="1.1. The Problem"><div class="titlepage"><div><div><h3 class="title"><a id="_the_problem"></a>1.1. The Problem</h3></div></div></div><p>There exists a dichotomy in modern storage products. Commodity storage
is inexpensive, but unreliable. Enterprise storage is expensive, but
reliable. Large capacities are present in both enterprise and
commodity class. The problem, then, becomes how to leverage
inexpensive commodity hardware to achieve high capacity enterprise
class reliability at a fraction of the cost.</p><p>This problem space has been researched extensively, especially in the
last few years: in academia, the commercial sector, and by open source
community.  Hibari uses techniques and algorithms from this research
to create a solution which is reliable, cost effective, and scalable.</p></div><div class="section" title="1.2. Key-Value Store"><div class="titlepage"><div><div><h3 class="title"><a id="_key_value_store"></a>1.2. Key-Value Store</h3></div></div></div><p>Hibari is key-value store.  If a key-value store were represented as
an SQL table, it would be defined as:</p><div class="example"><a id="sql-definition-key-value"></a><p class="title"><b>Example 1. SQL-like definition of a generic key value store</b></p><div class="example-contents"><pre class="screen">CREATE TABLE foo (
    BLOB key;
    BLOB value;
) PRIMARY KEY key;</pre></div></div><br class="example-break" /><p>In truth, each key stored in Hibari has three additional fields
associated with it. See <a class="xref" href="#hibari-data-model" title="4.2. The Hibari Data Model">Section 4.2, “The Hibari Data Model”</a> and
<a class="ulink" href="hibari-contributor-guide.en.html" target="_top">Hibari Contributor’s Guide</a> for details.</p></div><div class="section" title="1.3. Hibari’s Origins"><div class="titlepage"><div><div><h3 class="title"><a id="hibari-origins"></a>1.3. Hibari’s Origins</h3></div></div></div><p>Hibari was originally written by Gemini Mobile Technologies to support
mobile messaging and email services.  Hibari was released outside of
Gemini under the Apache Public License version 2.0 in July 2010.</p><p>Hibari has been deployed by multiple telecom carriers in Asia and
Europe.  Hibari may lack some features such as monitoring, event and
alarm management, and other "production environment" support services.
Since telecom operator has its own data center support infrastructure,
Hibari’s development has not included many services that would be
redundant in a carrier environment.  We hope that Hibari’s release to
the open source community will close those functional gaps as Hibari
spreads outside of carrier data centers.</p><p>Gemini Mobile Technologies provides full support, consulting, and
development services for Hibari.  Please see the
<a class="ulink" href="http://www.geminimobile.com/" target="_top">Gemini Mobile Technologies Web page</a> for
more information.</p></div><div class="section" title="1.4. Summary of Hibari’s Main Features"><div class="titlepage"><div><div><h3 class="title"><a id="_summary_of_hibari_8217_s_main_features"></a>1.4. Summary of Hibari’s Main Features</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
A Hibari cluster is a distributed system.
</li><li class="listitem">
A Hibari cluster is linearly scalable.
</li><li class="listitem">
A Hibari cluster is highly available.
</li><li class="listitem">
All updates are durable.
</li><li class="listitem">
All updates are strongly consistent.
</li><li class="listitem">
All client operations are lockless.
</li><li class="listitem">
A Hibari cluster’s performance is excellent.
</li><li class="listitem">
Multiple client access protocols are available.
</li><li class="listitem">
Data is repaired automatically after a server failure.
</li><li class="listitem">
Cluster configuration can be changed at any time.
</li><li class="listitem">
Data is automatically rebalanced.
</li><li class="listitem">
Heterogeneous hardware support is easy.
</li><li class="listitem">
Micro-transactions simplify creation of robust client applications.
</li><li class="listitem">
Per-table configurable performance options are available.
</li></ul></div></div><div class="section" title="1.5. The &quot;ACID vs. BASE&quot; Spectrum and Hibari"><div class="titlepage"><div><div><h3 class="title"><a id="acid-base-hibari"></a>1.5. The "ACID vs. BASE" Spectrum and Hibari</h3></div></div></div><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>We strongly believe that "ACID" and "BASE" properties exist
on a spectrum and are not exclusively one or the other
(black-or-white) properties.</p></td></tr></table></div><p>Most database users and administrators are familiar with the acronym
ACID: Atomic, Consistent, Independent, and Durable.  Now, consider an
alternative method of storing and managing data, BASE:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Basically available
</li><li class="listitem">
Soft state
</li><li class="listitem">
Eventually consistent
</li></ul></div><p>For an
<a class="ulink" href="http://queue.acm.org/detail.cfm?id=1394128" target="_top">exploration of ACID and BASE properties (at ACM Queue)</a>, see:</p><pre class="literallayout">BASE: An Acid Alternative
Dan Pritchett
ACM Queue, volume 6, number 3 (May/June 2008)
ISSN: 1542-7730
http://queue.acm.org/detail.cfm?id=1394128</pre><p>When both strict ACID and strict BASE properties are placed on a
spectrum, they are at the opposite ends.  However, a distributed
database system can fit anywhere in the middle of the spectrum.</p><p>A Hibari cluster lies near the ACID end of the ACID/BASE spectrum.  In
general, Hibari’s design will always favors consistency and durability
of updates at the expense of 100% availability in all situations.</p></div><div class="section" title="1.6. The CAP Theorem and Hibari"><div class="titlepage"><div><div><h3 class="title"><a id="cap-theorem-and-hibari"></a>1.6. The CAP Theorem and Hibari</h3></div></div></div><div class="warning" title="Warning" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="./images/icons/warning.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>Eric Brewer’s "CAP Theorem", and its proof by Gilbert and
Lynch, is a tricky thing.  It’s nearly impossible to cleanly apply the
purity of logic to the dirty world of real, industrial computing
systems.  We strongly suggest that the reader consider the CAP
properties as a spectrum, one of balances and trade-offs.  The
distributed database world is not black and white, and it is important
to know where the gray areas are.</p></td></tr></table></div><p>See the
<a class="ulink" href="http://en.wikipedia.org/wiki/CAP_theorem" target="_top">Wikipedia article about the CAP theorem</a>
for a summary of the theorem, its proof, and related links.</p><pre class="literallayout">CAP Theorem (postulated by Eric Brewer, Inktomi, 2000)
Wikipedia
http://en.wikipedia.org/wiki/CAP_theorem</pre><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>The CAP Theorem’s properties of the "Availability" and
"Partition tolerance" are very tightly related.</p></td></tr></table></div><p>The line between "CA" and "CP" systems is very fine and nuanced.  From
the viewpoint of operations/production staff, a Hibari cluster will
behave more like "CA" database systems, such as clustered RDBMS
systems, and less like "CP" systems such as HBase or Hypertable.  Most
distributed databases in the "CA" or "CP" families are mixture of
both.<sup>[<a id="id375188" href="#ftn.id375188" class="footnote">1</a>]</sup></p><p>Hibari includes an Erlang/OTP application specifically for detecting
network partitions.  See <a class="xref" href="#admin-server-and-network-partition" title="6.4. Admin Server and Network Partition">Section 6.4, “Admin Server and Network Partition”</a> for
details.</p></div><div class="section" title="1.7. Copyright Notices"><div class="titlepage"><div><div><h3 class="title"><a id="_copyright_notices"></a>1.7. Copyright Notices</h3></div></div></div><p>Copyright © 2005-2011 Gemini Mobile Technologies, Inc.  All rights
reserved.</p><p>Gemini Mobile® and HyperScale® are registered trademarks of Gemini
Mobile Technologies, Inc. in the United States and other countries.</p><p>Portions of Gemini products include third-party technology used under
license.</p><pre class="screen">Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</pre></div></div><div class="section" title="2. Hibari’s Main Features in Broad Detail"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_hibari_8217_s_main_features_in_broad_detail"></a>2. Hibari’s Main Features in Broad Detail</h2></div></div></div><div class="section" title="2.1. Distributed system"><div class="titlepage"><div><div><h3 class="title"><a id="_distributed_system"></a>2.1. Distributed system</h3></div></div></div><p>Multiple machines can participate in a single cluster.  The maximum
size of a Hibari cluster has not yet been determined.  A practical
limit of approximately 200-250 nodes is likely.</p><p>Any server node can handle any client request, forwarding a request to
the correct server node when necessary.  Clients maintain enough state
to send their queries directly to the correct server node in all
common cases.</p></div><div class="section" title="2.2. Scalable system"><div class="titlepage"><div><div><h3 class="title"><a id="_scalable_system"></a>2.2. Scalable system</h3></div></div></div><p>The total storage and processing capacity of a Hibari cluster
increases linearly as machines are added to the cluster.</p></div><div class="section" title="2.3. Durable updates"><div class="titlepage"><div><div><h3 class="title"><a id="_durable_updates"></a>2.3. Durable updates</h3></div></div></div><p>Every key update is written and flushed to stable storage (via the
<code class="literal">fsync()</code> system call) before sending acknowledgments to the client.</p></div><div class="section" title="2.4. Consistent updates"><div class="titlepage"><div><div><h3 class="title"><a id="_consistent_updates"></a>2.4. Consistent updates</h3></div></div></div><p>After a key’s update is acknowledged, no client in the cluster can see
an older version of that key.  Hibari uses the "chain replication"
algorithm to maintain consistency across all replicas of a key.</p><p>All data written to disk include MD5 checksums; the checksums are
validated on each read to avoid sending corrupted data to the client.</p></div><div class="section" title="2.5. Lockless client API"><div class="titlepage"><div><div><h3 class="title"><a id="lockless-client-api"></a>2.5. Lockless client API</h3></div></div></div><p>The Hibari client API requires that all operations (read queries
operations and/or update operations) be self-contained within a single
client request.  Therefore, locks are not implemented because they are
not required.</p><p>Inside Hibari, each key-value pair also contains a “timestamp”
value.  A timestamp is an integer.  Each time the key is updated, the
timestamp value must increase.  (This requirement is enforced by all
server nodes.)</p><p>In many database systems, if a client requires guarantees that a key has
not changed since the last time it was read, then the client acquires
a lock (or lease) on the key.  In Hibari, the client’s update
specifies the timestamp of the last read attempt of the key:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
If the timestamp matches the server, the operation is permitted.
</li><li class="listitem">
If the timestamp does not match the server’s timestamp, then the
  operation is not permitted, and the new timestamp is returned to the
  client.
</li></ul></div><p>It is recommended that all Hibari nodes use NTP to synchronize their
system clocks.  The simplest Hibari client API uses timestamps based
upon the OS system clock for timestamp values.  This feature can be
bypassed, however, by using a slightly more complex client API.</p><p>However, Hibari’s overload detection and work-dumping algorithms will
use the OS system clock, regardless of which client API is used.  All
system clocks, client and server, be synchronized to be within roughly
1 second of each other.</p></div><div class="section" title="2.6. High availability"><div class="titlepage"><div><div><h3 class="title"><a id="_high_availability"></a>2.6. High availability</h3></div></div></div><p>Each key can be replicated multiple times (configurable on a per-table
basis).  As long as one copy of the key survives, all operations on
that key are permitted.  A cluster can survive multiple cluster node
failures and still maintain full data integrity.</p><p>The cluster membership application, called the Hibari Admin Server,
runs as an active/standby application on one or more of the server
nodes.  The Admin Server’s configuration and private state are also
maintained in Hibari server nodes.  Shared storage such as NFS, shared
SCSI/Fibre Channel LUNs, or replicated block devices are not required.</p><p>If the Admin Server fails and is restarted on a standby node, the rest
of the cluster can continue normal operation.  If another brick fails
while the Admin Server is restarting, then clients may see service
interruptions (usually in the form of timeouts) until the Admin Server
has finished restarting and can react to the failure.</p></div><div class="section" title="2.7. Multiple Client Protocols"><div class="titlepage"><div><div><h3 class="title"><a id="_multiple_client_protocols"></a>2.7. Multiple Client Protocols</h3></div></div></div><p>Hibari supports many client protocols for queries and updates:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
A native Erlang API, via Erlang’s native message-passing mechanism
</li><li class="listitem">
Amazon S3 protocol, via HTTP
</li><li class="listitem">
UBF, Joe Armstrong’s “Universal Binary Format” protocol, via TCP
</li><li class="listitem">
UBF via several minor variations of TCP transport
</li><li class="listitem">
UBF over JSON-RPC, via HTTP
</li><li class="listitem">
JSON-encoded UBF, via TCP
</li></ul></div><p>Protocols under development:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Memcached, via TCP
</li><li class="listitem">
UBF over Thrift, via TCP
</li><li class="listitem">
UBF over Protocol Buffers, via TCP
</li></ul></div><p>Most of the client access protocols are implemented using the
Erlang/OTP <code class="literal">application</code> behavior.  By separating each access protocol
into separate OTP applications, Hibari’s packaging is quite flexible:
packaging can add or remove protocol support as desired.  Similarly,
protocols can be stopped and started at runtime.</p></div><div class="section" title="2.8. High performance"><div class="titlepage"><div><div><h3 class="title"><a id="overview-high-performance"></a>2.8. High performance</h3></div></div></div><p>Hibari’s performance is competitive with other distributed,
non-relational databases such as HBase and Cassandra, when used with
similar replication and durability configurations.  Despite the
constraints of durable writes and strong consistency, Hibari’s
performance can exceed those databases on some workloads.</p><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>The metadata of all keys stored by the brick, called the
“key catalog”, are stored in RAM to accelerate commonly-used
operations.  In addition, non-zero values of the "expiration_time" and
non-empty values of "flags" are also stored in RAM (see
<a class="xref" href="#">???</a>).  As a consequence, a multi-million key
brick can require many gigabytes of RAM.</p></td></tr></table></div></div><div class="section" title="2.9. Automatic repair"><div class="titlepage"><div><div><h3 class="title"><a id="_automatic_repair"></a>2.9. Automatic repair</h3></div></div></div><p>Replicas of keys are automatically repaired whenever a cluster node
crashes and restarts.</p></div><div class="section" title="2.10. Dynamic configuration"><div class="titlepage"><div><div><h3 class="title"><a id="_dynamic_configuration"></a>2.10. Dynamic configuration</h3></div></div></div><p>The number of replicas per key can be changed without service
interruption.  Likewise, replication chains can be added or removed
from the cluster without service interruption.  This permits the
cluster to grow (or shrink) as workloads change.  See
<a class="xref" href="#chain-migration" title="9.4. Chain Migration: Rebalancing Data Across Chains">Section 9.4, “Chain Migration: Rebalancing Data Across Chains”</a> for more details.</p></div><div class="section" title="2.11. Data rebalancing"><div class="titlepage"><div><div><h3 class="title"><a id="_data_rebalancing"></a>2.11. Data rebalancing</h3></div></div></div><p>Keys will be automatically be rebalanced across the cluster without
service interruption.  See <a class="xref" href="#chain-migration" title="9.4. Chain Migration: Rebalancing Data Across Chains">Section 9.4, “Chain Migration: Rebalancing Data Across Chains”</a> for more details.</p></div><div class="section" title="2.12. Heterogeneous hardware support"><div class="titlepage"><div><div><h3 class="title"><a id="_heterogeneous_hardware_support"></a>2.12. Heterogeneous hardware support</h3></div></div></div><p>Each replication chain can be assigned a weighting factor that will
increase or decrease the percentage of a table’s key space relative to
all other chains.  This feature can permit use of cluster nodes with
different CPU, RAM, and/or disk capacities.</p></div><div class="section" title="2.13. Micro-Transactions"><div class="titlepage"><div><div><h3 class="title"><a id="_micro_transactions"></a>2.13. Micro-Transactions</h3></div></div></div><p>Under limited circumstances, operations on multiple keys can be given
transactional commit/abort semantics.  Such micro-transactions can
considerably simplify the creation of robust applications that keep
data consistent despite failures by both clients and servers.</p></div><div class="section" title="2.14. Per-table configurable performance options"><div class="titlepage"><div><div><h3 class="title"><a id="per-table-config-perf-options"></a>2.14. Per-table configurable performance options</h3></div></div></div><p>Each Hibari table may be configured with the following options to
enhance performance … though each of these options has a
corresponding price to pay.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
RAM-based storage: All data (both keys and values) may be stored in
  RAM, at the expense of increased RAM consumption.
  Disk is used still used to log all updates, to protect against
  a catastrophic power failure.
</li><li class="listitem">
Asynchronous writes: Use of the <code class="literal">fsync()</code> system call can be disabled
  to improve performance, at the expense of data loss in a system
  crash or power failure.
</li><li class="listitem">
Non-durable updates: All update logging can be disabled to improve
  performance, at the expense of data loss when all nodes in a
  replication chain crash.
</li></ul></div></div></div><div class="section" title="3. Getting Started with Hibari (INCOMPLETE)"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_getting_started_with_hibari_incomplete"></a>3. Getting Started with Hibari (INCOMPLETE)</h2></div></div></div><p><span class="emphasis"><em>draft</em></span></p><div class="note" title="Note" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="./images/icons/note.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>Don’t forget to mention the recommendation of 2 physical network
interfaces.</p></td></tr></table></div><div class="section" title="3.1. Starting Hibari for the first time (INCOMPLETE)"><div class="titlepage"><div><div><h3 class="title"><a id="_starting_hibari_for_the_first_time_incomplete"></a>3.1. Starting Hibari for the first time (INCOMPLETE)</h3></div></div></div><p><span class="emphasis"><em>draft</em></span></p></div><div class="section" title="3.2. An Introductory Example (INCOMPLETE)"><div class="titlepage"><div><div><h3 class="title"><a id="_an_introductory_example_incomplete"></a>3.2. An Introductory Example (INCOMPLETE)</h3></div></div></div><p><span class="emphasis"><em>draft</em></span></p></div></div><div class="section" title="4. Building A Hibari Database"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_building_a_hibari_database"></a>4. Building A Hibari Database</h2></div></div></div><div class="section" title="4.1. Defining a Schema"><div class="titlepage"><div><div><h3 class="title"><a id="_defining_a_schema"></a>4.1. Defining a Schema</h3></div></div></div><p>Hibari is a key-value database.  Unlike a relational DBMS, Hibari
applications do not need to create a schema.  The only application
requirement is that all its tables be created in advance, see
<a class="xref" href="#creating-new-tables" title="4.5. Creating New Tables">Section 4.5, “Creating New Tables”</a> below.</p></div><div class="section" title="4.2. The Hibari Data Model"><div class="titlepage"><div><div><h3 class="title"><a id="hibari-data-model"></a>4.2. The Hibari Data Model</h3></div></div></div><p>If a Hibari table were represented within an SQL database, it would
look something like this:</p><pre class="screen">CREATE TABLE foo (
    BLOB key;
    BLOB value;
    INTEGER timestamp;                  -- Monotonically increasing
    INTEGER expiration_time;            -- Usually zero
    LIST OF ATOMS_AND_TWO_TUPLES flags; -- Metadata stored in RAM for speed
) PRIMARY KEY key;</pre><p>Hibari table names use the Erlang data type “atom”.  The types of
all key-related attributes are presented below.</p><div class="informaltable"><table cellpadding="4px" style="border-collapse: collapse;border-top: 2px solid #527bbd; border-bottom: 2px solid #527bbd; border-left: 2px solid #527bbd; border-right: 2px solid #527bbd; "><colgroup><col /><col /><col /><col /></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"> Attribute Name </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"> Erlang data type </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"> Storage Location </th><th style="border-bottom: 1px solid ; " align="left" valign="top"> Description</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>Key</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>binary</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>RAM</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>A binary blob of any size, though due to RAM storage the key should be small enough for all keys to fit in RAM.</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>Value</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>binary</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>RAM or disk</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>A binary blob of any size, though practical constraints limit value blobs to 16MB or so.</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>Timestamp</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>integer</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>RAM</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>A monotonically increasing counter, usually (but not always) based on the client’s wall-clock time.  Updating a key with a timestamp smaller than the key’s current timestamp is not permitted.</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>Expiration Time</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>integer</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>RAM</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>A UNIX <code class="literal">time_t</code> representing the expiration time for a key.  A value of 0 means no expiration, and no RAM overhead is required.</p></td></tr><tr><td style="border-right: 1px solid ; " align="center" valign="top"><p>Flags</p></td><td style="border-right: 1px solid ; " align="center" valign="top"><p>list</p></td><td style="border-right: 1px solid ; " align="center" valign="top"><p>RAM</p></td><td style="" align="left" valign="top"><p>This attribute cannot be represented in plain SQL.  It is a list of atoms and/or {atom(), term()} pairs.  Heavy use of this attribute is discouraged due to RAM-based storage.</p></td></tr></tbody></table></div><p>"Storage location = RAM" means that, during normal query handling,
data is retrieved from a copy in RAM.  All modifications of any/all
attributes of a key are written to the write-ahead log to prevent data
loss in case of a cluster-wide power failure.  See
<a class="xref" href="#write-ahead-logs" title="5.2. Write-Ahead Logs">Section 5.2, “Write-Ahead Logs”</a> for more details.</p><p>"Store location = disk" means that the value of the attribute is not
stored in RAM.  Metadata in RAM contains a pointer to the attribute’s
location:file #, byte offset, and length.  A log sequence file inside
the common log must be opened, call <code class="literal">lseek(2)</code>, and then <code class="literal">read(2)</code> to
retrieve the attribute.</p><div class="variablelist"><dl><dt><span class="term">
Best case
</span></dt><dd>
Zero disk seeks are required to read a key’s value
blob from disk, because all data in question is in the OS’s page
cache.
</dd><dt><span class="term">
Typical case
</span></dt><dd>
One seek and read is required: the file’s inode
info is cached, but the desired file page(s) is not cached.
</dd><dt><span class="term">
Worse case
</span></dt><dd>
The file system will need to perform additional seeks and
reads to read intermediate directory data, inode, and indirect storage
block data within the inode.
</dd></dl></div><p>The practical constraints on maximum value blob size are affected by
total blob size and frequency of large blob access.  For example,
storing an occasional 64MB value blob is different than a 100% write
workload of 100% 64MB value blobs.  The Hibari client API does not
have a method to update or fetch less than the entire value blob, so a
brick can be blocked for many seconds if it tried to operate on (for
example) even a single 4GB blob.  In addition, other processes can be
blocked by <code class="literal">'busy_dist_port'</code> events while processing big value blobs.</p></div><div class="section" title="4.3. Hibari’s Client Operations"><div class="titlepage"><div><div><h3 class="title"><a id="_hibari_8217_s_client_operations"></a>4.3. Hibari’s Client Operations</h3></div></div></div><p>Hibari’s basic client operations are enumerated below.</p><div class="variablelist"><dl><dt><span class="term">
<code class="literal">add</code>
</span></dt><dd>
Set a key/value/expiration/flags only if the key does not already exist.
</dd><dt><span class="term">
<code class="literal">delete</code>
</span></dt><dd>
Delete a key
</dd><dt><span class="term">
<code class="literal">get</code>
</span></dt><dd>
Get a key’s timestamp and value
</dd><dt><span class="term">
<code class="literal">get_many</code>
</span></dt><dd>
Get a range of keys
</dd><dt><span class="term">
<code class="literal">replace</code>
</span></dt><dd>
Set a key/value/expiration/flags only if the key does exist
</dd><dt><span class="term">
<code class="literal">set</code>
</span></dt><dd>
Set a key/value/expiration/flags
</dd><dt><span class="term">
<code class="literal">txn</code>
</span></dt><dd>
Start of a micro-transaction
</dd></dl></div><p>Each operation can be accompanied by operation-specific flags.  Some
of these flags include:</p><div class="variablelist"><dl><dt><span class="term">
<code class="literal">witness</code>
</span></dt><dd>
Do not return the value blob. (<code class="literal">get</code>, <code class="literal">get_many</code>)
</dd><dt><span class="term">
<code class="literal">must_exist</code>
</span></dt><dd>
Abort micro-transaction if key does not exist.
</dd><dt><span class="term">
<code class="literal">must_not_exist</code>
</span></dt><dd>
Abort micro-transaction if key does exist.
</dd><dt><span class="term">
<code class="literal">{testset, TS}</code>
</span></dt><dd>
Perform the action only if the key’s current timestamp
exactly matches <code class="literal">TS</code>. (<code class="literal">delete</code>, <code class="literal">replace</code>, <code class="literal">set</code>, micro-transaction)
</dd></dl></div><p>For details of these operations and lesser-used per-operation flags,
see:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="xref" href="#micro-transactions" title="5.5. Micro-Transactions">Section 5.5, “Micro-Transactions”</a>
</li><li class="listitem">
<a class="ulink" href="hibari-contributor-guide.en.html" target="_top">Hibari Contributor’s Guide</a>
</li></ul></div></div><div class="section" title="4.4. Indexes"><div class="titlepage"><div><div><h3 class="title"><a id="_indexes"></a>4.4. Indexes</h3></div></div></div><p>Hibari does not support automatic indexing of value blobs.  If an
application requires indexing, the application must build and maintain
those indexes.</p></div><div class="section" title="4.5. Creating New Tables"><div class="titlepage"><div><div><h3 class="title"><a id="creating-new-tables"></a>4.5. Creating New Tables</h3></div></div></div><p>New tables can be created by two different methods:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Via the Admin Server’s status server.  Follow the "Add a table" link
  at the bottom.
</li><li class="listitem">
Using the Erlang shell.
</li></ul></div><p>For details on the Erlang shell API and detailed explanations of the
table options presented in the Admin server’s HTTP interface, see the
<a class="ulink" href="hibari-contributor-guide.en.html" target="_top">Hibari Contributor’s Guide</a></p></div></div><div class="section" title="5. Hibari Architecture"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="hibari-architecture"></a>5. Hibari Architecture</h2></div></div></div><p>From a logical point of view, Hibari’s architecture has three layers:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Top layer: consistent hashing
</li><li class="listitem">
Middle layer: chain replication
</li><li class="listitem">
Bottom layer: the storage brick
</li></ul></div><p>This section discusses each of these major layers in detail, starting
from the bottom and working upward.</p><div class="figure"><a id="id422150"></a><p class="title"><b>Figure 1. Logical architecture diagram; physical hosts/bricks are color-coded with 5 colors</b></p><div class="figure-contents"><a class="ulink" href="images/logical-architecture1.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="80%"><tr><td align="center"><img src="images/logical-architecture1.png" align="middle" width="100%" alt="images/logical-architecture1.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><div class="figure"><a id="id422184"></a><p class="title"><b>Figure 2. Logical architecture diagram, alternative perspective</b></p><div class="figure-contents"><a class="ulink" href="images/logical-architecture-alt.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="80%"><tr><td align="center"><img src="images/logical-architecture-alt.png" align="middle" width="100%" alt="images/logical-architecture-alt.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><div class="section" title="5.1. Bricks, Physical and Logical"><div class="titlepage"><div><div><h3 class="title"><a id="_bricks_physical_and_logical"></a>5.1. Bricks, Physical and Logical</h3></div></div></div><p>The word "brick" has two different meanings in a Hibari system:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
An entire physical machine that has Hibari software installed,
configured, and (hopefully) running.
</li><li class="listitem">
A logical software entity that runs inside the Hibari application
that is responsible for managing key-value pairs.
</li></ul></div><div class="section" title="5.1.1. The physical brick"><div class="titlepage"><div><div><h4 class="title"><a id="the-physical-brick"></a>5.1.1. The physical brick</h4></div></div></div><p>The phrase "physical brick" and "machine" are interchangeable, most of
the time.  Hibari is designed to react correctly to the failure of any
part of the machine that the Hibari application is running:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
disk
</li><li class="listitem">
power supply
</li><li class="listitem">
CPU
</li><li class="listitem">
network
</li></ul></div><p>Hibari is designed to take advantage of low-cost, off-the-self
commodity servers.</p><p>A physical brick is the basic unit of failure.  Data replication (via
the chain replication algorithm) is responsible for protecting data,
not redundant equipment such as dual power supplies and RAID disk
subsystems.  If a physical brick crashes for any reason, copies of
data on other physical bricks can still be used.</p><p>It is certainly possible to decrease the chances of data loss by using
physical bricks with more expensive equipment.  Given the same number
of copies of a key-value pair, the chances of data loss are less if
each brick has multiple power supplies and RAID 1/5/6/10 disk.  But
risk of data loss can also be reduced by increasing the number of data
replicas ("chain length") using cheaper, non-redundant server
hardware.</p></div><div class="section" title="5.1.2. The logical brick"><div class="titlepage"><div><div><h4 class="title"><a id="_the_logical_brick"></a>5.1.2. The logical brick</h4></div></div></div><p>A logical brick is a software entity that runs within a Hibari
application instance on a physical brick.  A single Hibari physical
brick can support dozens or (potentially) hundreds of logical bricks,
though limitations of CPU, RAM, and/or disk capacity can impose a
smaller limit.</p><p>A logical brick maintains RAM and disk data structures to store a
collection of key-value pairs.  The keys are maintained in
lexicographic sorting order.</p><p>The replication technique used by Hibari, chain replication, maintains
identical copies of key-value pairs across multiple logical bricks.
The number of copies of a key-value pair is exactly equal to the
length of the chain.  See the next subsection below for more details.</p><p>It is possible to configure Hibari to place all of the logical bricks
for the same chain onto the same physical brick.  This practice can be
useful in a developer’s environment, but it is impractical for
production networks: such a configuration does not have any physical
redundancy, and therefore it poses a greater risk of data loss.</p></div></div><div class="section" title="5.2. Write-Ahead Logs"><div class="titlepage"><div><div><h3 class="title"><a id="write-ahead-logs"></a>5.2. Write-Ahead Logs</h3></div></div></div><p>By default, all logical bricks will record all updates to a
“write-ahead log”.  Used by many database systems, a write-ahead log
(WAL) appears to be an infinitely-sized log where all important events
(e.g. all write and delete operations) are appended to the end of the
log.  The log is considered “write-ahead” if a log entry is written
<span class="emphasis"><em>prior to</em></span> any significant processing by the application.</p><div class="section" title="5.2.1. Write-ahead logs in the Hibari application"><div class="titlepage"><div><div><h4 class="title"><a id="write-ahead-logs-in-hibari"></a>5.2.1. Write-ahead logs in the Hibari application</h4></div></div></div><p>Two types of write-ahead logs are used by the Hibari application.
These logs cooperate with each other to provide several benefits to
the logical brick.</p><p>There are two types of write-ahead logs:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The shared "common log".  This single write-ahead log instance provides
  durability guarantees to all logical bricks within the server node
  via the <code class="literal">fsync()</code> system call.
</li><li class="listitem">
Individual “private logs”.  Each logical brick maintains its own private
  write-ahead log instance.  All metadata regarding keys in the
  logical brick are stored in the logical brick’s private log.
</li></ul></div><p>All updates are written first to the common log, usually in a
synchronous manner.  At a later time, update metadata is lazily copied
from the common log to the corresponding brick’s private log.
Value blobs (for bricks that store value blobs on disk)
will remain in the common log and are managed by
the “scavenger”, see <a class="xref" href="#scavenger" title="8.7. The Scavenger">Section 8.7, “The Scavenger”</a>.</p><div class="informalfigure"><a class="ulink" href="images/private-and-common-logs.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="80%"><tr><td align="center"><img src="images/private-and-common-logs.png" align="middle" width="100%" alt="images/private-and-common-logs.svg" /></td></tr></table></div>
</a></div></div><div class="section" title="5.2.2. Two types of write-ahead logs"><div class="titlepage"><div><div><h4 class="title"><a id="two-wal-types"></a>5.2.2. Two types of write-ahead logs</h4></div></div></div><p>The two log types cooperate to support a number of useful properties.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Data durability in case of system crash or power failure.  All
  synchronous writes to the “common log” are guaranteed to be
  flushed to stable storage.
</li><li class="listitem">
Performance enhancement by limiting <code class="literal">fsync()</code> usage.  After a
  logical brick writes data to the common log, it will request an
  <code class="literal">fsync()</code>.  The common log will combine <code class="literal">fsync()</code> requests from
  multiple bricks into a single system call.
</li><li class="listitem">
Performance enhancement at logical brick startup.  A brick’s private
  log stores only that bricks key metadata.  Therefore, at startup
  time, the logical brick does not scan data maintained by other
  logical bricks.  This can be a very substantial time savings as the
  amount of metadata managed by all logical bricks grows over time.
</li><li class="listitem">
Performance enhancement by separating synchronous writes from
  asynchronous writes.  If the common log’s storage is on a separate
  device, e.g. a write-optimized flash memory block device, then all
  of the <code class="literal">fsync()</code> calls can finish much faster.  During later
  processing of the asynchronous/lazy copying of key metadata from the
  common log to individual private logs can take advantage of OS dirty
  page write coalescing and other I/O optimizations without
  interference by <code class="literal">fsync()</code>.  These copies are performed roughly once
  per second.
</li></ul></div></div><div class="section" title="5.2.3. Directories and files used by write-ahead logs"><div class="titlepage"><div><div><h4 class="title"><a id="wal-dirs-and-files"></a>5.2.3. Directories and files used by write-ahead logs</h4></div></div></div><p>Each write-ahead log is stored on disk as a collection of large files
(default = 100MB each).  Each file in the log is identified by a “log
sequence number” and is called a “log sequence file”.</p><p>Log sequence files are append-only and are never written again.
Consequently, data in a log sequence file is never overwritten.  Any
disk space reclaimed by checkpoint and scavenger operations is done by
copying data from old log sequence files and appending to new log
sequence files.  Once the new log sequence file(s) is flushed to
stable storage, the old log sequence file(s) can be deleted.</p><p>When a log sequence file reaches its maximum size, the current log
file is closed and a new one is opened with a monotonically increasing
log serial number.</p><p>All log files for a write-ahead log are grouped under a single
the brick or of the common log.  These directories are stored under
the <code class="literal">var/data</code> subdirectory of the application’s installation path,
<code class="literal">/usr/local/TODO/TODO/var/data</code> (by default).</p><p>The maximum log file size (<code class="literal">brick_max_log_size_mb</code> in the
<code class="literal">central.conf</code> file) is advisory only and is not enforced as a hard
limit.</p></div><div class="section" title="5.2.4. Reclaiming disk space used by write-ahead logs"><div class="titlepage"><div><div><h4 class="title"><a id="_reclaiming_disk_space_used_by_write_ahead_logs"></a>5.2.4. Reclaiming disk space used by write-ahead logs</h4></div></div></div><p>In practice, infinite storage is not yet available.  The Hibari system
uses two mechanisms to reclaim unused disk space:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The “checkpoint” mechanism, see <a class="xref" href="#checkpoints" title="8.6. Brick Checkpoint Operations">Section 8.6, “Brick Checkpoint Operations”</a>.
</li><li class="listitem">
The “scavenger” mechanism, see <a class="xref" href="#scavenger" title="8.7. The Scavenger">Section 8.7, “The Scavenger”</a>.
</li></ul></div></div><div class="section" title="5.2.5. Write-ahead log serial numbers"><div class="titlepage"><div><div><h4 class="title"><a id="_write_ahead_log_serial_numbers"></a>5.2.5. Write-ahead log serial numbers</h4></div></div></div><p>Each item written in a write-ahead log is assigned a serial number.
If the brick is in “standalone” or “head” roles, then the serial
number will be assigned by that brick.  For downstream bricks, the
serial number assigned by the “head” brick will be used.</p><p>The serial number mechanism is used to ensure that a single unique
ordering of log items will be written to each brick log.  In certain
failure cases, log items may be re-sent down the chain a second time,
see <a class="xref" href="#failure-middle-brick" title="9.3.3.3. Failure of a middle brick">Section 9.3.3.3, “Failure of a middle brick”</a>.</p></div></div><div class="section" title="5.3. Chains"><div class="titlepage"><div><div><h3 class="title"><a id="chains"></a>5.3. Chains</h3></div></div></div><p>A chain is the unit of data replication used by the
<a class="ulink" href="http://www.usenix.org/events/osdi04/tech/renesse.html" target="_top">“chain replication” technique as described in this paper</a>:</p><pre class="literallayout">Chain Replication for Supporting High Throughput and Availability
Robbert van Renesse and Fred B. Schneider
USENIX OSDI 2004 conference proceedings
http://www.usenix.org/events/osdi04/tech/renesse.html</pre><p>Data replication algorithms can be separated into two basic families:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
State machine replication
</li><li class="listitem">
Quorum replication
</li></ul></div><p>The chain replication algorithm is from the state machine family of
replication algorithms.  It is a variation of the familiar
“master/slave” replication algorithm, where all updates are sent to
a master node and then copies are sent to zero or more slave nodes.</p><p>Chain replication requires a very specific ordering of nodes (which
store copies of data) and the messages passed between them.  The
diagram below depicts the "key update" message flow in a chain of
length three.</p><div class="figure"><a id="diagram-write-path-3"></a><p class="title"><b>Figure 3. Message flow in a chain for a key update</b></p><div class="figure-contents"><a class="ulink" href="images/write-path-3.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="80%"><tr><td align="center"><img src="images/write-path-3.png" align="middle" width="100%" alt="images/write-path-3.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><p>If a chain is of length one, then the same brick assumes both “head”
and “tail” roles simultaneously.  In this case, the brick is called
a “standalone” brick.</p><div class="figure"><a id="id422772"></a><p class="title"><b>Figure 4. Message flow for a key update to a chain of length 1</b></p><div class="figure-contents"><a class="ulink" href="images/write-path-1.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="30%"><tr><td align="center"><img src="images/write-path-1.png" align="middle" width="100%" alt="images/write-path-1.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><p>To maintain the property strong consistency, a client must read data
from the tail brick in the chain.  A read processed by any other brick
member would permit the client to read an update that has not yet been
processed by all bricks and therefore could result in a strong
consistency violation.  Such a violation is frequently called a
“dirty read” in other database systems.</p><div class="figure"><a id="id422815"></a><p class="title"><b>Figure 5. Message flow for a read-only key query</b></p><div class="figure-contents"><a class="ulink" href="images/read-path-3.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="80%"><tr><td align="center"><img src="images/read-path-3.png" align="middle" width="100%" alt="images/read-path-3.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><div class="section" title="5.3.1. Bricks outside of chain replication"><div class="titlepage"><div><div><h4 class="title"><a id="bricks-outside-chain-replication"></a>5.3.1. Bricks outside of chain replication</h4></div></div></div><p>During Hibari’s development, we encountered a problem of managing the
state required by the Admin Server.  If data managed by chain
replication requires the Admin Server to be running, how can the Admin
Server read its own data?  There is a “chicken and the egg”
dependency problem that must be solved.</p><p>The solution is simple: do not use chain replication to manage the
Admin Server’s data.  Instead, that data is replicated using a simple
“quorum replication” technique.  These bricks all have names starting
with the string "bootstrap".</p><p>A brick must be in “standalone” mode to answer queries when it is
used outside of chain replication.  See <a class="xref" href="#brick-roles" title="8.3. Brick “Roles” Within A Chain">Section 8.3, “Brick “Roles” Within A Chain”</a> for details
on the standalone role.</p></div></div><div class="section" title="5.4. Tables"><div class="titlepage"><div><div><h3 class="title"><a id="_tables"></a>5.4. Tables</h3></div></div></div><p>A table is thing that divides the key namespace within Hibari.  If you
need to have two different keys called "foo" but have different
values, you store each "foo" key in a separate table.  The same is
true in other database systems.</p><p>Hibari’s implementation uses one or more replication chains to store
the data for one table.</p><div class="figure"><a id="id422898"></a><p class="title"><b>Figure 6. Relationship between tables, chains, and bricks.</b></p><div class="figure-contents"><a class="ulink" href="images/table-chain-brick.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="70%"><tr><td align="center"><img src="images/table-chain-brick.png" align="middle" width="100%" alt="images/table-chain-brick.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /></div><div class="section" title="5.5. Micro-Transactions"><div class="titlepage"><div><div><h3 class="title"><a id="micro-transactions"></a>5.5. Micro-Transactions</h3></div></div></div><p>In a single request, a Hibari client may send multiple update
operations to the cluster.  The client has the option of requesting
“micro-transaction” semantics for those updates: if there are no
errors, then all updates will be applied atomically.  This behaves
like the “transaction commit” behavior supported by most relational
databases.</p><p>On the other hand, if there is an error while processing one of the
update operations, then all of update operations will fail.  This
behaves like the “transaction abort” behavior supported by most
relational databases.</p><p>Unlike most relational databases, Hibari does not have a transaction
manager that can coordinate ACID semantics for arbitrary read and
write operations across any row in any table.  In fact, Hibari has no
transaction manager at all.  For this reason, Hibari calls its limited
transaction feature “micro-transactions”, to distinguish this
feature from other database systems.</p><p>Hibari’s micro-transaction support has two important limitations:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
All keys involved in the transaction must be stored in the same
  replication chain (and therefore by the same brick(s)).
</li><li class="listitem">
Operations within the micro-transaction cannot see updates by other
  operations within the the same micro-transaction.
</li></ul></div><div class="figure"><a id="footab-example"></a><p class="title"><b>Figure 7. Four keys in the "footab" table, distributed across two chains of length three.</b></p><div class="figure-contents"><a class="ulink" href="images/micro-transaction-example.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="70%"><tr><td align="center"><img src="images/micro-transaction-example.png" align="middle" width="100%" alt="images/micro-transaction-example.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><p>In the diagram above, a micro-transaction can be permitted if it
operates on only the keys "string1" &amp; "string4" or only the keys
"string2" and "string3".  If a client were to send a micro-transaction
that operates on keys "string1" and "string3", the micro-transaction
will be rejected: key "string3" is not stored by the same chain as the
key "string1".</p><div class="example"><a id="valid-utxn"></a><p class="title"><b>Example 2. Valid micro-transaction: all keys managed by same chain</b></p><div class="example-contents"><pre class="literallayout">[txn,
 {op = replace, key = "string1", value = "Hello, world!"},
 {op = delete, key = "string4"}
]</pre></div></div><br class="example-break" /><div class="example"><a id="invalid-utxn"></a><p class="title"><b>Example 3. Invalid micro-transaction: keys managed by different chains</b></p><div class="example-contents"><pre class="literallayout">[txn,
 {op = replace, key = "string1", value = "Hello, world!"},
 {op = delete, key = "string2"}
]</pre></div></div><br class="example-break" /><p>The client does not have direct control over how keys are distributed
across chains.  When a table is defined and created, its configuration
specifies the algorithm used to map a {TableName, Key} pair to a
specific chain.</p><div class="note" title="Note" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="./images/icons/note.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>See
<a class="ulink" href="hibari-contributor-guide.en.html#add-a-new-table" target="_top">Hibari Contributor’s Guide,
"Add a New Table" section</a>
for more information about table configuration.</p></td></tr></table></div></div><div class="section" title="5.6. Distribution: Workload Partitioning and Fault Tolerance"><div class="titlepage"><div><div><h3 class="title"><a id="_distribution_workload_partitioning_and_fault_tolerance"></a>5.6. Distribution: Workload Partitioning and Fault Tolerance</h3></div></div></div><div class="section" title="5.6.1. Partitioning by consistent hashing"><div class="titlepage"><div><div><h4 class="title"><a id="consistent-hashing-example"></a>5.6.1. Partitioning by consistent hashing</h4></div></div></div><p>To spread computation and storage workloads across all servers in the
cluster, Hibari uses a technique called “consistent hashing”.  This
hashing technique attempts to distribute a table’s key space evenly
across all chains used by that table.</p><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>The word “consistent” has slightly different meanings
relative to “consistent hashing” and “strong consistency”.  The
consistent hashing algorithm is a commonly-used algorithm for key →
storage location calculations.  Consistent hashing does not affect the
“eventual consistency” or “strong consistency” semantics of a
database system.</p></td></tr></table></div><p>See the <a class="xref" href="#footab-example" title="Figure 7. Four keys in the &quot;footab&quot; table, distributed across two chains of length three.">Figure 7, “Four keys in the "footab" table, distributed across two chains of length three.”</a> for an example of a table with two
chains.</p><p>See
<a class="ulink" href="hibari-contributor-guide.en.html#add-a-new-table" target="_top">Hibari Contributor’s Guide,
"Add a New Table" section</a>
for details on valid options when creating new tables.</p><div class="section" title="5.6.1.1. Consistent hashing algorithm"><div class="titlepage"><div><div><h5 class="title"><a id="_consistent_hashing_algorithm"></a>5.6.1.1. Consistent hashing algorithm</h5></div></div></div><p>Hibari uses the following steps in its consistent hashing algorithm
implementation:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
Calculate the “hashing prefix”, using part or all of the key as
  input to the next step.
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
This step is configurable, using built-in functions or by providing
   a custom implementation function.
</li><li class="listitem"><p class="simpara">
Built-in prefix functions:
</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
Null: use entire key
</li><li class="listitem">
Fixed length, e.g. 4 byte or 8 byte constant length prefix.
</li><li class="listitem">
Variable length: use separator character <code class="literal">'/'</code> (configurable)
    such that hash prefix is found between the first two (also
    configurable) <code class="literal">'/'</code> characters.  E.g. If the key is <code class="literal">/user/bar</code>,
    then the string <code class="literal">/user/</code> is used as the hash prefix.
</li></ul></div></li></ul></div></li><li class="listitem">
Calculate the MD5 checksum of the hashing prefix and then convert
  the result to the unit interval, 0.0 - 1.0, using floating point
  arithmetic.
</li><li class="listitem"><p class="simpara">
Consult the unit interval → chain map to calculate the chain name.
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
This map contains a tree of <code class="literal">{StartValue, EndValue, ChainName}</code> tuples.
   For example, <code class="literal">{0.0, 0.5, footab_ch1}</code> will map the interval
   <code class="literal">(0.0, 0.5]</code> to the chain named <code class="literal">footab_ch1</code>.
</p><div class="itemizedlist"><ul class="itemizedlist" type="square"><li class="listitem">
The mapping tree’s construction is affected by the chain weighting
    factor.  The weighting factor allows some chains to store more
    than other chains.
</li></ul></div></li></ul></div></li><li class="listitem"><p class="simpara">
Use the operation type to calculate the brick name.
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
For read-only operations, choose the tail brick.
</li><li class="listitem">
For update operations, choose the head brick.
</li></ul></div></li></ul></div></div><div class="section" title="5.6.1.2. Consistent hashing algorithm use within the cluster"><div class="titlepage"><div><div><h5 class="title"><a id="_consistent_hashing_algorithm_use_within_the_cluster"></a>5.6.1.2. Consistent hashing algorithm use within the cluster</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Hibari clients use the algorithm to calculate which chain must
handle operations for a key.  Clients obtain this information via
updates from the Hibari Admin Server.  These updates allow the client
to send its request directly to the correct server in most use cases.
</li><li class="listitem"><p class="simpara">
Servers use the algorithm to verify that the client’s calculation
was correct.
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
If a client sends an operation to the wrong brick, the brick will
forward the operation to the correct brick.
</li><li class="listitem">
If a client sends a list of operations such that some bricks are
stored on the brick and other keys are not, an error is returned to
the client.  Micro-transactions are not supported across chains.
</li></ul></div></li></ul></div></div><div class="section" title="5.6.1.3. Changing consistent hashing configuration dynamically"><div class="titlepage"><div><div><h5 class="title"><a id="_changing_consistent_hashing_configuration_dynamically"></a>5.6.1.3. Changing consistent hashing configuration dynamically</h5></div></div></div><p>Hibari’s Admin Server will allow changes to the consistent hashing
algorithm without service interruption.  Such changes are applied on a
per-table basis:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Adding or removing chains to the unit interval → chain map.
</li><li class="listitem">
Modifications of the chain weighting factor.
</li><li class="listitem">
Modifying the key → hashing prefix calculation function.
</li></ul></div><p>See the <a class="xref" href="#chain-migration" title="9.4. Chain Migration: Rebalancing Data Across Chains">Section 9.4, “Chain Migration: Rebalancing Data Across Chains”</a> section for more information.</p></div></div><div class="section" title="5.6.2. Multiple replicas for fault tolerance"><div class="titlepage"><div><div><h4 class="title"><a id="_multiple_replicas_for_fault_tolerance"></a>5.6.2. Multiple replicas for fault tolerance</h4></div></div></div><p>For fault tolerance, data replication is required.  As explained in
<a class="xref" href="#chains" title="5.3. Chains">Section 5.3, “Chains”</a>, the basic unit of failure is the brick.  The chain
replication algorithm will maintain replicas of keys in a strongly
consistent manner across all bricks: head, middle, and tail bricks.</p><p>To be able to tolerate <code class="literal">F</code> failures without data loss or service
interruption, each replication chain must be at least <code class="literal">F+1</code> bricks
long.  This is in contrast to quorum replication family algorithms,
which typically require <code class="literal">2F+1</code> replica bricks.</p><div class="section" title="5.6.2.1. Changing chain length configuration dynamically"><div class="titlepage"><div><div><h5 class="title"><a id="_changing_chain_length_configuration_dynamically"></a>5.6.2.1. Changing chain length configuration dynamically</h5></div></div></div><p>Hibari’s Admin Server will allow changes to a chain’s length without
service interruption.  Such changes are applied on a per-chain basis.
See the <a class="xref" href="#chain-length-change" title="9.3. Changing Chain Length (Changing Replication Factor)">Section 9.3, “Changing Chain Length (Changing Replication Factor)”</a> section for more information.</p></div></div></div></div><div class="section" title="6. The Admin Server Application"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="admin-server-app"></a>6. The Admin Server Application</h2></div></div></div><p>The Hibari “Admin Server” is an OTP application that runs in an
active/standby configuration within a Hibari cluster.  The Admin
Server is responsible for:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Monitoring the health of each brick in the cluster,
  see <a class="xref" href="#brick-lifecycle-fsm" title="8.1. Brick Lifecycle Finite State Machine">Section 8.1, “Brick Lifecycle Finite State Machine”</a>.
</li><li class="listitem">
Monitoring the status of each chain in the cluster,
  see <a class="xref" href="#chain-lifecycle-fsm" title="8.2. Chain Lifecycle Finite State Machine">Section 8.2, “Chain Lifecycle Finite State Machine”</a>.
</li><li class="listitem">
Managing administrative changes of chain → brick mappings,
  see <a class="xref" href="#chain-length-change" title="9.3. Changing Chain Length (Changing Replication Factor)">Section 9.3, “Changing Chain Length (Changing Replication Factor)”</a>.
</li><li class="listitem">
Managing data rebalancing, see <a class="xref" href="#chain-migration" title="9.4. Chain Migration: Rebalancing Data Across Chains">Section 9.4, “Chain Migration: Rebalancing Data Across Chains”</a>.
</li><li class="listitem">
Communicating cluster status to Hibari client nodes.
</li><li class="listitem">
Other administrative tasks, such as the creation of new tables.
</li></ul></div><p>Only one instance of the Admin Server is permitted to run within the
cluster at a time.  The Admin Server runs in an “active/standby”
configuration that is used in many high-availability clustered
applications.  The nodes that are eligible to participate in the
active/standby configuration are configured via the main Hibari
configuration file; see <a class="xref" href="#admin-server-in-central-conf" title="7.3.1. Admin Server entries in the central.conf file">Section 7.3.1, “Admin Server entries in the <code class="literal">central.conf</code> file”</a> and
<a class="xref" href="#central-conf-parameters" title="7.2. Parameters in the central.conf File">Section 7.2, “Parameters in the <code class="literal">central.conf</code> File”</a> for more details.</p><div class="section" title="6.1. Admin Server Active/Standby Implementation"><div class="titlepage"><div><div><h3 class="title"><a id="_admin_server_active_standby_implementation"></a>6.1. Admin Server Active/Standby Implementation</h3></div></div></div><p>The active/standby application failover is handled by the Erlang/OTP
application controller.  No extra third-party software is required.
See Chapter 7, "Applications", and Chapter 9, "Distributed
Applications", in the "OTP Design Principles User’s Guide" at
<a class="ulink" href="http://www.erlang.org/doc/design_principles/distributed_applications.html" target="_top">http://www.erlang.org/doc/design_principles/distributed_applications.html</a>.</p></div><div class="section" title="6.2. Admin Server’s Private State: the Bootstrap Bricks"><div class="titlepage"><div><div><h3 class="title"><a id="bootstrap-bricks"></a>6.2. Admin Server’s Private State: the Bootstrap Bricks</h3></div></div></div><p>On each active and standby node, there is a hint file called
<code class="literal">Schema.local</code> which contains the name of the “bootstrap bricks”.
These bricks operate outside of the chain replication algorithm to
provide redundant, persistent state for the Admin Server application.
See <a class="xref" href="#bricks-outside-chain-replication" title="5.3.1. Bricks outside of chain replication">Section 5.3.1, “Bricks outside of chain replication”</a> for a short summary of
standalone bricks.</p><p>All of the Admin Server’s private state is stored in the bootstrap
bricks.  This includes:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
All table definitions and their configuration, e.g. consistent
  hashing parameters.
</li><li class="listitem">
Status of all bricks and all chains.
</li><li class="listitem">
Operational history of all bricks and all chains.
</li></ul></div><p>With the help of the Erlang/OTP application controller and the Hibari
Partition Detector application, only a single instance of the Admin
Server is permitted to run at any one time.  That single application
instance has full control over the data stored in the bootstrap bricks
and therefore does not have to manage concurrent updates to bootstrap
brick data.</p></div><div class="section" title="6.3. Admin Server Crash and Restart"><div class="titlepage"><div><div><h3 class="title"><a id="_admin_server_crash_and_restart"></a>6.3. Admin Server Crash and Restart</h3></div></div></div><p>When the Admin Server application is stopped (e.g. node shutdown) or
crashes (e.g. software bug, power failure), all of the tasks outlined
at the beginning of <a class="xref" href="#admin-server-app" title="6. The Admin Server Application">Section 6, “The Admin Server Application”</a> are halted.  In theory,
the 20-30 seconds that are required for the Admin Server to restart
could mean 20-30 seconds of negative service impact to Hibari clients.</p><p>In practice, however, Hibari clients almost never notice when an Admin
Server instance crashes and restarts.  Hibari clients do not need the
Admin Server when the cluster is stable.  The Admin Server is only
necessary when the state of the cluster changes.  Furthermore, as far
as clients are concerned, clients are only affected when bricks crash.
Other cluster change events, such as when chain replication repair
finished, do not directly impact clients and thus can wait for the
Admin Server to finish restarting.</p><p>A Hibari client will only notice an Admin Server crash if another
logical brick crashes while the Admin Server is temporarily out of
service.  The reason is due to the nature of the Admin Server’s
responsibilities.  When chain is broken by a brick failure, the
remaining bricks must have their roles reconfigured to put the chain
back into full service.  The Admin Server is the only automated entity
that is permitted to change the role of a brick.  For more details,
see:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="xref" href="#brick-lifecycle-fsm" title="8.1. Brick Lifecycle Finite State Machine">Section 8.1, “Brick Lifecycle Finite State Machine”</a>
</li><li class="listitem">
<a class="xref" href="#chain-lifecycle-fsm" title="8.2. Chain Lifecycle Finite State Machine">Section 8.2, “Chain Lifecycle Finite State Machine”</a>, and
</li><li class="listitem">
<a class="xref" href="#chain-repair" title="8.5. Chain Repair">Section 8.5, “Chain Repair”</a>.
</li></ul></div></div><div class="section" title="6.4. Admin Server and Network Partition"><div class="titlepage"><div><div><h3 class="title"><a id="admin-server-and-network-partition"></a>6.4. Admin Server and Network Partition</h3></div></div></div><p>One feature of the Erlang/OTP application controller is that it is not
robust in event of a network partition.  To prevent multiple Admin
Server apps running simultaneously, another application is bundled
with Hibari: the Partition Detector.  See <a class="xref" href="#partition-detector" title="10. The Partition Detector Application">Section 10, “The Partition Detector Application”</a>
for an overview and explanation of the <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span> physical networks.</p><p>As described briefly in <a class="xref" href="#cap-theorem-and-hibari" title="1.6. The CAP Theorem and Hibari">Section 1.6, “The CAP Theorem and Hibari”</a>, Hibari does not
support the "Partition tolerance" aspect of Eric Brewer’s CAP theorem.
More specifically, if a network partition occurs, and a Hibari cluster
is split into two or more pieces, not all clients on both/all sides of
the network partition will be able to access Hibari services.</p><p>For the sake of discussion, we assume the cluster has been split into
two fragments by a single partition, though any number of fragments may
happen in real use.  We also assume that nodes on both sides of the
partition are configured in standby roles for the Admin Server.</p><p>If a network partition event happens, the following events will soon
follow:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The OTP application controller for some/all
  <code class="literal">central.conf</code>-configured nodes will notice that communication with
  the formerly active Admin Server is now impossible.
</li><li class="listitem">
Using internal logic, each application controller will make a
  decision of which standby node should move to active status.
</li><li class="listitem">
Each active status node will start an instance of the Admin Server.
</li></ul></div><p>Note that all steps above will happen in parallel on nodes on both
sides of the partition.  If this situation is permitted to continue,
the invariant of "Admin Server may only run on one node at a time"
will be violated.  However, with the help of the Partition Detector
application, multiple Admin Server instances can be detected and
halted.</p><p>UDP broadcasts on the <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span> networks can help the Admin Server
determine if it was restarted due to an Admin Server crash or by a
network partition.  In case of a network partition on network <span class="emphasis"><em>A</em></span>, the
broadcasts on network <span class="emphasis"><em>B</em></span> can indicate that another Admin Server
process remains alive.</p><p>If multiple Admin Server instances are detected, the following logic
is used:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
If an Admin Server is in its "running" phase, then any other any
  Admin Server instance that is still in its "initialization" phase
  will halt.
</li><li class="listitem">
If multiple Admin Server instances are all in the "initialization"
  phase, then only the Admin Server instance with the smallest name
  (in lexicographic sorting order) is permitted to run: all other
  instances will halt.
</li></ul></div><div class="section" title="6.4.1. Importance of two physically separate networks"><div class="titlepage"><div><div><h4 class="title"><a id="_importance_of_two_physically_separate_networks"></a>6.4.1. Importance of two physically separate networks</h4></div></div></div><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>It is possible for both the <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span> networks to
partition simultaneously.  The Admin Server and Partition Detector
applications cannot always correctly react to such events.  It is
extremely important that the <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span> networks be separate physical
networks, including: separate physical network interfaces on each
brick, separate cabling, separate network switches, and all other
network-related equipment also be physically separate.</p></td></tr></table></div><p>It is possible to reduce the reliance on multiple physical networks
and the Partition Detector application, but such techniques have not
been added to Hibari yet.  Until an alternative network partition
mitigation mechanism is implemented, we strongly recommend the proper
configuration of the Partition Detector app and all of its hardware
requirements.</p></div></div><div class="section" title="6.5. Admin Server, Network Partition, and Client Access"><div class="titlepage"><div><div><h3 class="title"><a id="_admin_server_network_partition_and_client_access"></a>6.5. Admin Server, Network Partition, and Client Access</h3></div></div></div><p>When a network partition event occurs, there are two cases that affect
a client’s ability to work with the cluster.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The client machine is on the same side of the partition as the Admin
  Server.
</li><li class="listitem">
The client machine is on the opposite side of the partition as the
  Admin Server.
</li></ul></div><p>If the client machine is on the same side of the partition, the client
may see no interruption of service at all.  If the Admin Server is
restarted in reaction to the partition event, there may be a small
window of time (e.g. 20-30 seconds) where requests might fail because
the Admin Server has not yet reconfigured chains on this side of the
partition.</p><p>If the client machine is on the opposite side of the partition, then
the client will not have access to the Admin Server and may not have
access to properly configured chains.  If a chain lies entirely
entirely on the same side of the partition as the client, then the
client can continue to use that chain successfully.  However, any
chain that is "cut in two" by the partition cannot support updates by
any client.</p></div></div><div class="section" title="7. Hibari System Information: Configuration Files, Etc."><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_hibari_system_information_configuration_files_etc"></a>7. Hibari System Information: Configuration Files, Etc.</h2></div></div></div><p>Hibari’s system information is stored in one of two places.  The first
is the application configuration file, <code class="literal">central.conf</code>.  By default,
this file is stored in <code class="literal">TODO/{version number}/etc/central.conf</code>.</p><p>The second location is within Hibari server nodes themselves.  This
kind of configuration, stored inside the "bootstrap" bricks, makes it
easy to share data with all nodes in the cluster.</p><p>Many of configuration values in <code class="literal">central.conf</code> will be the same on all
nodes in a Hibari cluster.  Given this reality, why not store those
items in Hibari itself?  The biggest problem comes when the
application is first starting.  See
<a class="xref" href="#bricks-outside-chain-replication" title="5.3.1. Bricks outside of chain replication">Section 5.3.1, “Bricks outside of chain replication”</a> for an overview of why it
isn’t easy to store all configuration data inside Hibari itself.</p><p>In the future, it’s likely that many of the configuration items in the
<code class="literal">central.conf</code> file will move to storage within Hibari itself.</p><div class="section" title="7.1. central.conf File Syntax and Usage"><div class="titlepage"><div><div><h3 class="title"><a id="_literal_central_conf_literal_file_syntax_and_usage"></a>7.1. <code class="literal">central.conf</code> File Syntax and Usage</h3></div></div></div><p>Each line of the <code class="literal">central.conf</code> file has the form</p><pre class="literallayout">parameter: value</pre><p>where <code class="literal">parameter</code> is the name of the configuration option being set and
<code class="literal">value</code> is the value that the configuration option is being set to.</p><p>Valid data types for configuration settings are INT (integer), STRING
(string), and ATOM (one of a pre-defined set of option names, such as
<code class="literal">on</code> or <code class="literal">off</code>). Apart from data type restrictions, no further valid
range restrictions are enforced for <code class="literal">central.conf</code> parameters.</p><p>All time values in <code class="literal">central.conf</code> (such as delivery retry intervals or
transaction timeouts) must be set as a number of seconds.</p><p>Blank lines and lines beginning with the pound sign (#) are ignored.</p><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>To apply changes that you have made to the <code class="literal">central.conf</code>
file, you must restart the server.  There are exceptions to this rule,
but it’s one of the cleanup/janitor tasks to access
<code class="literal">central.conf</code> using a standard set of APIs, e.g. always use the
<code class="literal">gmt_config_svr</code> API.</p></td></tr></table></div></div><div class="section" title="7.2. Parameters in the central.conf File"><div class="titlepage"><div><div><h3 class="title"><a id="central-conf-parameters"></a>7.2. Parameters in the <code class="literal">central.conf</code> File</h3></div></div></div><p>A detailed explanation of each of the items in <code class="literal">central.conf</code> can be
found at
<a class="ulink" href="../misc-files/central-conf.pdf" target="_top">Hibari <code class="literal">central.conf</code> Configuration Guide</a>.</p></div><div class="section" title="7.3. Admin Server Configuration"><div class="titlepage"><div><div><h3 class="title"><a id="_admin_server_configuration"></a>7.3. Admin Server Configuration</h3></div></div></div><p>Configuration for the Hibari “Admin Server” is stored in three
places:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
The <code class="literal">central.conf</code> file
</li><li class="listitem">
The <code class="literal">Schema.local</code> file
</li><li class="listitem">
Inside the “bootstrap” bricks
</li></ol></div><div class="section" title="7.3.1. Admin Server entries in the central.conf file"><div class="titlepage"><div><div><h4 class="title"><a id="admin-server-in-central-conf"></a>7.3.1. Admin Server entries in the <code class="literal">central.conf</code> file</h4></div></div></div><p>The following entries in the <code class="literal">central.conf</code> file are used by the
Hibari Admin Server:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
<code class="literal">admin_server_distributed_nodes</code>
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
This option specifies which nodes in the Hibari cluster are
   eligible to run the Admin Server.  Hibari server nodes not included
   in this list cannot run the Admin Server.
</li><li class="listitem">
Active/standby service is provided by the Erlang/OTP platform’s
   application management facility.
</li></ul></div></li><li class="listitem"><p class="simpara">
The <code class="literal">Schema.local</code> file
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
This file provides a list of {logical brick, Hibari server node name}
   tuples that store the Admin Server’s private state.  Each brick
   name in this list starts with the prefix <code class="literal">bootstrap_copy</code> followed
   by an integer.
</li></ul></div></li><li class="listitem"><p class="simpara">
The “bootstrap” bricks
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
Each of these bricks store an independent copy of all Hibari
   cluster state: table definitions, table → chain mappings,
   start &amp; stop history, etc.
</li><li class="listitem">
Data in each of the bootstrap bricks is not maintained by chain
   replication.  Rather, quorum-style replication is used.
   See <a class="xref" href="#bricks-outside-chain-replication" title="5.3.1. Bricks outside of chain replication">Section 5.3.1, “Bricks outside of chain replication”</a>.
</li></ul></div></li></ul></div></div></div><div class="section" title="7.4. Configuration Not Stored in Editable Config Files"><div class="titlepage"><div><div><h3 class="title"><a id="_configuration_not_stored_in_editable_config_files"></a>7.4. Configuration Not Stored in Editable Config Files</h3></div></div></div><p>All table and chain configuration parameters are stored within the
Admin Server’s “schema”.  The schema contains information on:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Table names and options (e.g. blob values stored in RAM or on disk,
  sync/async disk logging)
</li><li class="listitem">
Table → chain mappings
</li><li class="listitem">
Chain → brick mappings
</li></ul></div><p>Much of this information can be seen in HTML form by pointing a Web
browser at TCP port 23080 (default) of any Hibari server node.  For
example:</p><div class="example"><a id="id424354"></a><p class="title"><b>Example 4. Admin Server Top-Level Status &amp; Admin URL</b></p><div class="example-contents"><pre class="literallayout">http://hibari-server-node-hostname:23080/</pre></div></div><br class="example-break" /><p>Your Web browser should be redirected automatically to the Admin
Server’s top-level status &amp; admin page.</p><div class="note" title="Note" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="./images/icons/note.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>The APIs that expose this are, for the most part, already
written.  We need more "friendly" wrapper funcs as part of the "try
this first" set of APIs for administration.</p></td></tr></table></div></div></div><div class="section" title="8. The Life of a (Logical) Brick"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_the_life_of_a_logical_brick"></a>8. The Life of a (Logical) Brick</h2></div></div></div><p>All logical bricks within a Hibari cluster go through the same set of
lifecycle events.  Each is described in greater detail in this
section.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Brick initialization and operation states, described by a finite
  state machine.
</li><li class="listitem">
Brick roles within chain replication, also described by a finite
  state machine.
</li><li class="listitem">
Periodic housekeeping tasks performed by logical bricks and their
  internal support services, e.g. checkpoints and the “scavenger”.
</li></ul></div><div class="section" title="8.1. Brick Lifecycle Finite State Machine"><div class="titlepage"><div><div><h3 class="title"><a id="brick-lifecycle-fsm"></a>8.1. Brick Lifecycle Finite State Machine</h3></div></div></div><p>The lifecycle of each Hibari logical brick goes through a set of
states defined by a finite state machine (OTP <code class="literal">gen_fsm</code> behavior) that
is executed by a process within the Admin Server application.</p><div class="figure"><a id="id424440"></a><p class="title"><b>Figure 8. Logical brick lifecycle finite state machine</b></p><div class="figure-contents"><a class="ulink" href="images/brick-fsm.svg" target="_top">
  <div class="mediaobject" align="center"><img src="images/brick-fsm.png" align="middle" alt="images/brick-fsm.svg" /></div>
</a></div></div><br class="figure-break" /><div class="variablelist" title="Logical brick lifecycle FSM states"><p class="title"><b>Logical brick lifecycle FSM states</b></p><dl><dt><span class="term">
unknown
</span></dt><dd>
  This is the initial state of the FSM.  Because the Admin Server may
  crash or be restarted at any time, this state is used by the Admin
  Server when it has not been running long enough to determine the
  state of the logical brick.
</dd><dt><span class="term">
pre_init
</span></dt><dd>
  A brick moves itself to this state when it has finished scanning its
  private write-ahead log (see <a class="xref" href="#write-ahead-logs" title="5.2. Write-Ahead Logs">Section 5.2, “Write-Ahead Logs”</a>) and therefore
  knows the state of all keys that it manages.
</dd><dt><span class="term">
repairing
</span></dt><dd>
  In chain replication, the repairing state is used to synchronize a
  a newly started/restart brick with the rest of the chain.  At the
  end of this state, the brick is 100% in sync with all other active
  members of the chain.  Repair is initiated by the Admin Server’s
  chain monitor that is responsible for the chain.
</dd><dt><span class="term">
ok
</span></dt><dd><p class="simpara">
  The brick moves itself to this state when repair has finished.  The
  brick is now in service and capable of servicing Hibari client
  requests.  Client requests will be rejected if the brick is in any
  other state.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
If managed by chain replication, this brick is eligible to be put
    into service as a full member of a replication chain.
    See <a class="xref" href="#brick-roles" title="8.3. Brick “Roles” Within A Chain">Section 8.3, “Brick “Roles” Within A Chain”</a>.
</li><li class="listitem">
If managed by quorum replication, some external entity must change
    the logical brick’s state from <code class="literal">pre_init</code> → <code class="literal">ok</code>.  Hibari’s Admin
    Server automates this task for the <code class="literal">bootstrap_copy</code>* bricks.
    The present implementation of the Admin Server does not manage
    quorum replication bricks outside of the Admin Server’s private
    use.
</li></ul></div></dd><dt><span class="term">
disk_error
</span></dt><dd>
  A disk error has occurred, for example a missing file or directory
  or MD5 checksum error.  Administrator intervention is required to
  move a brick out of the <code class="literal">disk_error</code> state: shut down the entire
  Hibari server, kill the logical brick manually, or use the
  brick_chainmon:force_best_first_brick() function manually.
</dd></dl></div></div><div class="section" title="8.2. Chain Lifecycle Finite State Machine"><div class="titlepage"><div><div><h3 class="title"><a id="chain-lifecycle-fsm"></a>8.2. Chain Lifecycle Finite State Machine</h3></div></div></div><p>The chain FSM (OTP ‘gen_fsm` behavior) is executed by a process within
the Admin Server application.  All state transitions are triggered by
changes in the state of each member bricks’ state, into or out of the
<code class="literal">'ok'</code> state.  See <a class="xref" href="#brick-lifecycle-fsm" title="8.1. Brick Lifecycle Finite State Machine">Section 8.1, “Brick Lifecycle Finite State Machine”</a> for details.</p><div class="figure"><a id="id424633"></a><p class="title"><b>Figure 9. Chain replication finite state machine</b></p><div class="figure-contents"><a class="ulink" href="images/chain-fsm.svg" target="_top">
  <div class="mediaobject" align="center"><img src="images/chain-fsm.png" align="middle" alt="images/chain-fsm.svg" /></div>
</a></div></div><br class="figure-break" /><div class="variablelist" title="Chain lifecycle FSM states"><p class="title"><b>Chain lifecycle FSM states</b></p><dl><dt><span class="term">
unknown
</span></dt><dd>
  The state of the chain is unknown. Information regarding chain members is
  unavailable. Because the Admin Server may
  crash or be restarted at any time, this state is used by the Admin
  Server when it has not been running long enough to determine the
  state of the chain.
  It is possible that the chain was in <code class="literal">degraded</code> or <code class="literal">healthy</code> state
  before the crash and therefore Hibari client operations may be
  serviced while in this state.
</dd><dt><span class="term">
unknown_timeout
</span></dt><dd>
  This intermediate state is used by the Admin Server before moving
  automatically to another state.
</dd><dt><span class="term">
stopped
</span></dt><dd>
  All bricks in the chain are crashed or believed to have crashed.
  Service to Hibari clients will be interrupted.
</dd><dt><span class="term">
degraded
</span></dt><dd>
  Some (but not all) bricks in the chain are in service.  The Admin
  Server will wait for another chain member to enter its <code class="literal">pre_init</code>
  state before chain repair can start.
</dd><dt><span class="term">
healthy
</span></dt><dd>
  All bricks in the chain are in service.
</dd></dl></div></div><div class="section" title="8.3. Brick “Roles” Within A Chain"><div class="titlepage"><div><div><h3 class="title"><a id="brick-roles"></a>8.3. Brick “Roles” Within A Chain</h3></div></div></div><p>Each brick within a chain has a role.  The role will be changed by the
Admin Server whenever it detects that the chain’s state has changed.
These roles are:</p><div class="variablelist"><dl><dt><span class="term">
head
</span></dt><dd>
  The brick is first in the chain, i.e. at the “head” of the chain’s
  ordered list of bricks.
</dd><dt><span class="term">
tail
</span></dt><dd>
  The brick is last in the chain, i.e. at the “tail” of the chain’s
  ordered list of bricks.
</dd><dt><span class="term">
middle
</span></dt><dd>
  The brick is neither the “head” nor “tail” of the chain.
  Instead, the brick is somewhere in the middle of the chain.
</dd><dt><span class="term">
standalone
</span></dt><dd>
  In a chain of length 1, the “standalone” brick is a brick that
  acts both as a “head” and “tail” brick simultaneously.
</dd></dl></div><p>There is one additional attribute that is given to a brick in a
cluster.  Its name “official tail”.</p><div class="variablelist"><dl><dt><span class="term">
official tail
</span></dt><dd><p class="simpara">
  The official tail brick has two duties for the chain:
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
It handles read-only queries to the chain.
</li><li class="listitem">
It sends replies to the client for all update operations that are
  sent to the head of the chain.
</li></ul></div></dd></dl></div><p>As far as Hibari clients are concerned, the chain member with the
the "official tail" role is the brick that they consider the "tail" of
the chain.  Hibari clients are not aware of "tail" bricks that are
undergoing repair.  Any client request that is sent to a <code class="literal">repairing</code>
state brick will be rejected.</p><p>See <a class="xref" href="#diagram-write-path-3" title="Figure 3. Message flow in a chain for a key update">Figure 3, “Message flow in a chain for a key update”</a> for an example of a healthy chain of
length three.</p></div><div class="section" title="8.4. Brick Initialization"><div class="titlepage"><div><div><h3 class="title"><a id="brick-init"></a>8.4. Brick Initialization</h3></div></div></div><p>A logical brick does not maintain an on-disk data structure, such as a
binary tree or B-tree, to keep track of the keys it stores.  Instead,
each logical brick maintains that metadata entirely in RAM.
Therefore, the only time that the metadata in the private write-ahead
log is ever read is at brick initialization time, i.e. when the brick
restarts.</p><p>The contents of the private write-ahead log are used to repopulate the
brick’s “key catalog”, the list of all keys (and associated
metadata) stored by the brick.</p><p>When a logical brick is started, all of the log sequence files in
the private log are read, starting from the oldest and ending with the
newest.  (See <a class="xref" href="#wal-dirs-and-files" title="5.2.3. Directories and files used by write-ahead logs">Section 5.2.3, “Directories and files used by write-ahead logs”</a>.)  The total amount of data
required at startup can be quite small or it can be hundreds of
gigabytes.  The factors that influence the amount of data in the
private log are:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
The total number of keys stored by the logical brick.
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
More keys means that the log sequence file created by a checkpoint
   operation will be larger.
</li></ul></div></li><li class="listitem">
The size of the <code class="literal">brick_check_checkpoint_max_mb</code> configuration parameter
  in the <code class="literal">central.conf</code> config file.
</li></ul></div><p>When the log scan is complete, construction of the brick’s in-RAM key
catalog is finished.</p><p>See <a class="xref" href="#checkpoints" title="8.6. Brick Checkpoint Operations">Section 8.6, “Brick Checkpoint Operations”</a> for details on brick checkpoint operations.</p></div><div class="section" title="8.5. Chain Repair"><div class="titlepage"><div><div><h3 class="title"><a id="chain-repair"></a>8.5. Chain Repair</h3></div></div></div><p>When a chain is in the <code class="literal">degraded</code> state, new bricks that have entered
their <code class="literal">pre_init</code> state can become eligible to join the chain.  All
new bricks are added to the end of the chain and undergo the chain
repair process.</p><div class="figure"><a id="id425007"></a><p class="title"><b>Figure 10. Chain of length 2 in <code class="literal">degraded</code> state, a third brick under repair</b></p><div class="figure-contents"><a class="ulink" href="images/read-write-path-3-repair.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="80%"><tr><td align="center"><img src="images/read-write-path-3-repair.png" align="middle" width="100%" alt="images/read-write-path-3-repair.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><p>The protocol used between upstream and downstream bricks is an
iterative protocol that has two phases in a single iteration.</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
The upstream brick sends a subset of {Key, Timestamp} tuples
downstream.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The downstream brick deletes keys from its key catalog that do not
appear in the upstream’s subset.
</li><li class="listitem">
The downstream brick replies with the list of keys that it does not
have or have older timestamps.
</li></ul></div></li><li class="listitem"><p class="simpara">
The upstream bricks sends full information (all key metadata and
value blobs) for all keys requested by the downstream in step #1.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The downstream brick acknowledges the new/replacement keys.
</li></ul></div></li></ol></div><p>When the repair is finished, the Admin Server will change the roles of
some/all chain members to make the repairing brick the new tail of the
chain.</p><p>Only one brick may be repaired at one time.  In theory it is possible
to repair multiple bricks simultaneously, but the extra code
complexity that would be required to do so has been judged to be too
expensive (so far).</p><div class="section" title="8.5.1. Chain reordering when moving from degraded → healthy states"><div class="titlepage"><div><div><h4 class="title"><a id="_chain_reordering_when_moving_from_literal_degraded_literal_8594_literal_healthy_literal_states"></a>8.5.1. Chain reordering when moving from <code class="literal">degraded</code> → <code class="literal">healthy</code> states</h4></div></div></div><div class="figure"><a id="chain-reordering-middle-brick-fails"></a><p class="title"><b>Figure 11. Chain order after a middle brick fails and is repaired (but not yet reordered)</b></p><div class="figure-contents"><a class="ulink" href="images/chain-fail-repair-reorder.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="70%"><tr><td align="center"><img src="images/chain-fail-repair-reorder.png" align="middle" width="100%" alt="images/chain-fail-repair-reorder.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><p>After a middle brick fails and is repaired, the chain’s ordering is:
brick 1 → brick 3 → brick 2.  According to the algorithm in the
original Chain Replication paper, the final chain ordering is
expected.  The Hibari implementation adds another step: reordering the
chain.</p><p>For chains longer than length 1, when the Admin Server moves the chain
from <code class="literal">degraded</code> → <code class="literal">healthy</code> state, the Admin Server will reorder the
the chain to match the schema’s definition for the healthy chain
order.  The assumption is that the Hibari administrator wishes the
chain use a very specific order when it is in the <code class="literal">healthy</code> state.
For example, if the chain’s workload were extremely read-intensive,
the machine for logical brick #3 could have faster CPU or faster disks
than the other bricks in the chain.  To take full advantage of the
extra capacity, the chain should be reordered as soon as possible.</p><p>However, it is not easy to reorder the chain.  The replication of a
client update during the reordering could get lost and violate Hibari’s
strong consistency guarantees.  The following algorithm is used to
preserve consistency:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
Set all bricks to read-only mode.
</li><li class="listitem">
Wait for all updates to sync to disk at each brick and to progress
   downstream fully from head → tail.
</li><li class="listitem">
Set brick roles to reflect the final desired order.
</li><li class="listitem"><p class="simpara">
Set all bricks to read-write mode.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Client <code class="literal">do</code> operations that contain updates will be resubmitted
   (via the client-side API function brick_server:do()) to the
   cluster.
</li></ul></div></li></ol></div><p>Typically, executing this algorithm takes less than one second.
However, because the head brick is forced temporarily into read-only
mode, client update requests will be delayed until read-only mode is
turned off.</p><p>Client update requests submitted during read-only mode will be queued
by the head brick and will be processed when read-only mode is turned
off.  Client read-only requests are not affected by read-only mode.</p></div></div><div class="section" title="8.6. Brick Checkpoint Operations"><div class="titlepage"><div><div><h3 class="title"><a id="checkpoints"></a>8.6. Brick Checkpoint Operations</h3></div></div></div><p>As updates are received by a brick, those updates are written to the
brick’s private write-ahead log.  During normal operations, private
write-ahead log is write-only: the data there is only read at logical
brick initialization time.</p><p>The checkpoint operation is used to reclaim disk space in the brick’s
private write-ahead log.  See <a class="xref" href="#wal-dirs-and-files" title="5.2.3. Directories and files used by write-ahead logs">Section 5.2.3, “Directories and files used by write-ahead logs”</a> for a
description of log sequence files and <a class="xref" href="#central-conf-parameters" title="7.2. Parameters in the central.conf File">Section 7.2, “Parameters in the <code class="literal">central.conf</code> File”</a>
for details on the <code class="literal">central.conf</code> configuration file.</p><div class="orderedlist" title="Brick checkpoint processing steps"><p class="title"><b>Brick checkpoint processing steps</b></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
When the total log size (i.e. total size of all log files in the
   brick’s private log’s shortterm storage area) reaches the size of the
   <code class="literal">brick_check_checkpoint_max_mb</code> parameter in <code class="literal">central.conf</code>, a
   checkpoint operation is started.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Assume that the current log sequence file number is <span class="strong"><strong>N</strong></span>.
</li></ul></div></li><li class="listitem">
Two log sequence files are created, <span class="strong"><strong>N+1</strong></span> and <span class="strong"><strong>N+2</strong></span>.
</li><li class="listitem">
Checkpoint data is written to log sequence number <span class="strong"><strong>N+1</strong></span>.
</li><li class="listitem">
New updates by clients and chain replication are written to log
   sequence number <span class="strong"><strong>N+2</strong></span>.
</li><li class="listitem">
Contents of the brick’s in-RAM key catalog are dumped to log
   sequence file <span class="strong"><strong>N+1</strong></span>, subject to the bandwidth constraint of the
   <code class="literal">brick_check_checkpoint_throttle_bytes</code> configuration parameter.
</li><li class="listitem">
When the checkpoint is finished and flushed to disk, all log
   sequence files with a number less than or equal to <span class="strong"><strong>N</strong></span> are deleted.
</li></ol></div><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>Each logical brick will checkpoint itself as its private
log grows.  It is possible that multiple logical bricks can schedule
checkpoint operations simultaneously.  The bandwidth limitation of the
<code class="literal">brick_check_checkpoint_throttle_bytes</code> parameter is applied to the
<span class="emphasis"><em>sum of all writes by all checkpoint operations</em></span>.</p></td></tr></table></div></div><div class="section" title="8.7. The Scavenger"><div class="titlepage"><div><div><h3 class="title"><a id="scavenger"></a>8.7. The Scavenger</h3></div></div></div><p>As described in <a class="xref" href="#write-ahead-logs" title="5.2. Write-Ahead Logs">Section 5.2, “Write-Ahead Logs”</a>, all updates from all logical
bricks are first written to the “common log”.  The most common of
these updates are:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Metadata updates, e.g. key insert or key delete, by a logical brick.
</li><li class="listitem"><p class="simpara">
A new value blob associated with a metadata update such as a Hibari
client set operation.
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
This type is only applicable if the brick is configured to store
value blobs on disk.  This configuration is defined (by default) on a
per-table basis and is then propagated to the chain and brick level by
the Admin Server.
</li></ul></div></li></ul></div><p>As explained in <a class="xref" href="#write-ahead-logs" title="5.2. Write-Ahead Logs">Section 5.2, “Write-Ahead Logs”</a>, the write-ahead log provides
infinite storage at a logical level.  But in the physical level, disk
space must be reclaimed somehow.  Because the common log is shared by
multiple logical bricks, the technique described in <a class="xref" href="#checkpoints" title="8.6. Brick Checkpoint Operations">Section 8.6, “Brick Checkpoint Operations”</a>
cannot be used by the common log.</p><p>A process called the “scavenger” is used to reclaim disk space in
the common log.  By default, the scavenger runs at 03:00 daily.  The
steps it executes are described below.</p><div class="orderedlist" title="Common log scavenger processing steps"><p class="title"><b>Common log scavenger processing steps</b></p><ol class="orderedlist" type="1"><li class="listitem">
For all bricks that store value blobs on disk, scan each logical
brick’s in-RAM key catalog to create a list of all value blob storage
locations.
</li><li class="listitem">
Sort the value blob location list by log sequence number.
</li><li class="listitem">
Identify all log sequence files with a "live data ratio" of at
least <span class="strong"><strong>X</strong></span> percent (default = 90%, see
<code class="literal">brick_skip_live_percentage_greater_than</code> configuration parameter).
</li><li class="listitem">
For all log files where live data ratio is less than <span class="strong"><strong>X</strong></span>%, copy
value blobs to new log sequence files.  This copying is limited by the
amount of bandwidth configured by <code class="literal">brick_scavenger_throttle_bytes</code> in
<code class="literal">central.conf</code>.
</li><li class="listitem">
When all blobs have been copied out of an old log sequence file and
flushed to stable storage, update the storage locations in the in-RAM
key catalog, then delete the old log sequence file.
</li></ol></div><p><span class="inlinemediaobject"><img src="images/scavenger-techpubs.png" alt="images/scavenger-techpubs.png" /></span></p><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>The value of the <code class="literal">brick_skip_live_percentage_greater_than</code>
configuration parameter determines how much additional disk space is
required to store <span class="strong"><strong>X</strong></span> gigabytes of live data.  If the parameter is
<span class="strong"><strong>N</strong></span>, then 100-<span class="strong"><strong>N</strong></span> percent of all common log disk space may be wasted
by storing dead data.</p></td></tr></table></div><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>Additional disk space is required to log all updates that
are made after the scavenger has run.  This includes space in the
common log as well as in each logical brick private logs (subject to
the general limit of the <code class="literal">brick_check_checkpoint_max_mb</code> configuration
parameter.</p></td></tr></table></div><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>The current implementation of Hibari requires that
plenty of disk space <span class="emphasis"><em>always</em></span> be available for write-ahead logs and
for scavenger operations.  We strongly recommend that the
<code class="literal">brick_scavenger_temp_dir</code> configuration item use a different file
system than the <code class="literal">application_data_dir</code> parameter.  This directory
stores temporary files required for sorting and other operations that
would otherwise require large amounts of RAM.</p></td></tr></table></div></div></div><div class="section" title="9. Dynamic Cluster Reconfiguration"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_dynamic_cluster_reconfiguration"></a>9. Dynamic Cluster Reconfiguration</h2></div></div></div><div class="section" title="9.1. Adding a Table"><div class="titlepage"><div><div><h3 class="title"><a id="add-table"></a>9.1. Adding a Table</h3></div></div></div><p>A table can be added at any time, using either of two methods:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Use the Admin Server’s HTTP service: follow the "Add a table" hyperlink
  at the bottom of the top-level page.
</li><li class="listitem">
Use the <code class="literal">brick_admin</code> CLI interface at the Erlang shell.  See
<a class="ulink" href="hibari-contributor-guide.en.html#add-a-new-table" target="_top">Hibari Contributor’s Guide,
"Add a New Table" section</a>.
</li></ul></div></div><div class="section" title="9.2. Removing a Table"><div class="titlepage"><div><div><h3 class="title"><a id="remove-table"></a>9.2. Removing a Table</h3></div></div></div><div class="note" title="Note" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="./images/icons/note.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>The current Hibari implementation does not support removing
a table.</p></td></tr></table></div><p>In theory, most of the work of removing a table is already done:
chains that are abandoned after a migration are shut down
* Brick pinger processes are stopped.
* Chain monitor processes are stopped.
* Bricks are stopped.
* Brick data directories are removed.</p><p>All that remains is to update the Admin Server’s schema to remove
references to the table.</p></div><div class="section" title="9.3. Changing Chain Length (Changing Replication Factor)"><div class="titlepage"><div><div><h3 class="title"><a id="chain-length-change"></a>9.3. Changing Chain Length (Changing Replication Factor)</h3></div></div></div><p>The Hibari Admin Server manages each chain as an independent data
replication entity.  Though Hibari clients view multiple chains that
are associated with a single table, each chain is actually independent
of the other chains.  It is possible to change the length of one chain
without changing any others.  For long term operation, such
differences do not make sense.  But during short periods of cluster
reconfiguration, such differences are possible.</p><p>A chain’s length is determined by specifying a list of bricks that are
members of that chain.  The order of the list specifies the exact
chain order when the chain is in the <code class="literal">healthy</code> state.  By adding or
removing bricks from a chain definition, the length of the chain can
be changed.</p><p>A chain is defined by the Erlang 2-tuple of
<code class="literal">{ChainName, ListOfBricks}</code>, where each brick in <code class="literal">ListOfBricks</code> is a
2-tuple <code class="literal">{BrickName, NodeName}</code>.  For example, a chain of length two
called <code class="literal">footab_ch1</code> could be defined as:</p><pre class="literallayout">{footab_ch1, [{footab1_ch1_b1, 'gdss1@box-a'}, {footab1_ch1_b1, 'gdss1@box-b'}]}</pre><p>The current definition of all chains for table <code class="literal">TableName</code> can be
retrieved from the Admin Server using the
<code class="literal">brick_admin:get_table_chain_list()</code> function, for example:</p><pre class="literallayout">%% Get a list of all tables currently defined.
&gt; brick_admin:get_tables().
[tab1]</pre><pre class="literallayout">%% Get list of chains in 'tab1' as they are currently in operation.
&gt; brick_admin:get_table_chain_list(tab1).
{ok,[{tab1_ch1,[{tab1_ch1_b1,'gdss1@machine-1'},
                {tab1_ch1_b2,'gdss1@machine-2'}]},
     {tab1_ch2,[{tab1_ch2_b1,'gdss1@machine-2'},
                {tab1_ch2_b2,'gdss1@machine-1'}]}]}</pre><p>This above chain list for table <code class="literal">tab1</code> corresponds to the chain and
brick layout below.</p><div class="figure"><a id="id425866"></a><p class="title"><b>Figure 12. Table <code class="literal">tab1</code>: Two chains of length two across two Erlang nodes on two physical machines</b></p><div class="figure-contents"><a class="ulink" href="images/tab1-2x2.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="70%"><tr><td align="center"><img src="images/tab1-2x2.png" align="middle" width="100%" alt="images/tab1-2x2.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><div class="note" title="Note" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="./images/icons/note.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>To change the definition of a chain, use the
<code class="literal">change_chain_length/2</code> or <code class="literal">change_chain_length/3</code> functions.  For
documentation, see
<a class="ulink" href="hibari-contributor-guide.en.html#changing-chain-length" target="_top">Hibari Contributor’s Guide,
"Changing Chain Length" section</a></p></td></tr></table></div><div class="note" title="Note" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="./images/icons/note.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>When specifying a new chain definition, at least one brick from
the current chain must be included.</p></td></tr></table></div><div class="section" title="9.3.1. Chain changes: same algorithm, different tasks."><div class="titlepage"><div><div><h4 class="title"><a id="chain-change-same-algorithm"></a>9.3.1. Chain changes: same algorithm, different tasks.</h4></div></div></div><p>The same brick repair technique is used to handle all three of the
following cases:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
adding a brick to a chain
</li><li class="listitem">
brick failure
</li><li class="listitem">
removing a brick from a chain
</li></ul></div></div><div class="section" title="9.3.2. Adding a brick to a chain"><div class="titlepage"><div><div><h4 class="title"><a id="_adding_a_brick_to_a_chain"></a>9.3.2. Adding a brick to a chain</h4></div></div></div><p>When a brick <code class="literal">B</code> is added to a chain, that brick is treated as if it
was a member of the chain that had crashed long ago and has now been
restarted.  The same repair algorithm is used to synchronize data on
brick <code class="literal">B</code> that is used to repair bricks that were formerly in service
but since crashed and restarted.  See <a class="xref" href="#chain-repair" title="8.5. Chain Repair">Section 8.5, “Chain Repair”</a> for a
description of the Hibari repair mechanism.</p></div><div class="section" title="9.3.3. Brick failure"><div class="titlepage"><div><div><h4 class="title"><a id="_brick_failure"></a>9.3.3. Brick failure</h4></div></div></div><p>If a brick fails, the Admin Server must remove it from the chain by
reordering the chain.  The general order of operations are:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
Set new roles for the chain’s bricks, starting from the end of the
   chain and working backward.
</li><li class="listitem">
Broadcast the new chain membership to all Hibari clients.
</li></ol></div><p>If a Hibari client attempts to send an operation to a brick during
step #2 and before the new chain info from step #2 arrives, that
client may send the operation to the wrong brick.  Hibari servers will
automatically forward the query to the correct brick.  Due to network
latencies and asynchronous message passing, it is possible that the
query be forwarded multiple times before it arrives at the correct
brick.</p><p>Specific details of how chain replication handles brick failure can be
found in van Renesse and Schneider’s paper, see <a class="xref" href="#chains" title="5.3. Chains">Section 5.3, “Chains”</a> for
citation details.</p><div class="section" title="9.3.3.1. Failure of a head brick"><div class="titlepage"><div><div><h5 class="title"><a id="_failure_of_a_head_brick"></a>9.3.3.1. Failure of a head brick</h5></div></div></div><p>If the head brick fails, then the first middle brick is promoted to the
head role.  If there is no middle brick (i.e. the length of the chain
was two), then the tail brick is promoted to a standalone role (chain
length is one).</p></div><div class="section" title="9.3.3.2. Failure of a tail brick"><div class="titlepage"><div><div><h5 class="title"><a id="_failure_of_a_tail_brick"></a>9.3.3.2. Failure of a tail brick</h5></div></div></div><p>If the tail brick fails, then the last middle brick is promoted to the
tail role.  If there is no middle brick (i.e. the length of the chain
was two), then the head brick is promoted to a standalone role (chain
length is one).</p></div><div class="section" title="9.3.3.3. Failure of a middle brick"><div class="titlepage"><div><div><h5 class="title"><a id="failure-middle-brick"></a>9.3.3.3. Failure of a middle brick</h5></div></div></div><p>The failure of a middle brick requires the most complex recovery
procedure.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
Assume that the chain is three bricks: <code class="literal">A</code> → <code class="literal">B</code> → <code class="literal">C</code>.
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
If the chain is longer (more bricks upstream of <code class="literal">A</code> and/or more
 bricks downstream of <code class="literal">C</code>), the procedure remains the same.
</li></ul></div></li><li class="listitem">
Brick <code class="literal">C</code> is configured to have its upstream brick be <code class="literal">A</code>.
</li><li class="listitem">
Brick <code class="literal">A</code> is configured to have its downstream brick be <code class="literal">C</code>.
</li><li class="listitem">
The head of the chain (brick <code class="literal">A</code> or the head brick upstream of <code class="literal">A</code>)
  requests a log flush of all unacknowledged writes downstream.  This
  step is required to re-send updates that were processed by <code class="literal">A</code> but
  have not been received by <code class="literal">C</code> because of middle brick <code class="literal">B</code>'s
  failure.
</li><li class="listitem">
Brick <code class="literal">A</code> waits until it receives a write acknowledgment from the
  tail of the chain.  Once received, all bricks in the chain have
  synchronously written all items to their write-ahead logs in the
  correct order.
</li></ul></div></div></div><div class="section" title="9.3.4. Removing a brick from a chain"><div class="titlepage"><div><div><h4 class="title"><a id="_removing_a_brick_from_a_chain"></a>9.3.4. Removing a brick from a chain</h4></div></div></div><p>Removing a brick <code class="literal">B</code> permanently from a chain is a simple operation.
Brick <code class="literal">B</code> is
handled the same way that any other brick failure is handled: the
chain is simply reconfigured to exclude <code class="literal">B</code>.  See
<a class="xref" href="#chain-reordering-middle-brick-fails" title="Figure 11. Chain order after a middle brick fails and is repaired (but not yet reordered)">Figure 11, “Chain order after a middle brick fails and is repaired (but not yet reordered)”</a> for an example.</p><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>When a brick <code class="literal">B</code> is removed from a chain, all data from
brick <code class="literal">B</code> will be deleted when the operation is successful.  At this
time, the API does not have an option to allow <code class="literal">B</code>'s data to be
preserved.</p></td></tr></table></div></div></div><div class="section" title="9.4. Chain Migration: Rebalancing Data Across Chains"><div class="titlepage"><div><div><h3 class="title"><a id="chain-migration"></a>9.4. Chain Migration: Rebalancing Data Across Chains</h3></div></div></div><p>There are several cases where it is desirable to rebalance data across
chains and bricks in a Hibari cluster:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Chains are added or removed from the cluster
</li><li class="listitem">
Brick hardware is changed, e.g. adding extra disk or RAM capacity
</li><li class="listitem">
A change in a table’s consistent hashing algorithm configuration
  forces data (by definition) to another chain.
</li></ul></div><p>The same technique is used in all of these cases: chain migration.
This mirrors the same design philosophy that’s used for handling chain
changes (see <a class="xref" href="#chain-change-same-algorithm" title="9.3.1. Chain changes: same algorithm, different tasks.">Section 9.3.1, “Chain changes: same algorithm, different tasks.”</a>): use the same
algorithm to handle multiple use cases.</p><div class="section" title="9.4.1. Example: Migrating from three chains to four"><div class="titlepage"><div><div><h4 class="title"><a id="_example_migrating_from_three_chains_to_four"></a>9.4.1. Example: Migrating from three chains to four</h4></div></div></div><div class="figure"><a id="chain-migration-3to4"></a><p class="title"><b>Figure 13. Chain migration from 3 chains to 4 chains</b></p><div class="figure-contents"><a class="ulink" href="images/chain-migration-3to4.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="80%"><tr><td align="center"><img src="images/chain-migration-3to4.png" align="middle" width="100%" alt="images/chain-migration-3to4.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><p>In the example above, both the 3-chain and 4-chain configurations used
equal weighting factors.  When all chains use the same weighting
factor (e.g. 100), then the consistent hashing map in the “before”
and “after” cases look something like the figure below.</p><div class="figure"><a id="migration-3to4"></a><p class="title"><b>Figure 14. Migration from three chains to four chains</b></p><div class="figure-contents"><a class="ulink" href="images/migration-3to4.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="70%"><tr><td align="center"><img src="images/migration-3to4.png" align="middle" width="100%" alt="images/migration-3to4.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><p>It doesn’t matter that chain #4’s total area within the unit interval
is divided into three regions.  What matters is that chain #4’s total
area is equal to the regions of the other three chains.</p></div><div class="section" title="9.4.2. Example: Migrating from three chains to four with unequal weighting"><div class="titlepage"><div><div><h4 class="title"><a id="_example_migrating_from_three_chains_to_four_with_unequal_weighting"></a>9.4.2. Example: Migrating from three chains to four with unequal weighting</h4></div></div></div><p>The diagram <a class="xref" href="#migration-3to4" title="Figure 14. Migration from three chains to four chains">Figure 14, “Migration from three chains to four chains”</a> demonstrates how a migration would
work when all chains have an equal weighting factor, e.g. 100.  If
instead, the new chain had a weighting factor of only 50, then the
distribution of keys to each chain would look like this:</p><div class="table"><a id="id426444"></a><p class="title"><b>Table 1. Migration from three chains to four with unequal chain weighting factors</b></p><div class="table-contents"><table summary="Migration from three chains to four with unequal chain weighting factors" cellpadding="4px" style="border-collapse: collapse;border-top: 2px solid #527bbd; border-bottom: 2px solid #527bbd; border-left: 2px solid #527bbd; border-right: 2px solid #527bbd; "><colgroup><col /><col /><col /></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"> Chain Name </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"> Total % of keys before/after migration </th><th style="border-bottom: 1px solid ; " align="left" valign="top"> Total unit interval size before/after migration</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>Chain 1</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>33.3% → 28.6%</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>100/300 → 100/350</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>Chain 2</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>33.3% → 28.6%</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>100/300 → 100/350</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>Chain 3</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>33.3% → 28.6%</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>100/300 → 100/350</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>Chain 4</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>0% → 14.3% (4.8% in each of 3 regions)</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>0/300 → 50/350 (spread across 3 regions)</p></td></tr><tr><td style="border-right: 1px solid ; " align="left" valign="top"><p>Total</p></td><td style="border-right: 1px solid ; " align="left" valign="top"><p>100% → 100%</p></td><td style="" align="left" valign="top"><p>300/300 → 350/350</p></td></tr></tbody></table></div></div><br class="table-break" /><p>For the original three chains, the total amount of unit interval
devoted to those chains is (100+100+100)/350 = 300/350.  The 4th
chain, because its weighting is only 50, would be assigned 50/350 of
the unit interval.  Then, an equal amount of unit interval is taken
from the original chains and reassigned to chain #4, so (50/350)/3 of
the unit interval must be taken from each original chain.</p></div><div class="section" title="9.4.3. Hotspot migration"><div class="titlepage"><div><div><h4 class="title"><a id="_hotspot_migration"></a>9.4.3. Hotspot migration</h4></div></div></div><p>With the lowest level API, it is possible to assign "hot" keys to
specific chains, to try to balance a handful of keys that are very
frequently accessed from a large number of keys that are very
infrequently accessed.  The table below gives an example that builds
upon <a class="xref" href="#migration-3to4" title="Figure 14. Migration from three chains to four chains">Figure 14, “Migration from three chains to four chains”</a>.  We assume that our "hot" key is mapped
onto the unit interval at position 0.5.</p><div class="table"><a id="id426685"></a><p class="title"><b>Table 2. Consistent hashing lookup table with three chains of equal weight and a fourth chain with an extremely small weight</b></p><div class="table-contents"><table summary="Consistent hashing lookup table with three chains of equal weight and a fourth chain with an extremely small weight" cellpadding="4px" style="border-collapse: collapse;border-top: 2px solid #527bbd; border-bottom: 2px solid #527bbd; border-left: 2px solid #527bbd; border-right: 2px solid #527bbd; "><colgroup><col /><col /><col /></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"> Unit interval start </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"> Unit interval end </th><th style="border-bottom: 1px solid ; " align="left" valign="top"> Chain name</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>0.000000</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>0.333333…</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>Chain 1</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>0.333333…</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>0.5</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>Chain 2</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>0.5</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>0.500000000000001</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>Chain 4</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>0.500000000000001</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="left" valign="top"><p>0.666666…</p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>Chain 2</p></td></tr><tr><td style="border-right: 1px solid ; " align="left" valign="top"><p>0.666666…</p></td><td style="border-right: 1px solid ; " align="left" valign="top"><p>1.0</p></td><td style="" align="left" valign="top"><p>Chain 3</p></td></tr></tbody></table></div></div><br class="table-break" /><p>The table above looks almost exactly like the "Before Migration" half
of <a class="xref" href="#migration-3to4" title="Figure 14. Migration from three chains to four chains">Figure 14, “Migration from three chains to four chains”</a>.  However, there’s a very tiny "hole" that is
punched in chain #2’s space that maps key hashes in the range of 0.5
to 0.500000000000001 to chain #4.</p></div></div><div class="section" title="9.5. Adding/Removing Client Nodes"><div class="titlepage"><div><div><h3 class="title"><a id="adding-removing-client-nodes"></a>9.5. Adding/Removing Client Nodes</h3></div></div></div><p>It is not strictly necessary to formally configure a list of all
Hibari client nodes that may use a Hibari cluster.  However,
practically speaking, it is useful to do so.</p><p>To bootstrap itself to be able to use Hibari servers, a Hibari client
must be able to:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
Communicate with other Erlang nodes in the cluster.
</li><li class="listitem">
Receive "global hash" information from the cluster’s Admin
   Server.
</li></ol></div><p>To solve both problems, the Admin Server maintains a list of Hibari
client nodes.  (Hibari server nodes do not need this mechanism.)  For
each client node, a monitor process on the Admin Server polls the node
to see if the <code class="literal">gdss</code> or <code class="literal">gdss_client</code> application is running.  If the
client node is running, then problem #1 (connecting to other nodes in
the cluster) is automatically solved by using <code class="literal">net_adm:ping/1</code>.
Problem #2 is solved by the client monitor calling
<code class="literal">brick_admin:spam_gh_to_all_nodes/0</code>.</p><p>The Admin Server’s client monitor runs approximately once per second,
so there may be a delay of up to a couple of seconds before a
newly-started Hibari client node is connected to the rest of the
cluster and has all of the table info required to start work.</p><p>When a client node goes down, an OTP alarm is raised until the client
is up and running again.</p><p>Two methods can be used to view and change the client node monitor
list:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Use the Admin Server’s HTTP service: follow the "Add/Delete a client
  node monitor" hyperlink at the bottom of the top-level page.
</li><li class="listitem"><p class="simpara">
Use the Erlang CLI to use these functions:
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<code class="literal">brick_admin:add_client_monitor/1</code>
</li><li class="listitem">
<code class="literal">brick_admin:delete_client_monitor/1</code>
</li><li class="listitem">
<code class="literal">brick_admin:get_client_monitor_list/0</code>
</li></ul></div></li></ul></div></div></div><div class="section" title="10. The Partition Detector Application"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="partition-detector"></a>10. The Partition Detector Application</h2></div></div></div><p>For multi-node Hibari deployments, Hibari includes a network
monitoring feature that watches for partitions within the cluster, and
attempts to minimize the database consequences of such partitions.
This Erlang/OTP application is called the Partition Detector.</p><p>You can configure the network monitoring feature in the <code class="literal">central.conf</code>
file.  See <a class="xref" href="#central-conf-parameters" title="7.2. Parameters in the central.conf File">Section 7.2, “Parameters in the <code class="literal">central.conf</code> File”</a> for details.</p><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>Use of this feature is mandatory for a multi-node Hibari
deployment to prevent data corruption in the event of a network
partition.  If you don’t care about data loss, then as an ancient
Roman might say, “Caveat emptor.”
Or in English, “Let the buyer beware.”</p></td></tr></table></div><p>For the network monitoring feature to work properly, you must first
set up two separate networks, Network A and Network B, that connect to
each of your Hibari physical bricks. The networks must be set up as
follows:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Network A and Network B must be physically separate networks, with
different IP and broadcast addresses. See the diagram below for a two
node cluster.
</li><li class="listitem">
Network A must be the network used for all Hibari data communications.
</li><li class="listitem">
Network A should have as few physical failure points as
possible. For example, a single switch or load balancer is preferable
to two switches cabled together.
</li><li class="listitem">
The separate Network B will be used to compare node heartbeat patterns.
</li></ul></div><div class="important" title="Important" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Important"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Important]" src="./images/icons/important.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>For the network partition monitor to work properly, your
network partition monitor configuration settings must match as closely
as possible.  Each Hibari physical brick must have unique IP addresses
on its two network interfaces (as required by all IP networks), but
all configurations must use the same IP subnets for the <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span>
networks, and all configurations must use the same network <span class="emphasis"><em>A</em></span>
tiebreaker.</p></td></tr></table></div><div class="figure"><a id="a-and-b-network-diagram"></a><p class="title"><b>Figure 15. Network <span class="emphasis"><em>A</em></span> and network <span class="emphasis"><em>B</em></span> diagram</b></p><div class="figure-contents"><a class="ulink" href="images/a-and-b-diagram.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="80%"><tr><td align="center"><img src="images/a-and-b-diagram.png" align="middle" width="100%" alt="images/a-and-b-diagram.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><div class="section" title="10.1. Partition Detector Heartbeats"><div class="titlepage"><div><div><h3 class="title"><a id="_partition_detector_heartbeats"></a>10.1. Partition Detector Heartbeats</h3></div></div></div><p>Through the partition monitoring application, Hibari nodes send
heartbeat messages to one another at the configurable
heartbeat_beacon_interval, and each node keeps track of heartbeat
history from each of the other nodes in the cluster. The heartbeats
are transmitted through both Network A and Network B. If node
<code class="literal">gdss1@machine1</code> detects that the incoming heartbeats from
<code class="literal">gdss1@machine2</code> are absent both on Network A and on Network B, then
<code class="literal">gdss1@machine2</code> might have a problem. If the incoming heartbeats from
<code class="literal">gdss1@machine2</code> fail on Network A but not on Network B, a partition on
Network A might be the cause. If heartbeats fail on Network B but not
Network A, then Network B might have a partition problem, but this is
less serious because Hibari data communication does not take place on
Network B.</p><p>Configurable timers on each Hibari node determine the interval at
which the absence of incoming heartbeats from another node is
considered a problem. If on node <code class="literal">gdss1@machine1</code> no heartbeat has been
received from <code class="literal">gdss1@machine2</code> for the duration of the configurable
<code class="literal">heartbeat_warning_interval</code>, then a warning message is
written to the application log of node <code class="literal">gdss1@machine1</code>. This warning
message can be triggered by missing heartbeats either on Network A or
on Network B; the warning message will indicate which node has not
been heard from, and over which network.</p></div><div class="section" title="10.2. Partition Detector’s Tiebreaker"><div class="titlepage"><div><div><h3 class="title"><a id="_partition_detector_8217_s_tiebreaker"></a>10.2. Partition Detector’s Tiebreaker</h3></div></div></div><p>If on node <code class="literal">gdss1@machine1</code> no heartbeat has been received from
<code class="literal">gdss1@machine2</code> via Network A for the duration of the configurable
<code class="literal">heartbeat_failure_interval</code>, and if during that period heartbeats
from <code class="literal">gdss1@machine2</code> continue to be received via Network B, then a
network partition is presumed to have occurred in Network A. In this
scenario, node <code class="literal">gdss1@machine1</code> will attempt to ping the configurable
<code class="literal">network_a_tiebreaker</code> address. If <code class="literal">gdss1@machine1</code> successfully pings
the tiebreaker address, then <code class="literal">gdss1@machine1</code> considers itself to be
on the "correct" side of the Network A partition, and it continues
running. If by contrast <code class="literal">gdss1@machine1</code> cannot successfully ping the
tiebreaker address, then <code class="literal">gdss1@machine1</code> considers itself to be on
the "wrong" side of the Network A partition and shuts itself
down. Meanwhile, comparable calculations and decisions are being made
by node <code class="literal">gdss1@machine2</code>.</p><p>In a scenario where the network monitoring application determines that
a partition has occurred on Network B — that is, heartbeats are received
through Network A but not through Network B — then warnings are written
to the Hibari nodes' application logs but no node is shut down.</p></div></div><div class="section" title="11. Backup and Disaster Recovery"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_backup_and_disaster_recovery"></a>11. Backup and Disaster Recovery</h2></div></div></div><div class="section" title="11.1. Backup and Recovery Software"><div class="titlepage"><div><div><h3 class="title"><a id="_backup_and_recovery_software"></a>11.1. Backup and Recovery Software</h3></div></div></div><p>At the time of writing, Hibari’s largest cluster deployment is:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Well over 50 physical bricks
</li><li class="listitem">
Well over 4TB of disk space per physical brick
</li><li class="listitem">
Single data center, operated by a telecom carrier and integrated
  with third-party monitoring and control software
</li></ul></div><p>If a backup were made of all data in the cluster, the biggest question
is, "Where would you store the backup?"  Given the cluster’s purpose
(real-time email/messaging services), the quality of the data center’s
physical and software infrastructures, the length of the Hibari chains
used for physical data redundancy, the business factors influencing
the choice not to deploy a "hot backup" data center, and other
factors, Gemini has not developed the backup and recovery software for
Hibari.  Gemini’s smaller Hibari deployments also resemble the largest
deployment.</p><p>However, we expect that backup and recovery software will be high
priorities for open source Hibari users.  Together with the open
source users and developers, we expect this software to be developed
relatively quickly.</p></div><div class="section" title="11.2. Disaster Recovery via Remote Data Centers"><div class="titlepage"><div><div><h3 class="title"><a id="_disaster_recovery_via_remote_data_centers"></a>11.2. Disaster Recovery via Remote Data Centers</h3></div></div></div><div class="section" title="11.2.1. Single Hibari cluster spanning two data centers"><div class="titlepage"><div><div><h4 class="title"><a id="_single_hibari_cluster_spanning_two_data_centers"></a>11.2.1. Single Hibari cluster spanning two data centers</h4></div></div></div><p>It is certainly possible to deploy a single Hibari cluster across two
(or more) data centers.  At the moment, however, there is only one way
of doing it: each chain of data replication must have a brick located
in each data center.</p><p>As a consequence of brick placement, it is mandatory that Hibari
clients pay the full round-trip latency penalty for each update.  See
<a class="xref" href="#diagram-write-path-3" title="Figure 3. Message flow in a chain for a key update">Figure 3, “Message flow in a chain for a key update”</a> for a diagram; the "head" and "tail"
bricks would be in separate data centers, using WAN network
connectivity between them.</p><p>For some applications, strong consistency is a higher priority than
low latency (both for writes and possibly for reads, if the client is
not co-located in the same data center as the chain’s tail brick).  In
those cases, such cross-data-center brick placement can make sense.</p><p>However, Hibari’s Admin Server cannot handle all failure scenarios,
especially when WAN connectivity is broken between data centers; more
programming work is required for the Admin Server to automate the
handling of all processes.  Furthermore, Hibari’s basic design cannot
tolerate network partitions well, see <a class="xref" href="#cap-theorem-and-hibari" title="1.6. The CAP Theorem and Hibari">Section 1.6, “The CAP Theorem and Hibari”</a>
and <a class="xref" href="#admin-server-and-network-partition" title="6.4. Admin Server and Network Partition">Section 6.4, “Admin Server and Network Partition”</a>.  If the Admin Server
were capable of handling WAN network partitions, it’s almost certain
that all Hibari nodes in one of the partitioned data centers would be
inactive.</p></div><div class="section" title="11.2.2. Multiple Hibari clusters, one per data center"><div class="titlepage"><div><div><h4 class="title"><a id="_multiple_hibari_clusters_one_per_data_center"></a>11.2.2. Multiple Hibari clusters, one per data center</h4></div></div></div><p>Conceptually, it’s possible to run multiple Hibari clusters, one per
data center.  However, Hibari does not have the software required for
WAN-scale replication.</p><p>In theory, such software isn’t too difficult to develop.  The tail
brick of each chain can maintain a log of recent updates to the
chain.  Those updates can be transmitted asynchronously across a WAN
to another Hibari cluster in a remote data center.  Such a scheme is
depicted in the figure below.</p><div class="figure"><a id="async-replication-try1"></a><p class="title"><b>Figure 16. A future scenario of asynchronous, cross-data-center Hibari replication</b></p><div class="figure-contents"><a class="ulink" href="images/async-replication-try1.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="80%"><tr><td align="center"><img src="images/async-replication-try1.png" align="middle" width="100%" alt="images/async-replication-try1.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><p>This kind of replication makes the most sense if "Data Center #1"
were in an active role and Data Center #2" were in a hot-standby
role.  In that case, there would never be a "Data Center #2 Client",
so there would be no problem of strong consistency violations by
clients accessing both Hibari clusters simultaneously.  The only
consistency problem would be one of durability: the replay of async
update logs every <code class="literal">N</code> seconds would mean that up to <code class="literal">N</code> seconds of
updates within "Data Center #1" could be lost.</p><p>However, if clients access both Hibari clusters simultaneously, then
Hibari’s strong consistency guarantee would be violated.  Some
applications can tolerate weakened consistency.  Other applications,
however, cannot.  For the those apps that must have strong
consistency, Hibari will require additional design and code.</p><div class="tip" title="Tip" style="margin-left: 0; margin-right: 10%;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="./images/icons/tip.png" /></td><th align="left"></th></tr><tr><td align="left" valign="top"><p>A keen-eyed reader will notice that <a class="xref" href="#async-replication-try1" title="Figure 16. A future scenario of asynchronous, cross-data-center Hibari replication">Figure 16, “A future scenario of asynchronous, cross-data-center Hibari replication”</a>
is not fully symmetric.  If clients in "Data Center #2" make updates
to the chain, then the same async update log maintenance and replay to
"Data Center #1" would also be necessary.</p></td></tr></table></div></div></div></div><div class="section" title="12. Hibari Application Logging"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_hibari_application_logging"></a>12. Hibari Application Logging</h2></div></div></div><p>The Hibari application log records application-related alerts,
warnings, and informational messages, as well as trace messages for
debugging. By default the application log is written to this file:</p><pre class="literallayout">&lt;GDSS_HOME&gt;/var/log/gdss-app.log</pre><div class="section" title="12.1. Format of the Hibari Application Log"><div class="titlepage"><div><div><h3 class="title"><a id="_format_of_the_hibari_application_log"></a>12.1. Format of the Hibari Application Log</h3></div></div></div><p>Each log entry in the Hibari application log is composed of these
fields in this order, with vertical bar delimitation:</p><pre class="literallayout">&lt;PID&gt;|&lt;&lt;ERLANGPID&gt;&gt;|&lt;DATETIME&gt;|&lt;MODULE&gt;|&lt;LEVEL&gt;|&lt;MESSAGECODE&gt;|&lt;MESSAGE&gt;</pre><p>This Hibari application log entry format is not configurable.  Each of
these application log entry fields is described in the table that
follows.  The “Position” column indicates the position of the field
within a log entry.</p><div class="informaltable"><table cellpadding="4px" style="border-collapse: collapse;border-top: 2px solid #527bbd; border-bottom: 2px solid #527bbd; border-left: 2px solid #527bbd; border-right: 2px solid #527bbd; "><colgroup><col /><col /><col /></colgroup><thead><tr><th style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"> Position </th><th style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"> Field </th><th style="border-bottom: 1px solid ; " align="left" valign="top"> Description</th></tr></thead><tbody><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>1</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p><code class="literal">&lt;PID&gt;</code></p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>System-assigned process identifier (PID) of the process that generated the log message.</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>2</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p><code class="literal">&lt;ERLANGPID&gt;</code></p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>Erlang process identifier.</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>3</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p><code class="literal">&lt;DATETIME&gt;</code></p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>Timestamp in format <code class="literal">%Y%m%d%H%M%S</code>, where <code class="literal">%Y</code> = four digit year; <code class="literal">%m</code> = two digit month; <code class="literal">%d</code> = two digit date; <code class="literal">%H</code> = two digit hour; <code class="literal">%M</code> = two digit minute; and <code class="literal">%S</code> = two digit seconds. For example, <code class="literal">20081103230123</code>.</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>4</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p><code class="literal">&lt;MODULE&gt;</code></p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>The internal component with which the message is associated. This field is set to a minimum length of 13 characters. If the module name is shorter than 13 characters, spaces will be appended to the module name so that the field reaches the 13 character minimum.</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>5</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p><code class="literal">&lt;LEVEL&gt;</code></p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>The severity level of the message. The level will be one of the following: <code class="literal">ALERT</code>, a condition requiring immediate correction; <code class="literal">WARNG</code>, a warning message, indicating a potential problem; <code class="literal">INFO</code>, an informational message indicating normal activity, and requiring no action; <code class="literal">DEBUG</code>, a highly granular, process-descriptive message potentially of use when debugging the application.</p></td></tr><tr><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p>6</p></td><td style="border-right: 1px solid ; border-bottom: 1px solid ; " align="center" valign="top"><p><code class="literal">&lt;MESSAGECODE&gt;</code></p></td><td style="border-bottom: 1px solid ; " align="left" valign="top"><p>Integer code assigned to all messages of severity level <code class="literal">INFO</code> or higher.  NOTE: This code is not yet defined in the Hibari open source release.</p></td></tr><tr><td style="border-right: 1px solid ; " align="center" valign="top"><p>7</p></td><td style="border-right: 1px solid ; " align="center" valign="top"><p><code class="literal">&lt;MESSAGE&gt;</code></p></td><td style="" align="left" valign="top"><p>The message itself, describing the event that has occurred.</p></td></tr></tbody></table></div></div><div class="section" title="12.2. Application Log Example"><div class="titlepage"><div><div><h3 class="title"><a id="_application_log_example"></a>12.2. Application Log Example</h3></div></div></div><p>Items written to the Hibari application log come from multiple sources:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The Hibari OTP application
</li><li class="listitem">
Other OTP applications bundled with Hibari
</li><li class="listitem">
Other OTP applications within the Erlang runtime system,
  e.g. <code class="literal">kernel</code> and <code class="literal">sasl</code>.
</li></ul></div><p>The <code class="literal">&lt;MESSAGE&gt;</code> field is free-form text.  Application code can freely
add newline characters and various white-space padding wherever it
wishes.  However, the file format dictates that a newline character
(ASCII 10) appear only at the end of the entire app log message.</p><p>The Hibari error logger must therefore reformat the text of the
<code class="literal">&lt;MESSAGE&gt;</code> field to remove newlines and to remove whitespace
padding.  The result is not nearly as readable as the formatting
presented to the Erlang shell.  For example, within the shell, a
message can look like this:</p><pre class="literallayout">=PROGRESS REPORT==== 12-Apr-2010::17:49:22 ===
          supervisor: {local,sasl_safe_sup}
             started: [{pid,&lt;0.43.0&gt;},
                       {name,alarm_handler},
                       {mfa,{alarm_handler,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,2000},
                       {child_type,worker}]</pre><p>Within the Hibari application log, however, the same message is
reformatted as line #2 below.  The reformatted version is much more
difficult for a human to read than the version above, but the purpose
of the app log file is to be machine-parsable, not human-parsable.</p><pre class="literallayout">8955|&lt;0.54.0&gt;|20100412174922|gmt_app      |INFO|2190301|start: normal []
8955|&lt;0.55.0&gt;|20100412174922|SASL         |INFO|2199999|progress: [{supervisor,{local,gmt_sup}},{started,[{pid,&lt;0.56.0&gt;},{name,gmt_config_svr},{mfa,{gmt_config_svr,start_link,["../priv/central.conf"]}},{restart_type,permanent},{shutdown,2000},{child_type,worker}]}]
8955|&lt;0.55.0&gt;|20100412174922|SASL         |INFO|2199999|progress: [{supervisor,{local,gmt_sup}},{started,[{pid,&lt;0.57.0&gt;},{name,gmt_tlog_svr},{mfa,{gmt_tlog_svr,start_link,[]}},{restart_type,permanent},{shutdown,2000},{child_type,worker}]}]
8955|&lt;0.36.0&gt;|20100412174922|SASL         |INFO|2199999|progress: [{supervisor,{local,kernel_safe_sup}},{started,[{pid,&lt;0.59.0&gt;},{name,timer_server},{mfa,{timer,start_link,[]}},{restart_type,permanent},{shutdown,1000},{child_type,worker}]}]
[...skipping ahead...]
8955|&lt;0.7.0&gt;|20100412174923|SASL         |INFO|2199999|progress: [{application,gdss},{started_at,gdss_dev2@bb3}]
8955|&lt;0.98.0&gt;|20100412174923|DEFAULT      |INFO|2199999|brick_sb: Admin Server not registered yet, retrying
8955|&lt;0.65.0&gt;|20100412174923|SASL         |INFO|2199999|progress: [{supervisor,{local,brick_admin_sup}},{started,[{pid,&lt;0.98.0&gt;},{name,brick_sb},{mfa,{brick_sb,start_link,[]}},{restart_type,permanent},{shutdown,2000},{child_type,worker}]}]
8955|&lt;0.105.0&gt;|20100412174924|DEFAULT      |INFO|2199999|top of init: bootstrap_copy1, [{implementation_module,brick_ets},{default_data_dir,"."}]
8955|&lt;0.105.0&gt;|20100412174924|DEFAULT      |INFO|2199999|do_init_second_half: bootstrap_copy1
8955|&lt;0.79.0&gt;|20100412174924|SASL         |INFO|2199999|progress: [{supervisor,{local,brick_brick_sup}},{started,[{pid,&lt;0.105.0&gt;},{name,bootstrap_copy1},{mfa,{brick_server,start_link,[bootstrap_copy1,[{default_data_dir,"."}]]}},{restart_type,temporary},{shutdown,2000},{child_type,worker}]}]
8955|&lt;0.105.0&gt;|20100412174924|DEFAULT      |INFO|2199999|do_init_second_half: bootstrap_copy1 finished</pre></div></div><div class="section" title="13. Hardware and Software Considerations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_hardware_and_software_considerations"></a>13. Hardware and Software Considerations</h2></div></div></div><p>As noted in <a class="xref" href="#hibari-origins" title="1.3. Hibari’s Origins">Section 1.3, “Hibari’s Origins”</a>, at the time of writing, Hibari has
been deployed exclusively in data centers run by telecom carriers.
All carriers have very specific requirements for integrating with its
existing deployment, network monitoring, alarm management, and other
infrastructures.  As a result, many of those features have been
omitted to date from Hibari.  With Hibari’s release into an "open
source environment", we expect that these gaps will be closed.</p><p>Hibari’s carrier-centric heritage has also influenced the types of
hardware, networking gear, operating system, support software, and
internal Hibari configuration that have been used successfully to
date.  Some of these practices will change as Hibari evolves from its
original use patterns.  Until then, this section discusses some of the
things that a systems/network administrator must consider when
deploying a Hibari cluster.</p><p>Similarly, application developers must be very familiar with these
same issues.  An unaware developer can create an application that uses
too many resources on under-specified hardware, causing problems for
developers, support staff, and application users alike.  We wish
Hibari to grow and flourish in its non-relational DB niche.</p><div class="section" title="13.1. Notes on Brick Hardware"><div class="titlepage"><div><div><h3 class="title"><a id="_notes_on_brick_hardware"></a>13.1. Notes on Brick Hardware</h3></div></div></div><div class="section" title="13.1.1. Lots of RAM is better"><div class="titlepage"><div><div><h4 class="title"><a id="_lots_of_ram_is_better"></a>13.1.1. Lots of RAM is better</h4></div></div></div><p>Each Hibari logical brick stores all information about its keys in
RAM.  Both the logical brick’s private write-ahead log and the common
write-ahead log are not "disk-based data structures" in the typical
sense, such as on-disk hash tables or B-trees.  Therefore, Hibari
bricks require a lot of RAM to function.</p><p>For more details, see:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="xref" href="#overview-high-performance" title="2.8. High performance">Section 2.8, “High performance”</a>
</li><li class="listitem">
<a class="xref" href="#per-table-config-perf-options" title="2.14. Per-table configurable performance options">Section 2.14, “Per-table configurable performance options”</a> … if a table stores its value
  blobs in RAM, it will consume more RAM than if those value blobs are
  stored on disk.
</li><li class="listitem">
<a class="xref" href="#hibari-data-model" title="4.2. The Hibari Data Model">Section 4.2, “The Hibari Data Model”</a>
</li><li class="listitem">
<a class="xref" href="#brick-init" title="8.4. Brick Initialization">Section 8.4, “Brick Initialization”</a>
</li></ul></div></div><div class="section" title="13.1.2. Lots of disk I/O capacity is better"><div class="titlepage"><div><div><h4 class="title"><a id="_lots_of_disk_i_o_capacity_is_better"></a>13.1.2. Lots of disk I/O capacity is better</h4></div></div></div><p>By default, Hibari will write and flush each update to disk before
sending a reply downstream or back to the client.  Hibari will perform
better on systems that have higher disk I/O capacity.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Non-volatile/battery-backed cache on the disk controller(s) is
  helpful, when combined with a write-back cache policy.  The more
  cache, the better.  If the read/write ratio of the cache can be
  changed, a default policy of 10/90 or 0/100 (i.e. skewed to writes)
  is typically more helpful than a default 50/50 split.
</li><li class="listitem">
On-disk (volatile) cache on individual disks is not helpful.
</li><li class="listitem">
Faster spinning disks are more helpful than slower spinning disks.
</li><li class="listitem">
If using RAID, a large stripe width of e.g. 512KBytes or 1024KBytes
  is usually more helpful than the (usually) smaller default stripe
  width on most controllers.
</li><li class="listitem">
If using RAID, a hardware RAID implementation may be very slightly
  helpful.
</li><li class="listitem">
RAID redundancy (e.g. RAID 1, 10, 5, 6) is not required by Hibari,
  but it can help reduce the odds of failure of an individual physical
  brick.  If physical bricks do not use data redundant RAID
  (e.g. RAID 0, concatenation), it’s a good idea to consider using
  longer replication chains to compensate.
</li></ul></div><p>For more details, see:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="xref" href="#the-physical-brick" title="5.1.1. The physical brick">Section 5.1.1, “The physical brick”</a>
</li><li class="listitem">
<a class="xref" href="#per-table-config-perf-options" title="2.14. Per-table configurable performance options">Section 2.14, “Per-table configurable performance options”</a>
</li><li class="listitem">
<a class="xref" href="#hibari-data-model" title="4.2. The Hibari Data Model">Section 4.2, “The Hibari Data Model”</a>
</li></ul></div></div><div class="section" title="13.1.3. High I/O rate devices (e.g. SSD) may be used"><div class="titlepage"><div><div><h4 class="title"><a id="high-io-rate-devices"></a>13.1.3. High I/O rate devices (e.g. SSD) may be used</h4></div></div></div><p>Hibari has some support for high I/O rate devices such as solid state
disks, flash memory disks, flash memory storage cards, et al.  There
is nothing in Hibari’s implementation that would preclude using
high-speed disk devices as the only storage for Hibari write-ahead
logs.</p><p>Hibari has a feature that can segregate high write I/O with <code class="literal">fsync(2)</code>
operations onto a separate high-speed device, and use cheaper &amp;
lower-speed Winchester disk devices for bulk storage.  This feature
has not yet been well-tested and optimized.</p><p>For more details, see:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="xref" href="#write-ahead-logs" title="5.2. Write-Ahead Logs">Section 5.2, “Write-Ahead Logs”</a>
</li><li class="listitem">
<a class="xref" href="#two-wal-types" title="5.2.2. Two types of write-ahead logs">Section 5.2.2, “Two types of write-ahead logs”</a>
</li></ul></div></div><div class="section" title="13.1.4. Lots of disk storage capacity may be a secondary concern"><div class="titlepage"><div><div><h4 class="title"><a id="_lots_of_disk_storage_capacity_may_be_a_secondary_concern"></a>13.1.4. Lots of disk storage capacity may be a secondary concern</h4></div></div></div><p>More disks of smaller capacity are almost always more helpful than a
few disks of larger capacity.  RAID 0 (no data redundancy) or RAID 10
("mirror" data redundancy) is useful for combining the I/O capacity of
multiple disks into a single logical volume.  Other RAID levels, such
as 5 or 6, can be used, though at the expense of higher write I/O
overhead.</p><p>For more details, see:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="xref" href="#write-ahead-logs" title="5.2. Write-Ahead Logs">Section 5.2, “Write-Ahead Logs”</a>
</li></ul></div></div><div class="section" title="13.1.5. Lots of CPU capacity is a secondary concern"><div class="titlepage"><div><div><h4 class="title"><a id="considerations-cpu"></a>13.1.5. Lots of CPU capacity is a secondary concern</h4></div></div></div><p>Hibari storage bricks do not, as a general rule, require large amounts
of CPU capacity.  The largest single source of CPU consumption is in
MD5 checksum calculation.  If the data objects most commonly written &amp;
read by your application are small, then multi-socket, multi-core CPUs
are not required.</p><p>Each Hibari logical brick is implemented within the Erlang virtual
machine as a single <code class="literal">gen_server</code> process.  Therefore, each logical
brick can (generally speaking) only fully utilize one CPU core.  If
your Hibari cluster appears to have CPU-utilization imbalance, then
the recommended strategy is to change the chain placement policy of
the chains.  For example, there are two methods for arranging a chain
of length three across three physical bricks:</p><p><a id="1-chain-striped-across-3-bricks"></a>The first example shows one chain striped across three physical
bricks.  If the read/write ratio for the chain is extremely high
(i.e. most operations are reads), then most of the CPU activity (and
perhaps disk I/O, if blobs are stored on disk) will be directed to the
"Chain 1 tail" brick and cause a CPU utilization imbalance.</p><div class="example"><a id="id428460"></a><p class="title"><b>Example 5. One chain striped across three physical bricks</b></p><div class="example-contents"><pre class="literallayout">| Physical Brick X | Physical Brick Y | Physical Brick Z |
----------------------------------------------------------
   Chain 1 head   -&gt;  Chain 1 middle -&gt;  Chain 1 tail</pre></div></div><br class="example-break" /><p><a id="3-chains-striped-across-3-bricks"></a>The second example shows the same three physical bricks but with three
chains striped across them.  In this example, each physical brick is
responsible for three different roles: head, middle, and tail.
Regardless of the read/write operation ratio, all bricks will utilize
roughly the same amount of CPU.</p><div class="example"><a id="id428485"></a><p class="title"><b>Example 6. Three chains striped across three physical bricks</b></p><div class="example-contents"><pre class="literallayout">| Physical Brick T | Physical Brick U | Physical Brick V |
----------------------------------------------------------
   Chain 1 head   -&gt;  Chain 1 middle -&gt;  Chain 1 tail   ||
   Chain 2 tail   ||  Chain 2 head   -&gt;  Chain 2 middle -&gt;
   Chain 3 middle -&gt;  Chain 3 tail   ||  Chain 3 head   -&gt;</pre></div></div><br class="example-break" /><p>In multi-CPU and multi-core systems, a side-effect of using more
chains (and therefore more bricks) is that the Erlang virtual machine
can schedule more logical brick computation across a larger number of
cores and CPUs.</p></div></div><div class="section" title="13.2. Notes on Networking"><div class="titlepage"><div><div><h3 class="title"><a id="_notes_on_networking"></a>13.2. Notes on Networking</h3></div></div></div><p>Hibari works quite well using commodity "Gigabit Ethernet" interfaces.
Lower latency (and higher cost) networking gear, such as Infiniband,
is not required.</p><p>For production use, it is <span class="emphasis"><em>strongly recommended</em></span> that all Hibari
servers be configured with two physical network interfaces, cabling,
switches, etc.  For more details, see:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="xref" href="#partition-detector" title="10. The Partition Detector Application">Section 10, “The Partition Detector Application”</a>
</li></ul></div><div class="section" title="13.2.1. Client protocol load balancing"><div class="titlepage"><div><div><h4 class="title"><a id="_client_protocol_load_balancing"></a>13.2.1. Client protocol load balancing</h4></div></div></div><p>The native Erlang client, via the <code class="literal">gdss</code> or <code class="literal">gdss_client</code> OTP
applications, do not require any load balancing.  The Erlang client
already is a participant in the consistent hashing algorithm (see
<a class="xref" href="#consistent-hashing-example" title="5.6.1. Partitioning by consistent hashing">Section 5.6.1, “Partitioning by consistent hashing”</a>).  The Admin Server distributes
updates to a table’s consistent hash map each time cluster membership
or chain/brick status changes.</p><p>All other client access protocols are "dumb", by comparison.  Take for
example the Amazon S3 protocol service.  There is no easy way for a
Hibari cluster to convey to a generic HTTP client how to calculate
which brick to send a query to.  The HTTP redirect mechanism could be
used for this purpose, but other protocols don’t have an equivalent
feature.  Also, the latency overhead of sending a redirect is far
higher than Hibari’s solution to this problem.</p><p>Hibari’s solution is simple: the Hibari server-side "dumb" protocol
handler uses the same native Erlang client that any other Hibari
client app written in Erlang users.  That client is capable of making
direct routing decisions.  Therefore, the "dumb" protocol handler
within a Hibari node acts as a translating proxy: it uses the "dumb"
client access protocol on one side and uses the native Erlang client
API on the other.</p><div class="figure"><a id="id428589"></a><p class="title"><b>Figure 17. Hibari "dumb" protocol proxy</b></p><div class="figure-contents"><a class="ulink" href="images/dumb-protocol-proxy.svg" target="_top">
  <div class="mediaobject" align="center"><table border="0" summary="manufactured viewport for HTML img" cellspacing="0" cellpadding="0" width="80%"><tr><td align="center"><img src="images/dumb-protocol-proxy.png" align="middle" width="100%" alt="images/dumb-protocol-proxy.svg" /></td></tr></table></div>
</a></div></div><br class="figure-break" /><p>The deployed "state of the art" for such dumb protocols is to use a
TCP load balancer (aka a "layer 4" load balancer) to spread dumb
client workload across multiple Hibari dumb protocol servers.</p></div></div><div class="section" title="13.3. Notes on Operating System"><div class="titlepage"><div><div><h3 class="title"><a id="_notes_on_operating_system"></a>13.3. Notes on Operating System</h3></div></div></div><p>Hibari servers operate on top of the Erlang virtual machine.  In
principle, any operating system that is supported by the Erlang
virtual machine can support Hibari.</p><div class="section" title="13.3.1. Supported Operating Systems"><div class="titlepage"><div><div><h4 class="title"><a id="_supported_operating_systems"></a>13.3.1. Supported Operating Systems</h4></div></div></div><p>In practice, Hibari is supported on the following operating systems:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Linux (used in production and QA environments within Gemini Mobile)
</li><li class="listitem">
Mac OS X
</li><li class="listitem">
FreeBSD (though not currently in a jail environment, due to some TCP
  services getting EPROTONOSUPPORT errors)
</li></ul></div><p>To take advantage of RAM larger than 3.0, 3.5, or 4.0 gigabytes
(depending on the OS kernel), we recommended that you use a 64-bit
version of your OS’s kernel, 64-bit versions of the user runtime, and
a 64-bit version of the Erlang/OTP runtime.</p></div><div class="section" title="13.3.2. OS Readahead Configuration"><div class="titlepage"><div><div><h4 class="title"><a id="os-readahead-configuration"></a>13.3.2. OS Readahead Configuration</h4></div></div></div><p>Some operating systems have support for OS-based "readahead":
pre-fetching blocks of a file with the expectation that those blocks
will soon be requested by the application.  Properly configured,
readahead can substantially raise throughput and reduce latency on
many read-heavy I/O workloads.</p><p>The read I/O workloads for Hibari fall into two major categories:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
Extremely predictable sequential read-only I/O during brick
   initialization (see <a class="xref" href="#brick-init" title="8.4. Brick Initialization">Section 8.4, “Brick Initialization”</a>).
</li><li class="listitem">
Extremely unpredictable random read I/O for fetching value blobs
   from disk.
</li></ol></div><p>The first I/O pattern can usually benefit a great deal from an aggressive
readahead policy.  However, an aggressive readahead policy can have
the opposite effect on the second I/O pattern.  Readahead policies
under Linux, for example, are defined on a per-block device basis and
does not change in response to application runtime behavior.</p><p>If your OS supports readahead policy configuration, we recommend using
a small read and then measuring its effect with a real or simulated
workload with the real Hibari server.</p></div><div class="section" title="13.3.3. Disk Scheduler Configuration"><div class="titlepage"><div><div><h4 class="title"><a id="disk-scheduler-configuration"></a>13.3.3. Disk Scheduler Configuration</h4></div></div></div><p>We recommend that you experiment with disk scheduler configuration on
relevant OSes such as Linux.  The "deadline" scheduler is likely to
provide better performance characteristics.</p></div></div><div class="section" title="13.4. Notes on Supporting Software"><div class="titlepage"><div><div><h3 class="title"><a id="_notes_on_supporting_software"></a>13.4. Notes on Supporting Software</h3></div></div></div><p>A typical "server" type installation of a Linux or FreeBSD OS is
sufficient for Hibari.  The following is an incomplete list of other
software packages that are necessary for Hibari’s installation and/or
runtime.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
NTP
</li><li class="listitem">
Erlang/OTP version R13B04
</li><li class="listitem">
Either "lynx" or "elinks", a text-based Web browser
</li></ul></div><div class="section" title="13.4.1. NTP configuration of all Hibari server and client nodes"><div class="titlepage"><div><div><h4 class="title"><a id="ntp-config-strongly-recommended"></a>13.4.1. NTP configuration of all Hibari server and client nodes</h4></div></div></div><p>It is strongly recommended that all Hibari server and client nodes
have the NTP daemon (Network Time Protocol) installed, properly
configured, and running.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
The <code class="literal">brick_simple</code> client API uses the OS clock for automatic
  generation of timestamps for each key update.  The application
  problems caused by badly out-of-sync OS clocks can be easily avoided
  by NTP.
</li><li class="listitem"><p class="simpara">
If a client’s clock is skewed by more than the
  <code class="literal">brick_do_op_too_old_timeout</code> configuration attribute in
  <code class="literal">central.conf</code> (units = milliseconds), then the brick will silently
  discard the client’s operation.  The only symptoms of this are:
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
Client-side timeouts when using the <code class="literal">brick_simple</code>, <code class="literal">brick_server</code>,
  or <code class="literal">brick_squorum</code> APIs.
</li><li class="listitem">
Increasing <code class="literal">n_too_old</code> statistic counter on the brick.
</li></ul></div></li></ul></div></div></div><div class="section" title="13.5. Notes on Hibari Configuration"><div class="titlepage"><div><div><h3 class="title"><a id="_notes_on_hibari_configuration"></a>13.5. Notes on Hibari Configuration</h3></div></div></div><p>There are several reasons why disk I/O rates can temporarily increase
within a Hibari physical brick:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Logical brick checkpoints for increased write I/O ops, see
  <a class="xref" href="#checkpoints" title="8.6. Brick Checkpoint Operations">Section 8.6, “Brick Checkpoint Operations”</a>
</li><li class="listitem">
The common log "scavenger" for increased read and write I/O ops,
  see <a class="xref" href="#scavenger" title="8.7. The Scavenger">Section 8.7, “The Scavenger”</a>
</li><li class="listitem"><p class="simpara">
Chain replication repair, see <a class="xref" href="#chain-repair" title="8.5. Chain Repair">Section 8.5, “Chain Repair”</a>
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
As the upstream/"repairer" brick, the extra read I/O ops,
   if the brick stores value blobs on disk
</li><li class="listitem">
As the downstream/"repairee" brick, extra write I/O ops
</li></ul></div></li></ul></div><p>The Hibari <code class="literal">central.conf</code> file contains parameters that can limit the
amount of disk bandwidth used by most of these operations.</p><p>See also:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="xref" href="#considerations-cpu" title="13.1.5. Lots of CPU capacity is a secondary concern">Section 13.1.5, “Lots of CPU capacity is a secondary concern”</a>
</li><li class="listitem">
<a class="xref" href="#central-conf-parameters" title="7.2. Parameters in the central.conf File">Section 7.2, “Parameters in the <code class="literal">central.conf</code> File”</a>
</li></ul></div></div><div class="section" title="13.6. Notes on Monitoring a Hibari Cluster"><div class="titlepage"><div><div><h3 class="title"><a id="_notes_on_monitoring_a_hibari_cluster"></a>13.6. Notes on Monitoring a Hibari Cluster</h3></div></div></div><p>The Admin Server’s status page contains current status information
regarding all tables, chains, and bricks in the cluster.  By default,
this service listens to TCP port 23080 and is reachable via HTTP at
<a class="ulink" href="http://any-hibari-node-name:23080/" target="_top">http://any-hibari-node-name:23080/</a>.  HTTP redirect will steer your
browser to the Admin Server node.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Hypertext links for each table, chain, and brick can show more
  detailed info on each entity.
</li><li class="listitem">
The "Dump History" link at the bottom of the Admin Server’s HTTP
  status page can show operations history across multiple bricks,
  chains, and/or tables by using the regular expression feature.
</li><li class="listitem"><p class="simpara">
Each logical brick maintains counters of each type of Hibari client
  op primitive.  At present, these stats are only exposed via the HTTP
  status server or by the native Erlang interface, but it’s possible
  to expose these stats via SNMP and other protocols in a
  straightforward manner.
</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
Stats include: number of <code class="literal">add</code>, <code class="literal">replace</code>, <code class="literal">set</code>, <code class="literal">get</code>,
  <code class="literal">get_many</code>, <code class="literal">delete</code>, and micro-transactions.
</li></ul></div></li></ul></div><div class="section" title="13.6.1. Hibari Admin Server HTTP status"><div class="titlepage"><div><div><h4 class="title"><a id="_hibari_admin_server_http_status"></a>13.6.1. Hibari Admin Server HTTP status</h4></div></div></div><p>For example screen shots of the Admin Server status pages (a work in
progress), see <a class="ulink" href="./misc-screenshots/admin-server-status/index.html" target="_top">./misc-screenshots/admin-server-status/index.html</a>.</p><p>See also:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="xref" href="#chain-lifecycle-fsm" title="8.2. Chain Lifecycle Finite State Machine">Section 8.2, “Chain Lifecycle Finite State Machine”</a>
</li><li class="listitem">
<a class="xref" href="#brick-lifecycle-fsm" title="8.1. Brick Lifecycle Finite State Machine">Section 8.1, “Brick Lifecycle Finite State Machine”</a>
</li></ul></div></div></div></div><div class="footnotes"><br /><hr width="100" align="left" /><div class="footnote"><p><sup>[<a id="ftn.id375188" href="#id375188" class="simpara">1</a>] </sup>The crucial detail is that such systems may not be "CA"
and "CP" at the same time: they can shift around the spectrum in reaction
to events such as network partition.</p></div></div></div></body></html>

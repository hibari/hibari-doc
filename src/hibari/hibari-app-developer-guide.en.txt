// -*- Doc -*-
// vim: set syntax=asciidoc:

//
// Copyright (c) 2005-2011 Gemini Mobile Technologies, Inc.  All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

= Hibari Application Developer's Guide *DRAFT - IN PROGRESS*
:Date: 2011/02/11
:Revision: 0.1.7
:Copyright: Copyright (C) 2006-2011 Gemini Mobile Technologies, Inc.  All rights reserved.

<<<
== Introduction
_Under Construction_

// - _Dave S. to write, drawing from various materials._
// - _Will solicit info and review from Dev and Marketing as needed._

=== Why NoSQL?
_to be added_

=== Why Hibari?
_to be added_

=== How Hibari Works

==== Data Model
_to be added_

==== Partitioning Across Chains
_to be added_

==== Replication Within Chains
_to be added_

==== RAM and Disk Storage
_to be added_

==== Admin Server
_to be added_


== Getting Started
_Under Construction_

// - _KK Dev to provide initial content._
// - _Dave S. to review, edit, and solicit additional info as needed._

=== Requirements

==== System Requirements
_to be added_

==== Required Software

- OpenSSL - http://www.openssl.org/
  * _required for Erlang's crypto module_

=== Downloading Hibari

A Hibari pre-built release has 2 files:

- a tarball package "hibari-X.Y.Z-DIST-ARCH-WORDSIZE.tgz"
- a md5sum file "hibari-X.Y.Z-DIST-ARCH-WORDSIZE-md5sum.txt"

_X.Y.Z_ is the release version, _DIST_ is the release distribution,
_ARCH_ is the release architecture, and _WORDSIZE_ is the release
wordsize.

Hibari pre-built releases are not available for download yet.  See
<<HibariBuildingSource>> for instructions to build Hibari from source
for your target platform.

=== Installing Hibari

==== Single Node

. Create directory for running Hibari
+
------
$ mkdir running-directory-name
------
+
. Untar release files
+
------
$ tar -C running-directory-name -xvf hibari-X.Y.Z-DIST-ARCH-WORDSIZE.tgz
------

==== Multi Node

Cluster is a simple tool for installing, configuring, and
bootstrapping a cluster of Hibari nodes. The cluster tool requires one
installer node and one or more target nodes for the Hibari cluster.
The installer node can be a different node or can be one of the target
nodes.  The cluster tool requires an installing user (that is you) and
it must be different than the user for Hibari on the target nodes.

NOTE: This tool should meet the needs of most users.  However, this
tool's "target node" recipe is currently Linux-centric (e.g. useradd,
userdel, ...).  Patches and contributions for other OS and platforms
are welcome.  For non-Linux deployments, the cluster tool is rather
simple so installation can be done manually by following the tool's
recipe.

Your Hibari installer node must have following tools/environments
ready.  For further information and help for related tools, please
refer to the following links:

- Bash - http://www.gnu.org/software/bash/
- Expect - http://www.nist.gov/el/msid/expect.cfm
- Git - http://git-scm.com/
  * *Git 1.5.4 or newer*
  * _required for GitHub_
- Perl - http://www.perl.org/
- SSH (client) - http://www.openssh.com/

Your Hibari target nodes must have following tools/environments ready.
For further information and help for related tools, please refer to
the following links:

- SSH (server) - http://www.openssh.com/

So far, there are no "known" version requirements for Bash, Expect,
Perl, and SSH.

===== Prerequisites and Assumptions

A. 1 installer node
   * Bash, Expect, Git, Perl, Python, and SSH (client) is installed
     on installer node
   * Your login account ($USER) exists on installer node with ssh
     private/public keys and ssh agent setup to enable password-less
     ssh login
   * /etc/hosts file on installer node contains entries for all
     target nodes
B. 1 or more cluster target nodes (e.g. dev1, dev2, dev3)
   * Your login account ($USER) exists on target nodes
   * Your login account ($USER) is enabled with password-less sudo
     access on target nodes
   * Your login account ($USER) is accessible with password-less ssh
     login on target nodes
   * SSH (server) is installed on target nodes
   * /etc/hosts file on target nodes contains entries for all target
     nodes
   * Network A and Network B is setup and active (see note below)
C. Cluster configuration file. This will show up as hibari.config in
   latter explanation.  You have to manually create it on installer
   node and later provide it's location as an input to the cluster
   tool.
   * Hibari Admin nodes
   * Hibari Brick nodes
   * Hibari Bricks per Chain value (i.e. replication factor)
   * All Hibari nodes (union of Admin and Brick nodes)
   * All Hibari nodes Network A and Network B ip addresses plus
     Network broadcast addresses and Network A tiebreaker address
   * All Heartbeat UDP ports
+
Example configuration file (hibari.config) for a three node cluster
that uses the same physical network for Network A and Network B (see
note below):
+
------
ADMIN_NODES=(dev1 dev2 dev3)
BRICK_NODES=(dev1 dev2 dev3)
BRICKS_PER_CHAIN=2

ALL_NODES=(dev1 dev2 dev3)
ALL_NETA_ADDRS=("10.181.165.230" "10.181.165.231" "10.181.165.232")
ALL_NETB_ADDRS=("10.181.165.230" "10.181.165.231" "10.181.165.232")
ALL_NETA_BCAST="10.181.165.255"
ALL_NETB_BCAST="10.181.165.255"
ALL_NETA_TIEBREAKER="10.181.165.1"

ALL_HEART_UDP_PORT="63099"
ALL_HEART_XMIT_UDP_PORT="63100"
------
+
Example /etc/hosts file entries for above configuration:
+
------
10.181.165.230  dev1.your-domain.com    dev1
10.181.165.231  dev2.your-domain.com    dev2
10.181.165.232  dev3.your-domain.com    dev3
------
+
NOTE: See
http://hibari.github.com/hibari-doc/hibari-sysadmin-guide.en.html#partition-detector
for further information regarding the network partition detector
application, Network A, and Network B.  Additional information for the
application's configuration is embedded in the partition-detector's
OTP application source file
(https://github.com/hibari/partition-detector/raw/master/src/partition_detector.app.src).
+
CAUTION: In a production setting, Network A and Network B should be
physically different networks and network interfaces.  However, the
same network can be used (as in this example) for Network A and
Network B for testing and development purposes.
+
CAUTION: Currently hostname must not contain "-" (minus).  If hostname
contain "-", like dev-1 for example, your Hibari cluster will not
startup. This will be fixed in future release and this sentence will
be deleted at that time. Thanks for your patience.

===== Example how to prepare installing user

Setup your user (i.e. your login - $USER) on all Hibari nodes if not
already existing.  This user will only be used for Hibari installation
purposes.

1. As root user, add your user to all of the Hibari nodes and grant
   sudo access for your user.
+
------
$ useradd $USER
$ passwd $USER
$ visudo
# append the following line and save it
$USER  ALL=(ALL)       NOPASSWD: ALL
------
+
NOTE: If you get "sudo: sorry, you must have a tty to run sudo" error
while testing 'sudo', consider to comment out following line inside of
the /etc/sudoers file:
+
------
$ visudo
Defaults    requiretty
------
+
2. Create a new ssh private/public key for your user on the installer
   node.
+
------
$ ssh-keygen
# enter your password for the private key
$ eval `ssh-agent`
$ ssh-add ~/.ssh/id_rsa
# re-enter your password for the private key
------
+
3. Append an entry for the installer node to your \~/.ssh/known_hosts
   file on each of the Hibari nodes and append an entry to your
   ~/.ssh/authorized_keys file on all of the Hibari nodes for your
   public ssh key.
+
------
$ ssh-copy-id -i ~/.ssh/id_rsa.pub $USER@dev1
$ ssh-copy-id -i ~/.ssh/id_rsa.pub $USER@dev2
$ ssh-copy-id -i ~/.ssh/id_rsa.pub $USER@dev3
------
+
NOTE: If your installer node will be one of the Hibari cluster nodes,
make sure that you ssh-copy-id to the installer node also.
+
4. Confirm password-less access to the each of the Hibari nodes works
as expected.
+
------
$ ssh $USER@dev1
$ ssh $USER@dev2
$ ssh $USER@dev3
------

TIP: If needed, check
http://inside.mines.edu/~gmurray/HowTo/sshNotes.html for further SSH
setup help.

===== Example how to prepare installer node

1. Configure your e-mail and name for Git
+
------
$ git config --global user.email "you@example.com"
$ git config --global user.name "Your Name"
------
+
2. Create working directory
+
------
$ mkdir working-directory-name
------
+
3. Download cluster tool's Git repository
+
------
$ cd working-directory-name
$ git clone git://github.com/hibari/clus.git
------
+
4. Place a copy of the Hibari pre-built release and your hibari.config
   file into the working directory.
+
------
$ cd working-directory-name
$ ls -1
clus
hibari-X.Y.Z-DIST-ARCH-WORDSIZE-md5sum.txt
hibari-X.Y.Z-DIST-ARCH-WORDSIZE.tgz
hibari.config
$
------

===== Example how to create all Hibari nodes

All of the operations below are run on the installer node via two Bash
scripts (i.e. clus.sh and clus-hibari.sh).

1. Create (or re-create) "hibari" user on all Hibari nodes
+
NOTE: If your ssh private key is protected by a password, please make
sure your private key is registered with the ssh agent before
proceeding.
+
------
$ cd working-directory-name
$ for i in dev1 dev2 dev3 ; do ./clus/priv/clus.sh -f init hibari $i ; done
hibari@dev1
hibari@dev2
hibari@dev3
------
+
CAUTION: The -f option will forcefully delete and then re-create
the "hibari" user on the target node.
+
2. Copy pre-built release to all Hibari nodes and then setup Hibari
   package on all Hibari nodes via the "hibari" user.
+
------
$ cd working-directory-name
$ ./clus/priv/clus-hibari.sh -f init hibari hibari.config hibari-X.Y.Z-DIST-ARCH-WORDSIZE.tgz
hibari@dev1
hibari@dev2
hibari@dev3
------

TIP: As described in the next section, the clus-hibari.sh script can
be used for starting and stopping of a Hibari multi node cluster even
after it's creation.

=== Deploying a Simple Hibari System

==== Single Node

===== Basic Configuration
_to be added_

===== Starting a Single Node Cluster

. Start Hibari
+
------
$ running-directory-name/hibari/bin/hibari start
------
+
. Bootstrap Hibari
+
------
$ running-directory-name/hibari/bin/hibari-admin bootstrap
------
+
The Hibari bootstrap process starts Hibari's Admin server on the
single node and creates a single table "tab1" serving as Hibari's
default table.

===== Verifying Your System

A few simple operations to verify system is set up properly.

. Open "Hibari Web Administration" page
+
------
$ your-favorite-browser http://127.0.0.1:23080
------
+
. Ping node to check the health.
+
------
$ running-directory-name/hibari/bin/hibari ping
------

_to be added_

===== Creating Other Tables
_to be added_

===== Stopping a Single Node Cluster

. Stop Hibari
+
------
$ running-directory-name/hibari/bin/hibari stop
------

==== Multi Node

===== Basic Configuration
_to be added_

===== Starting a Multi Node Cluster

. Start Hibari on all Hibari nodes via the "hibari" user
+
------
$ cd working-directory-name
$ ./clus/priv/clus-hibari.sh -f start hibari hibari.config
hibari@dev1
hibari@dev2
hibari@dev3
------
+
. Bootstrap Hibari on first Hibari admin node via the "hibari" user
+
------
$ cd working-directory-name
$ ./clus/priv/clus-hibari.sh -f bootstrap hibari hibari.config
hibari@dev1 => hibari@dev1 hibari@dev2 hibari@dev3
------
+
The Hibari bootstrap process starts Hibari's admin server on the first
admin node and creates a single table "tab1" serving as Hibari's
default table.
+
NOTE: If bootstrapping fails due to "another_admin_server_running"
error, please stop the other Hibari cluster(s) running on the network
or repeat the installation from the beginning with udp ports
(i.e. ALL_HEART_UDP_PORT and ALL_HEART_XMIT_UDP_PORT) that are not in
use by other applications or another Hibari cluster.

===== Verifying Your System

A few simple operations to verify system is set up properly.

. Open "Hibari Web Administration" page
+
------
$ your-favorite-browser http://dev1:23080
------
+
. Ping each of the nodes to check the health.
+
------
$ cd working-directory-name
$ ./clus/priv/clus-hibari.sh -f ping hibari hibari.config
hibari@dev1 ... pong
hibari@dev2 ... pong
hibari@dev3 ... pong
------

_to be added_

===== Creating Other Tables
_to be added_

===== Stopping a Multi Node Cluster

. Stop Hibari on all Hibari nodes via the "hibari" user
+
------
$ cd working-directory-name
$ ./clus/priv/clus-hibari.sh -f stop hibari hibari.config
ok
ok
ok
hibari@dev1
hibari@dev2
hibari@dev3
------

[[client-api-erlang]]
== Client API: Native Erlang

=== Overview

As a key-value database, Hibari provides a simple client API with primitive operations for inserting, retrieving, and deleting data. Within certain restrictions, the API also supports compound operations that optionally can be executed as all-or-nothing transactions.

More specifically, Hibari's client API supports the operations listed below. For details on the native Erlang API for each operation, follow the links.

*Data Insertion*

 - Add a key-value pair that does not yet exist, along with optional flags:
  * link:#brick-simple-add[brick_simple:add/6]
 - Assign a new value and/or new flags to a key that already exists:
  * link:#brick-simple-replace[brick_simple:replace/6]
 - Set a key-value pair and optional flags regardless of whether the key yet exists:
  * link:#brick-simple-set[brick_simple:set/6]

*Data Retrieval*

 - Retrieve a key and optionally its associated value and flags:
  * link:#brick-simple-get[brick_simple:get/4]
 - Retrieve multiple lexicographically contiguous keys and optionally their associated values and flags:
  * link:#brick-simple-get-many[brick_simple:get_many/5]

*Data Deletion*

 - Delete a key-value pair and associated flags:
  * link:#brick-simple-delete[brick_simple:delete/4]

*Compound Operations*

 - Execute a specified list of operations, optionally as an all-or-nothing transaction:
  * link:#brick-simple-do[brick_simple:do/4]

*Fold Operations*

 - Implement a fold operation across all keys in a table:
  * link:#brick-simple-fold-table[brick_simple:fold_table/7]
 - Implement a fold operation across all keys having a specified prefix:
  * link:#brick-simple-fold-key[brick_simple:fold_key_prefix/9]

If desired, clients can apply a "test 'n set" logic to data insertion, retrieval, and deletion operations so that the operation will be executed only if the target key has the exact timestamp specified in the request.

==== Erlang Basic Data Types

The following provides a high level introduction to Erlang basic data types that are referenced in this chapter. This material is excerpted with minor modifications from the http://www.erlang.org/doc/reference_manual/data_types.html[official Erlang documentation on data types]. For further information, see the official Erlang documentation.

IMPORTANT: All Erlang commands must conclude with a period (.).

Term:: A piece of data of any data type is called a *term*.

Number:: There are two types of numeric literals, *integers* and *floats*.

Atom:: An *atom* is a literal, a constant with name. An atom should be enclosed in single quotes (') if it does not begin with a lower-case letter or if it contains other characters than alphanumeric characters, underscore (_), or @. Optionally, any atom can be enclosed in single quotes. For example:
+
-----
hello
phone_number
'Monday'
'phone number'
'hello'
'phone_number'
-----

Bit String and Binary:: A *bit string* is used to store an area of untyped memory. Bit strings are expressed using http://www.erlang.org/doc/reference_manual/expressions.html#bit_syntax[Erlang bit syntax]. A bit string that consists of a number of bits that is evenly divisible by eight is called a *binary*. For example:
+
-----
<<10,20>>
<<"ABC">>
-----

Tuple:: A *tuple* is a compound data type with a fixed number of terms, enclosed by braces:
+
-----
{Term1,...,TermN}
-----

List:: A *list* is a compound data type with a variable number of terms, enclosed by square brackets:
+
-----
[Term1,...,TermN]
-----

String:: Strings are enclosed in double quotes ("), but are not a true data type in Erlang. Instead a string "hello" is shorthand for the list [$h,$e,$l,$l,$o], that is [104,101,108,108,111].

Boolean:: There is no Boolean data type in Erlang. Instead the atoms `true` and `false` are used to denote Boolean values.


[[brick-simple-add]]
=== brick_simple:add/6

SYNOPSIS

 ::
*brick_simple:add(Tab, Key, Value, ExpTime, Flags, Timeout).*

DESCRIPTION

 ::
Add `Key` and `Value` pair (and optional `Flags`) to the table `Tab` if the key does not already exist.  The operation will fail if `Key` already exists.

PARAMETERS

 ::
*Tab*

- Name of the table to which to add the key-value pair.
- Mandatory.
- Type:
  * `Tab = table()`
  * `table() = atom()`

 ::
*Key*

- Key to add to the table, in association with a paired value.
- Mandatory.
- Type:
  * `Key = key()`
  * `key() = iodata()`
  * `iodata() = iolist() | binary()`
  * `iolist()  = [char() | binary() | iolist()]`
+
NOTE: While the `Key` may be specified as either `iolist()` or `binary()`, it will be converted into binary before operation execution. The same is true of `Value`.
+

 ::
*Value*

- Value to associate with the key.
- Mandatory.
- Type:
  * `Value = val()`
  * `val() = iodata()`
  * `iodata() = iolist() | binary()`
  * `iolist()  = [char() | binary() | iolist()]`

 ::
*ExpTime*

- Time at which the key will expire, expressed as a Unix time_t().
- Optional; defaults to 0 (no expiration).
- Type:
  * `ExpTime = exp_time()`
  * `exp_time() = time_t()`
  * `time_t() = integer()`

 ::
*Flags*

- List of operational flags to apply to the `add' operation, and/or custom property flags to associate with the key-value pair in the database. Heavy use of custom property flags is discouraged due to RAM-based storage.
- Optional; defaults to empty list.
- Type:
  * `Flags = flags_list()`
  * `flags_list() = [do_op_flag() | property()]`
  * `do_op_flag() = 'value_in_ram'`
  * `property() = atom() | {term(), term()}`
- Operational flag usage
  * `'value_in_ram'`
    ** Store the value blob in RAM, overriding the default storage location of the brick.
+
NOTE: This flag has not yet been extensively tested by Gemini QA.
+

 ::
*Timeout*

- Operation timeout in milliseconds.
- Optional; defaults to 15000.
- Type:
  * `Timeout = timeout()`
  * `timeout() = integer() | 'infinity'`

RETURNS

 ::
Success return

- `'ok'`

 ::
Error returns

- `{'key_exists',timestamp()}`
  * The operation failed because the key already exists.
  * `timestamp() = integer()`
- `'invalid_flag_present'`
  * The operation failed because an invalid `do_op_flag()` was found in the `Flags` argument.
- `'brick_not_available'`
  * The operation failed because the chain that is responsible for this key is currently length zero and therefore unavailable.
- `{{'nodedown',node()},{'gen_server','call',term()}}`
  * The operation failed because the server brick handling the request has crashed or else a network partition has occurred between the client and server. The client should resend the query after a short delay, on the assumption that the Admin Server will have detected the failure and taken steps to repair the chain.
  * `node() = atom()`

ALIASES

 ::
- brick_simple:add/3
  * `brick_simple:add(Tab, Key, Value).`
- brick_simple:add/4
  * `brick_simple:add(Tab, Key, Value, Flags).`
  * `brick_simple:add(Tab, Key, Value, Timeout).`

EXAMPLES

 ::
Successful adding of a new key-value pair:
+
------
> brick_simple:add(tab1, <<"foo">>, <<"Hello, world!">>).
ok
------
+

 ::
Failed attempt to add a key that already exists:
+
------
> brick_simple:add(tab1, <<"foo">>, <<"Goodbye, world!">>).
{key_exists,1271542959131192}
------
+

 ::
Successful adding of a new key-value pair, with value to be stored in RAM regardless of brick's default storage setting:
+
------
> brick_simple:add(tab1, "foo1", "this is value1", ['value_in_ram']).
ok
------
+

 ::
Successful adding of a new key-value pair, using a non-default operation timeout:
+
------
> brick_simple:add(tab1, "foo2", "this is value2", 20000).
ok
------

[[brick-simple-replace]]
=== brick_simple:replace/6

SYNOPSIS

 ::
*brick_simple:replace(Tab, Key, Value, ExpTime, Flags, Timeout).*

DESCRIPTION

 ::
Replace `Key` and `Value` pair (and optional `Flags`) in the table `Tab` if the key already exists.  The operation will fail if `Key` does not already exist.

PARAMETERS

 ::
*Tab*

- Name of the table in which to replace the key-value pair.
- Mandatory.
- Type:
  * `Tab = table()`
  * `table() = atom()`

 ::
*Key*

- Key to replace in the table, in association with a new paired value.
- Mandatory.
- Type:
  * `Key = key()`
  * `key() = iodata()`
  * `iodata() = iolist() | binary()`
  * `iolist()  = [char() | binary() | iolist()]`
+
NOTE: While the `Key` may be specified as either `iolist()` or `binary()`, it will be converted into binary before operation execution. The same is true of `Value`.
+

 ::
*Value*

- New value to associate with the key.
- Mandatory.
- Type:
  * `Value = val()`
  * `val() = iodata()`
  * `iodata() = iolist() | binary()`
  * `iolist()  = [char() | binary() | iolist()]`

 ::
*ExpTime*

- Time at which the key will expire, expressed as a Unix time_t().
- Optional; defaults to 0 (no expiration).
- Type:
  * `ExpTime = exp_time()`
  * `exp_time() = time_t()`
  * `time_t() = integer()`

 ::
*Flags*

- List of operational flags to apply to the `replace' operation, and/or custom property flags to associate with the key-value pair in the database. Heavy use of custom property flags is discouraged due to RAM-based storage.
- Optional; defaults to empty list.
- Type:
  * `Flags = flags_list()`
  * `flags_list() = [do_op_flag() | property()]`
  * `do_op_flag() = {'testset', timestamp()} |'value_in_ram'`
  * `timestamp() = integer()`
  * `property() = atom() | {term(), term()}`
- Operational flag usage
  * `{'testset', timestamp()}`
    ** Fail the operation if the existing key's timestamp is not exactly equal to `timestamp()`.  If used inside a link:#brick-simple-do[micro-transaction], abort the transaction if the key's timestamp is not exactly equal to `timestamp()`.
  * `'value_in_ram'`
    ** Store the value blob in RAM, overriding the default storage location of the brick.
+
NOTE: This flag has not yet been extensively tested by Gemini QA.
+

 ::
*Timeout*

- Operation timeout in milliseconds.
- Optional; defaults to 15000.
- Type:
  * `Timeout = timeout()`
  * `timeout() = integer() | 'infinity'`

RETURNS

 ::
Success return

- `'ok'`

 ::
Error returns

- `'key_not_exist'`
  * The operation failed because the key does not exist.
- `{'ts_error', timestamp()}`
  * The operation failed because the `{'testset', timestamp()}` flag was used and there was a timestamp mismatch. The `timestamp()` in the return is the current value of the existing key's timestamp.
  * `timestamp() = integer()`
- `'invalid_flag_present'`
  * The operation failed because an invalid `do_op_flag()` was found in the `Flags` argument.
- `'brick_not_available'`
  * The operation failed because the chain that is responsible for this key is currently length zero and therefore unavailable.
- `{{'nodedown',node()},{'gen_server','call',term()}}`
  * The operation failed because the server brick handling the request has crashed or else a network partition has occurred between the client and server. The client should resend the query after a short delay, on the assumption that the Admin Server will have detected the failure and taken steps to repair the chain.
  * `node() = atom()`

ALIASES

 ::
- brick_simple:replace/3
  * `brick_simple:replace(Tab, Key, Value).`
- brick_simple:replace/4
  * `brick_simple:replace(Tab, Key, Value, Flags).`
  * `brick_simple:replace(Tab, Key, Value, Timeout).`

EXAMPLES

 ::
Successful replacement of a key-value pair:
+
------
> brick_simple:replace(tab1, <<"foo">>, <<"Goodbye, world!">>).
ok
------
+

 ::
Failed attempt to replace a key that does not yet exist:
+
------
> brick_simple:replace(tab1, <<"key3">>, <<"new and improved value">>).
key_not_exist
------
+

 ::
Successful replacement of a key-value pair, with value to be stored in RAM regardless of brick's default storage setting:
+
------
> brick_simple:replace(tab1, "foo", "You again, world!", ['value_in_ram']).
ok
------
+

 ::
Failed attempt to replace a key for which we have incorrectly specified its current timestamp:
+
------
> brick_simple:replace(tab1, "foo", "Whole new value", [{'testset', 12345}]).
{ts_error,1271543165272987}
------
+

 ::
Successful replacement of a key-value pair for which we have correctly specified its current timestamp:
+
------
> brick_simple:replace(tab1, "foo", "Whole new value", [{'testset', 1271543165272987}]).
ok
------
+

 ::
Successful replacement of a key-value pair, using a non-default operation timeout:
+
------
> brick_simple:replace(tab1, "foo", "Foo again?", 30000).
ok
------

[[brick-simple-set]]
=== brick_simple:set/6

SYNOPSIS

 ::
*brick_simple:set(Tab, Key, Value, ExpTime, Flags, Timeout).*

DESCRIPTION

 ::
Set `Key` and `Value` pair (and optional `Flags`) in the table `Tab`, regardless of whether or not the key already exists.

PARAMETERS

 ::
*Tab*

- Name of the table in which to set the key-value pair.
- Mandatory.
- Type:
  * `Tab = table()`
  * `table() = atom()`

 ::
*Key*

- Key to set in the table, in association with a paired value.
- Mandatory.
- Type:
  * `Key = key()`
  * `key() = iodata()`
  * `iodata() = iolist() | binary()`
  * `iolist()  = [char() | binary() | iolist()]`
+
NOTE: While the `Key` may be specified as either `iolist()` or `binary()`, it will be converted into binary before operation execution. The same is true of `Value`.
+

 ::
*Value*

- Value to associate with the key.
- Mandatory.
- Type:
  * `Value = val()`
  * `val() = iodata()`
  * `iodata() = iolist() | binary()`
  * `iolist()  = [char() | binary() | iolist()]`

 ::
*ExpTime*

- Time at which the key will expire, expressed as a Unix time_t().
- Optional; defaults to 0 (no expiration).
- Type:
  * `ExpTime = exp_time()`
  * `exp_time() = time_t()`
  * `time_t() = integer()`

 ::
*Flags*

- List of operational flags to apply to the `set' operation, and/or custom property flags to associate with the key-value pair in the database. Heavy use of custom property flags is discouraged due to RAM-based storage.
- Optional; defaults to empty list.
- Type:
  * `Flags = flags_list()`
  * `flags_list() = [do_op_flag() | property()]`
  * `do_op_flag() = {'testset', timestamp()} |'value_in_ram'`
  * `timestamp() = integer()`
  * `property() = atom() | {term(), term()}`
- Operational flag usage
  * `{'testset', timestamp()}`
    ** Fail the operation if the existing key's timestamp is not exactly equal to `timestamp()`.  If used inside a link:#brick-simple-do[micro-transaction], abort the transaction if the key's timestamp is not exactly equal to `timestamp()`. Using this flag with `set` will result in an error if the key does not already exist or if the key exists but has a non-matching timestamp.
  * `'value_in_ram'`
    ** Store the value blob in RAM, overriding the default storage location of the brick.
+
NOTE: This flag has not yet been extensively tested by Gemini QA.
+

 ::
*Timeout*

- Operation timeout in milliseconds.
- Optional; defaults to 15000.
- Type:
  * `Timeout = timeout()`
  * `timeout() = integer() | 'infinity'`

RETURNS

 ::
Success return

- `'ok'`

 ::
Error returns

- `'key_not_exist'`
  * The operation failed because the `{'testset', timestamp()}` flag was used and the key does not exist.
- `{'ts_error', timestamp()}`
  * The operation failed because the `{'testset', timestamp()}` flag was used and there was a timestamp mismatch. The `timestamp()` in the return is the current value of the existing key's timestamp.
  * `timestamp() = integer()`
- `'invalid_flag_present'`
  * The operation failed because an invalid `do_op_flag()` was found in the `Flags` argument.
- `'brick_not_available'`
  * The operation failed because the chain that is responsible for this key is currently length zero and therefore unavailable.
- `{{'nodedown',node()},{'gen_server','call',term()}}`
  * The operation failed because the server brick handling the request has crashed or else a network partition has occurred between the client and server. The client should resend the query after a short delay, on the assumption that the Admin Server will have detected the failure and taken steps to repair the chain.
  * `node() = atom()`

ALIASES

 ::
- brick_simple:set/3
  * `brick_simple:set(Tab, Key, Value).`
- brick_simple:set/4
  * `brick_simple:set(Tab, Key, Value, Flags).`
  * `brick_simple:set(Tab, Key, Value, Timeout).`

EXAMPLES

 ::
Successful setting of a key-value pair:
+
------
> brick_simple:set(tab1, <<"key4">>, <<"cool value">>).
ok
------
+

 ::
Successful setting of a key-value pair, with value to be stored in RAM regardless of brick's default storage setting:
+
------
> brick_simple:set(tab1, "goo", "value6", ['value_in_ram']).
ok
------
+

 ::
Failed attempt to set a key-value pair, when we have used the `testset` flag but the key does not yet exist:
+
------
> brick_simple:set(tab1, "boo", "hoo", [{'testset', 1271543165272987}]).
key_not_exist
------
+

 ::
Successful setting of a key-value pair, when we have used the `testset` flag and the key does already exist and its timestamp matches our specified timestamp:
+
------
> brick_simple:set(tab1, "goo", "value7", [{'testset', 1271543165272432}]).
ok
------

[[brick-simple-get]]
=== brick_simple:get/4

SYNOPSIS

 ::
*brick_simple:get(Tab, Key, Flags, Timeout).*

DESCRIPTION

 ::
From table `Tab`, retrieve `Key` and specified attributes of the key (as determined by `Flags`).

PARAMETERS

 ::
*Tab*

- Name of the table from which to retrieve the key.
- Mandatory.
- Type:
  * `Tab = table()`
  * `table() = atom()`

 ::
*Key*

- Key to retrieve from the table.
- Mandatory.
- Type:
  * `Key = key()`
  * `key() = iodata()`
  * `iodata() = iolist() | binary()`
  * `iolist()  = [char() | binary() | iolist()]`
+
NOTE: While the `Key` may be specified as either `iolist()` or `binary()`, it will be converted into binary before operation execution.
+

 ::
*Flags*

- List of operational flags to apply to the `get' operation, and/or custom property flags.
- Optional; defaults to empty list.
- Type:
  * `Flags = flags_list()`
  * `flags_list() = [do_op_flag() | property()]`
  * `do_op_flag() = 'get_all_attribs' | 'witness' | {'testset', timestamp()} | 'must_exist' | 'must_not_exist'`
  * `timestamp() = integer()`
  * `property() = atom() | {term(), term()}`
- Operational flag usage
  * `'get_all_attribs'`
    ** Return all attributes of the key. May be used in combination with the `witness` flag.
  * `'witness'`
    ** Do not return the value blob in the result. This flag will guarantee that the brick does not require disk access to satisfy this request.
  * `{'testset', timestamp()}`
    ** Fail the operation if the key's timestamp is not exactly equal to `timestamp()`. If used inside a link:#brick-simple-do[micro-transaction], abort the transaction if the key's timestamp is not exactly equal to `timestamp()`.
  * `'must_exist'`
    ** For use inside a link:#brick-simple-do[micro-transaction]: abort the transaction if the key does not exist.
  * `'must_not_exist'`
    ** For use inside a link:#brick-simple-do[micro-transaction]: abort the transaction if the key exists.

 ::
*Timeout*

- Operation timeout in milliseconds.
- Optional; defaults to 15000.
- Type:
  * `Timeout = timeout()`
  * `timeout() = integer() | 'infinity'`

RETURNS

 ::
Success returns

- `{'ok', timestamp(), val()}`
  * Success return when the get request uses neither the `'witness'` flag nor the `'get_all_attribs'` flag.
  * `timestamp() = integer()`
  * `val() = iodata()`
  * `iodata() = iolist() | binary()`
  * `iolist()  = [char() | binary() | iolist()]`
- `{'ok', timestamp()}`
  * Success return when the get uses `'witness'` but not `'get_all_attribs'`.
- `{'ok', timestamp(), proplist()}`
  * Success return when the get uses both `'witness'` and `'get_all_attribs'`.
  * `proplist() = [property()]`
  * `property() = atom() | {term(), term()}`
- `{'ok', timestamp(), val(), exp_time(), proplist()}`
  * Success return when the get uses `'get_all_attribs'` but not `'witness'`.
  * `exp_time() = time_t()`
+
NOTE: When a `proplist()` is returned, one of the properties in the list will always be `{val_len,Size::integer()}`, where `Size` is the size of the value blob in bytes.
+

 ::
Error returns

- `'key_not_exist'`
  * The operation failed because the key does not exist.
- `{'ts_error', timestamp()}`
  * The operation failed because the `{'testset', timestamp()}` flag was used and there was a timestamp mismatch. The `timestamp()` in the return is the current value of the existing key's timestamp.
- `'invalid_flag_present'`
  * The operation failed because an invalid `do_op_flag()` was found in the `Flags` argument.
- `'brick_not_available'`
  * The operation failed because the chain that is responsible for this key is currently length zero and therefore unavailable.
- `{{'nodedown',node()},{'gen_server','call',term()}}`
  * The operation failed because the server brick handling the request has crashed or else a network partition has occurred between the client and server. The client should resend the query after a short delay, on the assumption that the Admin Server will have detected the failure and taken steps to repair the chain.
  * `node() = atom()`

ALIASES

 ::
- brick_simple:get/2
  * `brick_simple:get(Tab, Key).`
- brick_simple:get/3
  * `brick_simple:get(Tab, Key, Flags).`
  * `brick_simple:get(Tab, Key, Timeout).`

EXAMPLES

 ::
Successful retrieval of a key-value pair:
+
------
> brick_simple:get(tab1, "goo").
{ok,1271543165272432,<<"value7">>}
------
+

 ::
Successful retrieval of a key without its associated value blob:
+
------
> brick_simple:get(tab1, "goo", ['witness']).
{ok,1271543165272432}
------
+

 ::
Failed attempt to retrieve a key that does not exist:
+
------
> brick_simple:get(tab1, "moo").
key_not_exist
------

[[brick-simple-get-many]]
=== brick_simple:get_many/5

SYNOPSIS

 ::
*brick_simple:get_many(Tab, Key, MaxNum, Flags, Timeout).*

DESCRIPTION

 ::
Get many `Key` and `Value` pairs from a single chain in the table `Tab`, up to a maximum of `MaxNum`. Keys are returned in lexicographic sorting order. The key `Key` is the starting point: if there is a key `KeyNext` that follows `Key` (in lexicographic order), and if the `boolean()` value in the return is `true`, then `KeyNext` will be the first key's info in the `get_many_res_list()`.
+
IMPORTANT: The `get_many()` function call cannot be used to find all keys in all chains in a Hibari table. The consistent hash of `Key` will send the `get_many` operation to the tail brick in a single chain; all keys returned will come from that single brick only.
+

PARAMETERS

 ::
*Tab*

- Name of the table from which to retrieve the keys.
- Mandatory.
- Type:
  * `Tab = table()`
  * `table() = atom()`

 ::
*Key*

- Key at which to start the `get_many` retrieval, proceeding in lexicographic order.
- Mandatory.
- Type:
  * `Key = key()`
  * `key() = iodata()`
  * `iodata() = iolist() | binary()`
  * `iolist()  = [char() | binary() | iolist()]`
+
NOTE: While the `Key` may be specified as either `iolist()` or `binary()`, it will be converted into binary before operation execution.
+

 ::
*MaxNum*

- Maximum number of keys to return.
- Mandatory.
- Type:
  * `MaxNum = integer()`

 ::
*Flags*

- List of operational flags to apply to the `get_many' operation, and/or custom property flags.
- Optional; defaults to empty list.
- Type:
  * `Flags = flags_list()`
  * `flags_list() = [do_op_flag() | property()]`
  * `do_op_flag() = 'get_all_attribs' | 'witness' | {'binary_prefix', binary()} | {'max_bytes', integer()}`
  * `property() = atom() | {term(), term()}`
- Operational flag usage
  * `'get_all_attribs'`
    ** Return all attributes of each key. May be used in combination with the `witness` flag.
  * `'witness'`
    ** Do not return the value blobs in the result. This flag will guarantee that the brick does not require disk access to satisfy this request.
  * `{'binary_prefix', binary()}`
    ** Return only keys that have a
binary prefix that is exactly equal to `binary()`.
  * `{'max_bytes', integer()}`
    ** Return only as many keys as the sum of the sizes of their corresponding value blobs does not exceed `integer()` bytes.

 ::
*Timeout*

- Operation timeout in milliseconds.
- Optional; defaults to 15000.
- Type:
  * `Timeout = timeout()`
  * `timeout() = integer() | 'infinity'`

RETURNS

 ::
Success returns

- `{ok, {[{key(), timestamp(), val()}], boolean()}}`
  * Success return when the `get_many` request uses neither the `'witness'` flag nor the `'get_all_attribs'` flag.
  * `timestamp() = integer()`
  * `val() = iodata()`
  * `iodata() = iolist() | binary()`
  * `iolist()  = [char() | binary() | iolist()]`
- `{ok, {[{key(), timestamp()}], boolean()}}`
  * Success return when the `get_many` uses `'witness'` but not `'get_all_attribs'`.
- `{ok, {[{key(), timestamp(), proplist()}], boolean()}}`
  * Success return when the `get_many` uses both `'witness'` and `'get_all_attribs'`.
  * `proplist() = [property()]`
  * `property() = atom() | {term(), term()}`
- `{ok, {[{key(), timestamp(), val(), exp_time(), proplist()}], boolean()}}`
  * Success return when the `get_many` uses `'get_all_attribs'` but not `'witness'`.
  * `exp_time() = time_t()`
+
NOTE: When a `proplist()` is returned, one of the properties in the list will always be `{val_len,Size::integer()}`, where `Size` is the size of the value blob in bytes.
+

 ::
Error returns

- `'invalid_flag_present'`
  * The operation failed because an invalid `do_op_flag()` was found in the `Flags` argument.
- `'brick_not_available'`
  * The operation failed because the chain that is responsible for this key is currently length zero and therefore unavailable.
- `{{'nodedown',node()},{'gen_server','call',term()}}`
  * The operation failed because the server brick handling the request has crashed or else a network partition has occurred between the client and server. The client should resend the query after a short delay, on the assumption that the Admin Server will have detected the failure and taken steps to repair the chain.
  * `node() = atom()`

ALIASES

 ::
- brick_simple:get_many/3
  * `brick_simple:get_many(Tab, Key, MaxNum).`
- brick_simple:get_many/4
  * `brick_simple:get_many(Tab, Key, MaxNum, Flags).`
  * `brick_simple:get_many(Tab, Key, MaxNum, Timeout).`

EXAMPLES

 ::
Successful retrieval of all keys from a table that currently has only two keys. The boolean `false' indicates that there are no keys following the `foo` key:
+
------
> brick_simple:get_many(tab1, "", 5).
{ok,{[{<<"another">>,1271543102911775,<<"yes!">>},
      {<<"foo">>,1271543165272987,<<"Foo again?">>}],
     false}}
------
+

 ::
Successful retrieval of all keys from a table that currently has only two keys, using the `witness` flag in the request.
+
------
> brick_simple:get_many(tab1, "", 5, ['witness']).
{ok,{[{<<"another">>,1271543102911775},
      {<<"foo">>,1271543165272987}],
     false}}
------

 ::
Successful retrieval of all keys from a table that currently has only two keys, using the `get_all_attribs` flag in the request.
+
------
> brick_simple:get_many(tab1, "", 5).
{ok,{[{<<"another">>,1271543102911775,<<"yes!">>,0,
       [{val_len,4}]},
      {<<"foo">>,1271543165272987,<<"Foo again?">>,0,[{val_len,6}]}],
     false}}
------

[[brick-simple-delete]]
=== brick_simple:delete/4

SYNOPSIS

 ::
*brick_simple:delete(Tab, Key, Flags, Timeout).*

DESCRIPTION

 ::
Delete key `Key` from the table `Tab`.

PARAMETERS

 ::
*Tab*

- Name of the table from which to delete the key and its associated value.
- Mandatory.
- Type:
  * `Tab = table()`
  * `table() = atom()`

 ::
*Key*

- Key to delete from the table.
- Mandatory.
- Type:
  * `Key = key()`
  * `key() = iodata()`
  * `iodata() = iolist() | binary()`
  * `iolist()  = [char() | binary() | iolist()]`
+
NOTE: While the `Key` may be specified as either `iolist()` or `binary()`, it will be converted into binary before operation execution.
+

 ::
*Flags*

- List of operational flags to apply to the `delete' operation, and/or custom property flags.
- Optional; defaults to empty list.
- Type:
  * `Flags = flags_list()`
  * `flags_list() = [do_op_flag() | property()]`
  * `do_op_flag() = {'testset', timestamp()} |'must_exist' | 'must_not_exist'`
  * `timestamp() = integer()`
  * `property() = atom() | {term(), term()}`
- Operational flag usage
  * `{'testset', timestamp()}`
    ** Fail the operation if the existing key's timestamp is not exactly equal to `timestamp()`.  If used inside a link:#brick-simple-do[micro-transaction], abort the transaction if the key's timestamp is not exactly equal to `timestamp()`. Using this flag with `set` will result in an error if the key does not already exist or if the key exists but has a non-matching timestamp.
  * `'must_exist'`
    ** For use inside a link:#brick-simple-do[micro-transaction]: abort the transaction if the key does not exist.
  * `'must_not_exist'`
    ** For use inside a link:#brick-simple-do[micro-transaction]: abort the transaction if the key exists.

 ::
*Timeout*

- Operation timeout in milliseconds.
- Optional; defaults to 15000.
- Type:
  * `Timeout = timeout()`
  * `timeout() = integer() | 'infinity'`

RETURNS

 ::
Success return

- `'ok'`

 ::
Error returns

- `'key_not_exist'`
  * The operation failed because the key does not exist.
- `{'ts_error', timestamp()}`
  * The operation failed because the `{'testset', timestamp()}` flag was used and there was a timestamp mismatch. The `timestamp()` in the return is the current value of the existing key's timestamp.
  * `timestamp() = integer()`
- `'invalid_flag_present'`
  * The operation failed because an invalid `do_op_flag()` was found in the `Flags` argument.
- `'brick_not_available'`
  * The operation failed because the chain that is responsible for this key is currently length zero and therefore unavailable.
- `{{'nodedown',node()},{'gen_server','call',term()}}`
  * The operation failed because the server brick handling the request has crashed or else a network partition has occurred between the client and server. The client should resend the query after a short delay, on the assumption that the Admin Server will have detected the failure and taken steps to repair the chain.
  * `node() = atom()`

ALIASES

 ::
- brick_simple:delete/2
  * `brick_simple:delete(Tab, Key).`
- brick_simple:delete/3
  * `brick_simple:delete(Tab, Key, Flags).`
  * `brick_simple:delete(Tab, Key, Timeout).`

EXAMPLES

 ::
Successful deletion of a key and its associated value and attributes:
+
------
> brick_simple:delete(tab1, <<"foo">>).
ok
------
+

 ::
Failed attempt to delete a key that does not exist:
+
------
> brick_simple:delete(tab1, "key6").
key_not_exist
------
+

 ::
Failed attempt to delete a key for which we have incorrectly specified its current timestamp:
+
------
> brick_simple:delete(tab1, "goo", [{'testset', 12345}]).
{ts_error,1271543165272987}
------
+

 ::
Successful deletion of a key for which we have correctly specified its current timestamp:
+
------
> brick_simple:delete(tab1, "goo", [{'testset', 1271543165272987}]).
ok
------
+

 ::
Successful deletion of a key, using a non-default operation timeout:
+
------
> brick_simple:delete(tab1, "key3", 30000).
ok
------

[[brick-simple-do]]
=== brick_simple:do/4

SYNOPSIS

 ::
*brick_simple:do(Tab, OpList, OpFlags, Timeout).*

DESCRIPTION

 ::
Send a list of primitive operations to the table `Tab`. They will be executed at the same time by a Hibari brick. If the first item in the `OpList` is `brick_server:make_txn()` then the list of operations is executed in the context of a micro-transaction: either all operations will be executed successfully or none will be executed. We term these "micro"-transactions because they are subject to certain limitations that apply to all operations that use the `brick_simple:do()` API:
* All impacted keys must be in the same table.
* All impacted keys must be in the same chain.
* All operations in the transaction must be sent in a single `brick_simple:do()` call. Unlike some other databases, it is not possible to request a transaction handle and to add operations to that transaction in an one-by-one, "ad hoc" manner.

 ::
For further information about micro-transactions, see link:hibari-sysadmin-guide.en.html#micro-transactions[Hibari System Administrator's Guide, "Micro-Transactions" section].

PARAMETERS

 ::
*Tab*

- Name of the table in which to perform the operations.
- Mandatory.
- Type:
  * `Tab = table()`
  * `table() = atom()`

 ::
*OpList*

- List of primitive operations to perform. Each primitive is invoked using the `brick_server:make_*()` API.
- Mandatory.
- Type:
  * `OpList = do_op_list()`
  * `do_op_list() = [do1_op()]`
  * `do1_op() =`
    ** `brick_server:make_add(Key, Value, ExpTime, Flags)`
    ** `brick_server:make_replace(Key, Value, ExpTime, Flags)`
    ** `brick_server:make_set(Key, Value, ExpTime, Flags)`
    ** `brick_server:make_get(Key, Flags)`
    ** `brick_server:make_get_many(Key, Flags)`
    ** `brick_server:make_delete(Key, Flags)`
    ** `brick_server:make_txn()`
       *** Include `brick_server:make_txn()` as the first item in your `OpList` if you want the `do` operation to be executed as an all-or-nothing transaction.
    ** Note that the arguments for each primitive are the same as those for the primitives when they are executed on their own, with the exclusion of the `Tab` and `Timeout` arguments, both of which serve as arguments to the overall `do` operation rather than as arguments to the primitives. For example, an `add` on its own is `brick_simple:add(Tab, Key, Value, ExpTime, Flags, Timeout)`, whereas in the context of a `do` operation an `add` primitive is `brick_server:make_add(Key, Value, ExpTime, Flags)`.
    ** For further information about each primitive, see link:#brick-simple-add[brick_simple:add/6], link:#brick-simple-replace[brick_simple:replace/6], link:#brick-simple-set[brick_simple:set/6], link:#brick-simple-get[brick_simple:get/4], link:#brick-simple-get-many[brick_simple:get_many/5], and link:#brick-simple-delete[brick_simple:delete/4].

 ::
*OpFlags*

- List of operational flags to apply to the overall `do' operation.
- Optional; defaults to empty list.
- Type:
  * `OpFlags = do_flags_list()`
  * `do_flags_list() = [do_flag()]`
  * `do_flag() = 'fail_if_wrong_role' | 'ignore_role'`
- Operational flag usage
  * `'fail_if_wrong_role'`
    ** _description to be added_
  * `'ignore_role'`
    ** _description to be added_

 ::
*Timeout*

- Operation timeout in milliseconds.
- Optional; defaults to 15000.
- Type:
  * `Timeout = timeout()`
  * `timeout() = integer() | 'infinity'`

RETURNS

 ::
Success return

- `[do1_res_ok]`
  * List of `do1_res_ok`, one for each primitive operation specified in the `do` request. Return list order corresponds to the order in which primitive operations are listed in the request's `OpList`. Note that if the `do` request does not use transaction semantics, then some individual primitive operations may fail without the overall `do` operation failing.
  * Within the return list, possible `do1_res_ok` returns to each individual primitive operation are the same as the possible returns that the primitive operation type could generate if it were executed on its own. For example, within the `do` operation's success return list, the possible returns for a primitive `add` operation are the same as the returns described in the link:#brick-simple-add[brick_simple:add/6] section; potential returns to a primitive `replace` operation are the same as those described in the link:#brick-simple-replace[brick_simple:replace/6] section; and likewise for link:#brick-simple-set[set], link:#brick-simple-get[get], link:#brick-simple-get-many[get_many], and link:#brick-simple-delete[delete].

 ::
Error returns

- `{txn_fail, [{integer(), do1_res_fail()}]}`
  * Operation failed because transaction semantics were used in the `do` request and one or more primitive operations within the transaction failed. The `integer()` identifies the failed primitive operation by its position within the request's `OpList`. For example, a 2 indicates that the second primitive listed in the request's `OpList` failed. Note that this position identifier does not count the `txn()` specifier at the start of the `OpList`.
  * `do1_res_fail()` indicates the type of failure for the failed primitive operation. Possibilities are:
    ** `{'key_exists', timestamp()}`
       *** `timestamp() = integer()`
    ** `'key_not_exist'`
    ** `{'ts_error', timestamp()}`
    ** `'invalid_flag_present'`
- `'invalid_flag_present'`
  * The operation failed because an invalid `do_flag()` was found in the `do` request's `OpFlags` argument. Note this is a different error than an invalid flag being found within an individual primitive.
- `'brick_not_available'`
  * The operation failed because the chain that is responsible for this key is currently length zero and therefore unavailable.
- `{{'nodedown',node()},{'gen_server','call',term()}}`
  * The operation failed because the server brick handling the request has crashed or else a network partition has occurred between the client and server. The client should resend the query after a short delay, on the assumption that the Admin Server will have detected the failure and taken steps to repair the chain.
  * `node() = atom()`

ALIASES

 ::
- brick_simple:do/2
  * `brick_simple:do(Tab, OpList).`
- brick_simple:do/3
  * `brick_simple:do(Tab, OpList, Timeout).`

EXAMPLES

 ::
Successful `do` operation adding two new keys to table `tab1`, without transaction semantics:
+
------
> brick_simple:do(tab1, [brick_server:make_add("foo3", "bar3"),brick_server:make_add("foo4", "bar4")]).
[ok,ok]
------
+

 ::
Successful creation of two `get` primitives `Do1` and `Do2`, and their subsequent combination into a `do` request, without transaction semantics:
+
------
> Do1 = brick_server:make_get("foo").
{get,<<"foo">>,[]}
> Do2 = brick_server:make_get("foo2").
{get,<<"foo2">>,[]}
> brick_simple:do(tab1, [Do1, Do2]).
[{ok,1271543102911775,<<"Foo again?">>},key_not_exist]
------
+

 ::
Failed operation with transaction semantics. Because transaction semantics are used, the failure of the primitive `Do2b` causes the entire operation to fail.
+
------
> Do1b = brick_server:make_get("foo").
{get,<<"foo">>,[]}
> Do2b = brick_server:make_get("foo2", [must_exist]).
{get,<<"foo2">>,[must_exist]}
> brick_simple:do(tab1, [brick_server:make_txn(), Do1b, Do2b]).
{txn_fail,[{2,key_not_exist}]}
------

[[brick-simple-fold-table]]
=== brick_simple:fold_table/7

_Under Construction_

SYNOPSIS

 ::
*brick_simple:fold_table(Tab, Fun, Acc, NumItems, Flags, MaxParallel, Timeout).*

DESCRIPTION

 ::
Attempt a fold operation across all keys in a table.

IMPORTANT: Do not execute this operation while a data migration is being performed.

PARAMETERS

 ::
*Tab*

- Name of the table across which to perform the fold operation.
- Mandatory.
- Type:
  * `Tab = table()`
  * `table() = atom()`

 ::
*Fun*

- _description to be added_.
- Mandatory.
- Type:
  * `Fun  = fun_arity_2()`
  * `fun_arity_2()` arguments =
   ** `{ChainName, Tuple_from_get_many}`
     *** `Tuple_From_get_many` is a single result tuple from a link:#brick-simple-get-many[brick_simple:get_many()] result. Its format can vary according to the `Flags` argument, which is passed as-is to a `get_many()` call. For example, if `Flags` = `[]`, then `Tuple_From_get_many` will match `{Key, TS, Value}`. If `Flags` = `[witness]`, then `Tuple_From_get_many` will match `{Key, TS}`.
   ** `UserAccumulatorTerm`

 ::
*Acc*

- _description to be added_.
- Mandatory.
- Type:
  * `Acc  = term()`

 ::
*NumItems*

- _description to be added_.
- Mandatory.
- Type:
  * `NumItems  = integer()`

 ::
*Flags*

- List of operational flags to apply to the `fold_table' operation. The supported flags are the same as those for link:#brick-simple-get-many[brick_simple:get_many()].
- Mandatory.
- Type:
  * `Flags = flags_list()`
  * `flags_list() = [do_op_flag() | property()]`
  * `do_op_flag() = 'get_all_attribs' | 'witness' | {'binary_prefix', binary()} | {'max_bytes', integer()}`
  * `property() = atom() | {term(), term()}`
- Operational flag usage
  * `'get_all_attribs'`
    ** Return all attributes of each key. May be used in combination with the `witness` flag.
  * `'witness'`
    ** Do not return the value blobs in the result. This flag will guarantee that the brick does not require disk access to satisfy this request.
  * `{'binary_prefix', binary()}`
    ** Return only keys that have a
binary prefix that is exactly equal to `binary()`.
  * `{'max_bytes', integer()}`
    ** Return only as many keys as the sum of the sizes of their corresponding value blobs does not exceed `integer()` bytes.

 ::
*MaxParallel*

- If `MaxParallel` = 0, a true fold will be performed. If `MaxParallel` >= 1, then an independent fold will be performed on each chain, with up to `MaxParallel` number of folds running in parallel. The result from each chain fold will be returned to the caller as-is, i.e. will *not* be combined like in a "reduce" phase of a map-reduce cycle.
- Optional; defaults to 0.
- Type:
  * `MaxParallel = integer()`

 ::
*Timeout*

- Operation timeout in milliseconds.
- Optional; defaults to 5000.
- Type:
  * `Timeout = timeout()`
  * `timeout() = integer()

RETURNS

_more info to be added_

- `term() | {term(), integer(), integer(), integer()}`

ALIASES

 ::
- brick_simple:fold_table/5
  * `brick_simple:fold_table(Tab, Fun, Acc, NumItems, Flags).`
- brick_simple:fold_table/6
  * `brick_simple:fold_table(Tab, Fun, Acc, NumItems, Flags, MaxParallel).`

EXAMPLES

_to be added_

[[brick-simple-fold-key]]
=== brick_simple:fold_key_prefix/9

_Under Construction_

SYNOPSIS

 ::
*brick_simple:fold_key_prefix(Tab, Prefix, StartKey, Fun, Acc, Flags0, NumItems,SleepTime,Timeout).*

DESCRIPTION

 ::
For a binary key prefix `Prefix`, fold over all keys in table `Tab` starting with `StartKey`, sleeping for `SleepTime` milliseconds between iterations and using `Flags` and `NumItems` as arguments to link:#brick-simple-get-many[brick_simple:get_many()].

IMPORTANT: Do not execute this operation while a data migration is being performed.

PARAMETERS

 ::
*Tab*

- Name of the table in which to perform the fold operation.
- Mandatory.
- Type:
  * `Tab = table()`
  * `table() = atom()`

 ::
*Prefix*

- Key prefix for which to perform the fold operation.
- Mandatory.
- Type:
  * `Prefix = binary()`

 ::
*StartKey*

- Key at which to initiate the fold operation.
- Optional; defaults to equal your specified `Prefix`.
- Type:
  * `StartKey = binary()`

 ::
*Fun*

- _description to be added_.
- Mandatory.
- Type:
  * `Fun  = fun_arity_2()`
  * `fun_arity_2()` arguments =
   ** `Tuple_from_get_many`
     *** `Tuple_From_get_many` is a single result tuple from a link:#brick-simple-get-many[brick_simple:get_many()] result. Its format can vary according to the `Flags0` argument, which is passed as-is to a `get_many()` call. For example, if `Flags0` = `[]`, then `Tuple_From_get_many` will match `{Key, TS, Value}`. If `Flags0` = `[witness]`, then `Tuple_From_get_many` will match `{Key, TS}`.
   ** `UserAccumulatorTerm`

 ::
*Acc*

- _description to be added_.
- Mandatory.
- Type:
  * `Acc  = term()`

 ::
*Flags0*

- List of operational flags to apply to the `fold_key_prefix` operation. The supported flags are the same as those for link:#brick-simple-get-many[brick_simple:get_many()], excluding the `{'binary_prefix', binary()}` flag. This flag is inappropriate since the key prefix is passed directly through the `Prefix` argument of `brick_simple:fold_key_prefix()`.
- Mandatory.
- Type:
  * `Flags0 = 'get_all_attribs' | 'witness' | {'max_bytes', integer()}`
- Operational flag usage
  * `'get_all_attribs'`
    ** Return all attributes of each key. May be used in combination with the `witness` flag.
  * `'witness'`
    ** Do not return the value blobs in the result. This flag will guarantee that the brick does not require disk access to satisfy this request.
  * `{'max_bytes', integer()}`
    ** Return only as many keys as the sum of the sizes of their corresponding value blobs does not exceed `integer()` bytes.

 ::
*NumItems*

- _description to be added_.
- Optional; defaults to 100.
- Type:
  * `NumItems  = integer()`

 ::
*SleepTime*

- Sleep time between interations, in milliseconds.
- Optional; defaults to 0.
- Type:
  * `SleepTime = integer()`

 ::
*Timeout*

- Operation timeout in milliseconds.
- Optional; defaults to 15000.
- Type:
  * `Timeout = timeout()`
  * `timeout() = integer()

RETURNS

_more info to be added_

 ::
Success return

- `{ok, Acc, Iterations}`
  * `Acc = term()`
  * `Iterations = integer()`

 ::
Error return

- `{error, GdssError, Acc, Iterations}`
  * `GdssError = term()`
  * `Acc = term()`
  * `Iterations = integer()`

ALIASES

 ::
- brick_simple:fold_key_prefix/5
  * `brick_simple:fold_key_prefix(Tab, Prefix, Fun, Acc, Flags).`

EXAMPLES

_to be added_



[[client-api-ubf]]
== Client API: UBF

link:http://github.com/norton/ubf[The UBF protocol] is a
formally-specified family of protocols that are supported by a large
number of client languages.  This section attempts to describe the
layers of the UBF protocol stack, how to use the UBF client in Erlang
and other languages, and how to use that client to access a Hibari
storage cluster.

The Hibari source distribution includes UBF/EBF protocol support for the
following languages:

* Erlang, see xref:using-ubf-erlang-client[]
* Java, see xref:using-ubf-java-client[]
* Python, see xref:using-ubf-python-client[]

[[hibari-server-impl-of-ubf-proto-stack]]
=== The Hibari Server's Implementation of the UBF Protocol Stack

UBF(A): Bottom Layer, transport and session protocol layer::
This layer plays the same basic role as many other serialized data
transport protocols that use TCP for host-to-host transport, such as
link:http://en.wikipedia.org/wiki/Open_Network_Computing_Remote_Procedure_Call[ONC-RPC],
link:http://en.wikipedia.org/wiki/IIOP[CORBA IIOP],
link:http://en.wikipedia.org/wiki/Protocol_buffers[Protocol Buffers],
and
link:http://en.wikipedia.org/wiki/Thrift_(protocol)[Thrift].
+
Hibari servers support several of these session protocols on top
of a TCP/IP transport protocol.  The choice of session protocol is
a matter of convenience and/or support for the application
developer. Hibari should be as easy for an app developer to use
Ruby and JSON-RPC as it is to use Python and Protocol Buffers.
+
* UBF(A), Joe Armstrong's original session layer protocol
* EBF, the Erlang Binary Format.  The session layer protocol is a
  thin, efficient that uses the Erlang BIFs `term_to_binary()` and
  `binary_to_term()` to serialize Erlang data terms.  This protocol
  is very closely related to the link:http://bert-rpc.org/[BERT protocol].
* JSON over TCP, also called JSF (the JavaScript
  Format).  Erlang terms are encoded as
  link:http://en.wikipedia.org/wiki/JSON[JSON terms]
  and transmitted directly over a TCP transport.  This
  protocol is not in common use but is easy to implement in the UBF
  server framework.
* HTTP, the link:http://en.wikipedia.org/wiki/HTTP[Hypertext
  Transfer Protocol].  This protocol is used to support Hibari's
  link:http://en.wikipedia.org/wiki/JSON-RPC[JSON-RPC] server.
* link:http://en.wikipedia.org/wiki/Thrift_(protocol)[Thrift].
  Similar to EBF, except that Thrift's binary encoding is used for
  the wire protocol instead of UBF(A) or Erlang's native wire
  formats.
  *Hibari support is experimental (i.e. not yet implemented).*
* link:http://en.wikipedia.org/wiki/Protocol_buffers[Protocol Buffers].
  Similar to EBF, except that Google's Protocol Buffers binary
  encoding is used for the wire protocol instead of UBF(A) or
  Erlang's native wire formats.
  *Hibari support is experimental (i.e. not yet implemented).*
* link:http://hadoop.apache.org/avro/docs/current/[Avro].
  Similar to EBF, except that Avro's binary encoding is used for the
  wire protocol instead of UBF(A) or Erlang's native wire formats.
  *Hibari support is experimental (i.e. not yet implemented).*
+
UBF(B): Middle Layer, the "contract"::
UBF(B) is a programming language for describing types in UBF(A)
and protocols between clients and servers. UBF(B) is roughly
equivalent to to Verified XML, XML-schemas, SOAP and WDSL.
+
This layer enforces a protocol "contract", a formal specification of
all data sent by the client and by the server.  Any data that does not
precisely conform to the protocol is rejected by the contract checker
(which is embedded in the server).  If the client wishes, it may also
use the contract checker to validate data sent by the server, though
this not commonly done.
+
UBF( C): Top Layer, the UBF Metaprotocol::
The metaprotocol is used at the beginning of a UBF session to select
one of the UBF(B) contracts that the TCP listener is capable of
offering.  At the moment, Hibari servers support only the "gdss"
contract, but other contracts may be added in the future.

[[ubf-representation-of-strings]]
=== UBF representation of strings vs. binaries

The Erlang language does not have a data type specifically for
strings.  Instead, strings are typically represented as lists of
integers (ASCII byte values) and/or binaries.

A UBF contract makes a distinction between a string, list, and
binary.  In the case of a string, UBF(A) encodes a string using the
notation `{'#S', "Hello, world!"}` to represent the string "Hello,
world!".

This string encoding is cumbersome to use for developers; in Erlang,
the `ubf.hrl` header file includes a macro `?S("Hello, world!")` as a
slightly less ugly shortcut.  When using other languages, the 2-tuple
and the atom `'#S'`  would be created as any other 2-tuple and atom.

Fortunately, there is only one case where the string type is
necessary: using the `startSession` metaprotocol command to start
using the Hibari data server contract.  An example will be shown
below.

[[using-ubf-in-any-language]]
=== Steps for Using a UBF-based Protocol in Any Language

The steps to use a UBF-based protocol are the same in any language.

1. Create a connection to the UBF server.
* ... or the EBF server, or the JSON-RPC server, or the Thrift
server, or the ....
2. Use the UBF metaprotocol to start using the `gdss` contract,
   i.e. the Hibari server contract.
3. Send one or more Hibari server queries and decode the respective
   server responses.
4. Close the connection to the UBF server.

[[the-hibari-ubf-protocol-contract]]
=== The Hibari UBF Protocol Contract

The Hibari UBF Protocol contract can be found in the file
`ubf_gdss_plugin.con`.

NOTE: See the Hibari source code for the most up-to-date version of
this file.  link:./misc-codes/ubf_gdss_plugin.con[This documentation has a copy
of `ubf_gdss_plugin.con`], though it may be slightly out-of-date.

The names of the UBF types specified in the contract may differ
slightly from the names of the types used in this document's
xref:client-api-erlang[].  For example, the UBF contract calls the key
expiration time time `exp_time()`, while the type system in this
document calls it `expiry()`.  However, in all cases of slightly
different names, the fundamental data type that both names use is the
same: e.g. `integer()` for expiration time.

For each command, the UBF contract uses the following naming
conventions:

* `CommandName_req()` for the request sent from client -> server,
  e.g. `set_req()` for the `set` command.
* `CommandName_res()` for the response sent from server -> client,
  e.g. `set_res()` for the `set` response.

The general form of a UBF RPC call is a tuple.  The first element in
the tuple is the name of the command, and the following elements are
arguments for that command.  The response can be any Erlang term, but
the Hibari contract will only return the atom or tuple types.

The following is a mapping of UBF client request type to its Erlang
API function, in alphabetical order.:

* `add_req()` -> `brick_simple:add()`, see xref:brick-simple-add[].
* `delete_req()` -> `brick_simple:delete()`, see xref:brick-simple-delete[].
* `do_req()` -> `brick_simple:do()`, see xref:brick-simple-do[].
* `get_req()` -> `brick_simple:get()`, see xref:brick-simple-get[].
* `get_many_req()` -> `brick_simple:get_many()`, see xref:brick-simple-get-many[].
* `replace_req()` -> `brick_simple:replace()`, see xref:brick-simple-replace[].
* `set_req()` -> `brick_simple:set()`, see xref:brick-simple-set[].


[[using-ubf-erlang-client]]
=== Using the UBF Client Library for Erlang

[IMPORTANT]
==============================
1. When using the Erlang shell for experimentation & prototyping, that
   shell must have the path to the Erlang UBF client
   library in its search path.  The easiest way to do this is to use
   the arguments `-pz /path/to/ubf/library/ebin` to your Erlang
   shell's `erl` command.
2. When writing code, the statement `-include("ubf.hrl").` at the top
   of your source module to gain access to the `?S()` macro.  Due to
   limitations in the Erlang shell, macros cannot be used in the shell.
==============================

As outlined in xref:using-ubf-in-any-language[], the first step is to
create a connection to a Hibari server.  If the Hibari cluster has
multiple nodes, then it doesn't matter which one that you connect to:
all nodes can handle any UBF request and will route the query to the
proper brick.

.Create a connection to the UBF server (on "localhost" TCP port 7581)
----------------------------
(asdf@bb3)54> {ok, P1, _} = ubf_client:connect("localhost", 7581, [{proto, ubf}], 5000).
{ok,<0.139.0>,{'#S', "gdss_meta_server"}}
----------------------------

The second step is to use the UBF metaprotocol to select the Hibari
server, contract, called "gdss",
for all further commands for this connection.

TIP: The Hibari server contract is "stateless".  All replies terms from the
`ubf_client:rpc/2` function use the form
`{reply,ServerReply,UBF_StateName}`.  Because the Hibari server
contract is stateless, the `UBF_StateName` will always be the atom
`none`.

.Use the UBF metaprotocol to request the "gdss" contract
----------------------------
(asdf@bb3)55> ubf_client:rpc(P1, {startSession, {'#S', "gdss"}, []}).
{reply,{ok,ok},none}
----------------------------

Now that the UBF connection is set up, we can use it to set a key "foo".

.Set the key "foo" in table `tab1` with the value "foo val", no expiration time, no flags, and a timeout of 5 seconds
----------------------------
(asdf@bb3)59> ubf_client:rpc(P1, {set, tab1, <<"foo">>, <<"foo val">>, 0, [], 5000}).
{reply,ok,none}
----------------------------

[NOTE]
===========================================
Note that the return value of both the
`set_req()` (in the example above) and `get_req()` (in the example
below) return the same types described in the xref:brick-simple-set[]
and xref:brick-simple-get[], respectively.

The only difference is that the `ubf_client:rpc/2` function wraps the
server's reply in a 3-tuple: `{reply,ServerReply,none}`.
===========================================

.Get the key "foo" in table `tab1`, timeout in 5 seconds
----------------------------
(asdf@bb3)66> ubf_client:rpc(P1, {get, tab1, <<"foo">>, [], 5000}).
{reply,{ok,1273009092549799,<<"foo val">>},none}
----------------------------

If the client sends a request that violates the contract, the server
will tell you, as in this example:

.Send a contract-violating request
----------------------------
(asdf@bb3)89> ubf_client:rpc(P1, {bbb, 3000}).
{reply,{clientBrokeContract,{bbb,3000},[]},none}
----------------------------

When you are done with the connection, it is polite to close the
connection explicitly.  The server will quietly clean up its side of
the connection if the client forgets to call or cannot call `stop/1`.

.Close the UBF connection
----------------------------
(asdf@bb3)92> ubf_client:stop(P1).
ok
----------------------------

[[using-ubf-java-client]]
=== Using the UBF Client Library for Java

The source code for the UBF client library for Java is included in
the UBF source repository at
link:http://github.com/norton/ubf[http://github.com/norton/ubf], in
the `priv/java` subdirectory.

==== Compiling the UBF client library for Java

1. Please update your UBF client library code to the "master" branch
   for a date after 10 May 2010, or use the Git tag "v1.14" or later.
   Versions of the library before 10 May 2010 and tag "v1.14" have
   several bugs that will prevent the UBF client from working
   correctly.
2. Change directory to the `priv/java` directory of the UBF client
   library source distribution.
3. Run `make`.
4. (Optional) Copy the class files in the `classes` subdirectory to
   a suitable directory for your Java development environment.

==== Compiling the UBF client library test program HibariTest.java

1. Change directory to the `gdss-ubf-proto/priv/java` subdirectory in
   the Hibari source distribution.
2. Edit the `Makefile` to change the `UBF_CLASSES_DIR` variable to
   point to the `priv/java/classes` subdirectory of the UBF package's
   source code (or the subdirectory where those classes have been
   formally installed on your system).
3. Run the following two `make` commands.  The second assumes that the
   Hibari server's UBF server is on the local machine, "localhost".
+
-------------------------
% make HibariTest
% make run-HibariTest
-------------------------
4. If the Hibari server is not running on the local machine, then run
   `make -n run-HibariTest` to show the `java` command that is used to
   run the test program.  Cut-and-paste the command into your shell,
   then edit the last argument to specify the hostname of a Hibari
   server.

==== Examining the HibariTest.java test program

The `main()` function does three things:

1. Create a new UBF connection to a Hibari server (hostname/IP address
   is specified in the first command line argument) and requests the
   `gdss` contract via the UBF metaprotocol.
2. Run the small test cases in the `test_hibari_basics()` method.
3. Close the UBF session and exit.

[[the-ubf-hibaritest-main-method]]
.The ubf.HibariTest.main() method
--------------------------------------
public class HibariTest
{
    public static void main(String[] args)
        throws Exception
    {
        Socket sock = null;
        UBFClient ubf = null;

        try {
            sock = new Socket(args[0], 7581);
            ubf = UBFClient.new_via_sock(new UBFString("gdss"), new UBFList(),
                    new FooHandler(), sock);
        }
        catch (Exception e) {
            System.out.println(e);
            System.exit(1);
        }

        test_hibari_basics(ubf);

        ubf.stopSession();
        System.out.println("Success, it works");
        System.exit(0);
    }
/* ... */
}
--------------------------------------

The `test_hibari_basics()` method performs the same basic UBF
operations as the Python EBF demonstration script described in
xref:using-ubf-python-client[].  Unlike the Python demo script, the
demo program does not use the Hibari `do()` command but rather then
single-operation commands like `get()` and `set()`.

1. Delete the key `foo` from table `tab1`.
+
---------------------------
    public static void test_hibari_basics(UBFClient ubf)
        throws IOException, UBFException
    {
        // setup
        UBFObject res1 = ubf.rpc(
                UBF.tuple( new UBFAtom("delete"), new UBFAtom("tab1"),
                            new UBFBinary("foo"), new UBFList(),
                            new UBFInteger(4000)));
        System.out.println("Res 1:" + res1.toString());
---------------------------
2. Add the key `foo` to table `tab1`.
+
---------------------------
        // add - ok
        UBFObject res2 = ubf.rpc(
                UBF.tuple( new UBFAtom("add"), atom_tab1,
                            new UBFBinary("foo"), new UBFBinary("bar"),
                            new UBFInteger(0), new UBFList(),
                            new UBFInteger(4000)));
        System.out.println("Res 2:" + res2.toString());
        if (! res2.equals(atom_ok))
            System.exit(1);
---------------------------
3. Add the key `foo` to table `tab1` again, this time expecting a
failure.
+
---------------------------
        // add - ng
        UBFObject res3 = ubf.rpc(
                UBF.tuple( new UBFAtom("add"), atom_tab1,
                            new UBFBinary("foo"), new UBFBinary("bar"),
                            new UBFInteger(0), new UBFList(),
                            new UBFInteger(4000)));
        System.out.println("Res 3:" + res3.toString());
        if (! ((UBFTuple)res3).value[0].equals(atom_key_exists))
            System.exit(1);
---------------------------
4. Get the key `foo` from table `tab1`.
+
---------------------------
        // get - ok
        UBFObject res4 = ubf.rpc(
                UBF.tuple( new UBFAtom("get"), atom_tab1,
                            new UBFBinary("foo"), new UBFList(),
                            new UBFInteger(4000)));
        System.out.println("Res 4:" + res4.toString());
        if (! ((UBFTuple)res4).value[0].equals(atom_ok) ||
            ! ((UBFTuple)res4).value[2].equals("bar"))
            System.exit(1);
---------------------------
5. Set the key `foo` in table `tab1` to `bar bar`.
+
---------------------------
        // set - ok
        UBFObject res5 = ubf.rpc(
                UBF.tuple( new UBFAtom("set"), atom_tab1,
                            new UBFBinary("foo"), new UBFBinary("bar bar"),
                            new UBFInteger(0), new UBFList(),
                            new UBFInteger(4000)));
        System.out.println("Res 5:" + res5.toString());
        if (! res5.equals(atom_ok))
            System.exit(1);
---------------------------
6. Get `foo` again and verify that the value is `bar bar`
+
---------------------------
        // get - ok
        UBFObject res6 = ubf.rpc(
                UBF.tuple( new UBFAtom("get"), atom_tab1,
                            new UBFBinary("foo"), new UBFList(),
                            new UBFInteger(4000)));
        System.out.println("Res 6:" + res6.toString());
        if (! ((UBFTuple)res6).value[0].equals(atom_ok) ||
            ! ((UBFTuple)res6).value[2].equals("bar bar"))
            System.exit(1);
---------------------------

==== The UBF event handler interface

Each `UBFClient` instance uses a separate thread to read data from the
server and do any of the following:

1. Signal to the other thread that a synchronous RPC response was
received from the server.
2. Run a callback function when an `event_out` asynchronous event is
received from the server.
3. The socket was closed unexpectedly.

In cases #2 and #3, a class that implements the `UBFEventHandler`
interface is used to define the action to be taken in those cases.

The `HibariTest.java` contains a sample implementation of callback
functions for asynchronous events.  A real application would probably
want to do something much more helpful than this example does.

------------------------------
    public static class FooHandler implements UBFEventHandler {
        public FooHandler() {
        }
        public void handleEvent(UBFClient client, UBFObject event) {
            System.out.println("Hey, got an event: " + event.toString());
        }
        public void connectionClosed(UBFClient client) {
            System.out.println("Hey, connection closed, ignoring it\n");
        }
    }
------------------------------

TIP: See xref:the-ubf-hibaritest-main-method[] for an example that
uses this `FooHandler` class.

[[using-ubf-python-client]]
=== Using the EBF Client Library for Python

The source code for the EBF client library for Python is included in
the UBF source repository at
link:http://github.com/norton/ubf[http://github.com/norton/ubf], in
the `priv/python` subdirectory.

NOTE: Recall that the EBF protocol is very closely related to UBF.  The
only significant difference is the "layer 5" session protocol layer:
instead of using the UBF(A) protocol, the EBF (Erlang Binary Format)
protocol is used instead.  See
xref:hibari-server-impl-of-ubf-proto-stack[] for more details.

In addition, you will need the "py_interface" package, developed by
Tomas Abrahamsson and others.  "py-interface" is distributed under the
link:http://www.fsf.org/licensing/education/licenses/lgpl.html[GNU
Library General Public License].  A git repository is hosted at
repo.or.cz. To clone it and build it, use:

-------------------
git clone git://repo.or.cz/py_interface.git
cd py_interface
autoconf
./configure
make
pwd
-------------------

Use the output of the last command, `pwd`, to remember the full
directory path to the "py-interface" library.  The example below
assumes that path is `/tmp/py-interface`.

The `pyebf.py` file contains a small unit test that makes several
calls to the Hibari UBF contract's `do_req()` command.  The results of
(almost) every command are verified using the `assert` function.

--------------------
env PYTHONPATH=/path/to/py_interface python pyebf.py
--------------------

1. Connect to the Hibari server on "localhost" TCP port 7580 and use
the UBF metaprotocol to switch to the `gdss` contract.
+
---------------------------
    ## login
    ebf.login('gdss', 'gdss_meta_server')
---------------------------
2. Delete the key `'foo'` from table `tab1`.
+
---------------------------
    ## setup
    req0 = (Atom('do'), Atom('tab1'), [(Atom('delete'), 'foo', [])], [], 1000)
    res0 = ebf.rpc('gdss', req0)
---------------------------
3. Get the key `'foo'` from table `tab1`.
+
---------------------------
    ## get - ng
    req1 = (Atom('do'), Atom('tab1'), [(Atom('get'), 'foo', [])], [], 1000)
    res1 = ebf.rpc('gdss', req1)
    assert res1[0] == 'key_not_exist'
---------------------------
4. Add the key `'foo'` to table `tab1`.  The `do_req()` interface
requires managing the timestamp integers explicitly by the client; the
timestamp `1` is used here.
+
---------------------------
    ## add - ok
    req2 = (Atom('do'), Atom('tab1'), [(Atom('add'), 'foo', 1, 'bar', 0, [])], [
], 1000)
    res2 = ebf.rpc('gdss', req2)
    assert res2[0] == 'ok'
---------------------------
5. Add the key `'foo'` to table `tab1`.
+
---------------------------
    ## add - ng
    req3 = (Atom('do'), Atom('tab1'), [(Atom('add'), 'foo', 1, 'bar', 0, [])], [
], 1000)
    res3 = ebf.rpc('gdss', req3)
    assert res3[0][0] == 'key_exists'
    assert res3[0][1] == 1
---------------------------
6. Get the key `'foo'` from table `tab1`, verifying that the timestamp
is still `1` and value is still `'bar'`.
+
---------------------------
    ## get - ok
    req4 = (Atom('do'), Atom('tab1'), [(Atom('get'), 'foo', [])], [], 1000)
    res4 = ebf.rpc('gdss', req4)
    assert res4[0][0] == 'ok'
    assert res4[0][1] == 1
    assert res4[0][2] == 'bar'
---------------------------
7. Set the key `'foo'` from table `tab1`, using a new timestamp `2`.
+
---------------------------
    ## set - ok
    req5 = (Atom('do'), Atom('tab1'), [(Atom('set'), 'foo', 2, 'baz', 0, [])], [
], 1000)
    res5 = ebf.rpc('gdss', req5)
    assert res5[0] == 'ok'
---------------------------
8. Get the key `'foo'` from table `tab1`, verifying both the new
timestamp and new value.
+
---------------------------
    ## get - ok
    req6 = (Atom('do'), Atom('tab1'), [(Atom('get'), 'foo', [])], [], 1000)
    res6 = ebf.rpc('gdss', req6)
    assert res6[0][0] == 'ok'
    assert res6[0][1] == 2
    assert res6[0][2] == 'baz'
---------------------------

[[client-api-tbf]]
== Client API: Thrift
"TBF" is a link:https://github.com/apache/thrift[Thrift
protocol] defined by UBF contract
xref:the-hibari-ubf-protocol-contract[].  This section attempts to
describe the Hibari Thrift API which allows users to access Hibari with
Thrift clients in any Thrift supported programming languages, and how
to extend the API for application uses.

=== The Hibari Thrift API
The Hibari Thrift API is defined as Hibari Service in
link:./misc-codes/hibari.thrift[hibari.thrift].  At the time this API
was developed, only Thrift 0.4.0 is available to us.  This version is
our first attempt to adopt Thrift.  Some of the functions and options
are not yet supported.

IMPORTANT: The Hibari Thrift API only supports Thrift 0.4.0 or above.
---------------------------
service Hibari {

  /**
   * Check connection availability / keepalive
   */
  oneway void keepalive()

  /**
   * Hibari Server Info
   */
  string info()

  /**
   * Hibari Description
   */
  string description()

  /**
   * Hibari Contract
   */
  string contract()

  /**
   * Add
   */
  HibariResponse Add(1: Add request)
      throws (1:HibariException ouch)

  /**
   * Replace
   */
  HibariResponse Replace(1: Replace request)
      throws (1:HibariException ouch)

  /**
   * Set
   */
  HibariResponse Set(1: Set request)
      throws (1:HibariException ouch)

  /**
   * Delete
   */
  HibariResponse Delete(1: Delete request)
      throws (1:HibariException ouch)

  /**
   * Get
   */
  HibariResponse Get(1: Get request)
      throws (1:HibariException ouch)
}
---------------------------

For each primitive utility function, it has exactly one input
parameter.  The parameter is an object that has a name matching its
function. The object carries all mandatory and optional parameters to
Hibari. This object could also be used to implement micro-transactions
in the future.

=== Mapping UBF Contract Types to Thrift Types
You can find more details of the UBF / Thrift type conversion in
(link:https://github.com/norton/ubf-thrift[UBF-Thrift]).

=== Mapping UBF Contract to Thrift Service
Mapping UBF types to thrift primitives is different from mapping UBF
contracts to service. Thrift mainly uses 2 different types to compose
a request (struct and field).

If you are using Thrift to generate client code, you probably don't
need to worry about how the request being constructed. Visit
link:http://wiki.apache.org/thrift/ThriftGeneration[Thrift Wiki] for
the instruction to install Thrift and to generate client code.  You
will also need link:./misc-codes/hibari.thrift[hibari.thrift] to get
started.

If you are interested in the UBF contract, the Hibari NTBF contract
can be found in the file of `ntbf_gdss_plugin.con`.

=== Examples of using a Thrift client
Once you get the generated code, connecting to Hibari is easy.  For
example, adding the key `'fookey'` to table `tab1` with a value of
`'Hello, world!'` in the following 3 languages.

In Erlang:
------------------------------------
  -include("hibari_thrift.hrl").

  % init
  {ok, Client} = thrift_client:start_link("127.0.0.1", 7600, hibari_thrift),

  % create the input parameter object
  Request = #add{table=<<"tab1">>, key=<<"fookey">>, value=<<"Hello, world!">},

  % send request
  try
    HibariResponse = thrift_client:call(Client, 'Add', [Request]),
  catch
    HibariException ->
      HibariException
  end,

  ok = thrift_client:close(Client).
------------------------------------

In Java:
------------------------------------
  import com.hibari.rpc.*;

  // init
  TTransport transport = new TSocket("127.0.0.1", 7600);
  TProtocol proto = new TBinaryProtocol(transport);
  Hibari.Client client = new Hibari.Client(proto);
  transport.open();

  // create the input parameter object
  Add request = new Add("tab1", ByteBuffer.wrap("fookey".getBytes()),
    ByteBuffer.wrap("Hello, world!".getBytes())))

  // send request
  try {
    HibariResponse response = client.Add(request);
  } catch (HibariException e) {
    // ...
  }

  transport.close();
------------------------------------

In python:
------------------------------------
  from hibari import Hibari

  # init
  transport = TSocket.TSocket('localhost', 7600)
  transport.setTimeout(None)
  transport = TTransport.TBufferedTransport(transport)
  protocol = TBinaryProtocol.TBinaryProtocol(transport)
  client = Hibari.Client(protocol)
  transport.open()

  # create the input parameter object
  request = Add()
  request.table = "tab1"
  request.key = b"fookey"
  request.value = b"Hello, world!"

  # send request
  response = client.Add(request)

  transport.close()
------------------------------------

=== Mapping TBF Contract Responses From Thrift Client
TBF only responses one of two generic types to all functions in Hibari
Thrift API, HibariResponse or HibariException.  One could expect a
HibariResponse in an any successful cases.  Otherwise a
HibariException should be thrown.

== Developer Utilities
_Under Construction_

// - _KK Dev to provide initial content._
// - _Dave to review, edit, and solicit additional info as needed._

=== Basho Bench
_to be added_

=== Yahoo! Cloud Serving Benchmark
_to be added_


[[HibariBuildingSource]]
== Building Hibari from Source
_Under Construction_

// - _KK Dev to provide initial content._
// - _Dave to review, edit, and solicit additional info as needed._

This section describes the basic recipes to build the following items:

- Hibari Release Package
- Hibari Documentation
- Erlang/OTP System

Before getting started, review this checklist of tools and software.
Please install and setup as needed.

Git (Mandatory)::
- Git - http://git-scm.com/
  * *Git 1.5.4 or newer, Git 1.7.3.4 has been tested recently*
  * _required for Repo and GitHub_
- GitHub - https://github.com
  * Anonymous read-only access using the GIT protocol is default.
  * Team members having read-write access should add his/her ssh
    public key under your GitHub account.
Erlang/OTP (Mandatory)::
- OpenSSL - http://www.openssl.org/
  * _required for Erlang's crypto module_
- Erlang - http://www.erlang.org/
  * *R13B04 or newer*
  * *R14B01 has been tested most recently*
  * If needed, see <<ErlangOTP>> for instructions to build Erlang/OTP
    from source.
Python (Mandatory)::
- Python - http://www.python.org
  * *Python 2.4 or newer, Python 2.7 has been tested most recently
     (CAUTION: Python 3.x might be too new)*
  * _required for Repo and AsciiDoc_
AsciiDoc (Optional)::
- AsciiDoc - http://www.methods.co.nz/asciidoc/index.html
  * *asciidoc 8.6.1 and asciidoc 9.6.3 have been tested most recently*
  * plus the following tools:
    ** ImageMagick - http://www.imagemagick.org/
    ** graphviz - http://www.graphviz.org/
    ** mscgen - http://www.mcternan.me.uk/mscgen/
  * Mandatory for building Hibari's documentation. See
    <<HibariAsciiDoc>> for further details.
- docbook - http://www.docbook.org/
  * Optional for building _pdf_ version of Hibari's documentation
- xmlto - https://fedorahosted.org/xmlto/
  * Optional for building _text_ version of Hibari's documentation

In addition to the above list, Hibari also depends on two tools to
automate the downloading and the packaging steps.

- Repo - http://source.android.com/source/git-repo.html
- Rebar - https://github.com/basho/rebar/wiki

Instructions for downloading the Repo tool are described next.  The
Rebar tool is included in Hibari's git repositories so there is no
need to download it separately.  Please refer to the above sites for
further information regarding the usage of these tools.

The first step is to download the Git repositories from GitHub.

. Downloading _basic recipe_
  .. Configure your e-mail and name for Git
+
------
$ git config --global user.email "you@example.com"
$ git config --global user.name "Your Name"
------
+
  .. Install Repo
+
------
$ mkdir -p ~/bin
$ wget -O - http://android.git.kernel.org/repo > ~/bin/repo
$ chmod a+x ~/bin/repo
------
+
  .. Create working directory
+
------
$ mkdir working-directory-name
$ cd working-directory-name
$ repo init -u git://github.com/hibari/manifests.git -m hibari-default.xml
------
+
NOTE: Your "Git" identity is needed during the init step.  Please
enter the name and email of your GitHub account if you have one.  Team
members having read-write access are recommended to use "repo init -u
git@github.com:hibari/manifests.git -m hibari-default-rw.xml".
+
TIP: If you want to checkout the latest development version of Hibari,
please append " -b dev" to the repo init command.
+
  .. Download Git repositories
+
------
$ cd working-directory-name
$ repo sync
------
+

The working directory has the following layout structure:

-----
<working-directory-name>
  |- hibari/
    |- .git/
    |- .gitignore
    |- Makefile
    |- dialyze-ignore-warnings.txt
    |- dialyze-nospec-ignore-warnings.txt
    |- lib/                             <1>
      |- <application_name>/
        |- .git/
        |- .gitignore
        |- ebin/
        |- include/
          |- *.hrl
        |- priv/
        |- rebar.config
        |- src/
          |- <application_name>.app.src
          |- *.erl
        |- test/
          |- eunit/
            |- *.erl
          |- eqc/
            |- *.erl
      :
    |- rebar
    |- rebar.config
    |- rel/                             <2>
      |- files/
        |- app.config
        |- erl
        |- hibari
        |- hibari-admin
        |- nodetool
        |- nodetool-admin
        |- vm.args
      |- hibari/
        :
        |- releases/
          |- <release_vsn>/
            :
          :
        :
      |- reltool.config
  |- hibari-doc/                        <3>
    :
  |- manifests/                         <4>
    :
  |- patches/                           <5>
    :
  |- rebar/                             <6>
    :
  |- .repo/
    :
-----

<1> Applications
<2> Releases
<3> Documentation
<4> Manifests
<5> Patches
<6> Rebar

=== Hibari Release Package

This section is the first step to build, to _eunit_ test, and to
package your own Hibari.

. Building _basic recipe_
+
------
$ cd working-directory-name/hibari
$ make
------
+
TIP: If the response is "make: erl: Command not found", please make
sure Erlang/OTP is installed and "otp-installing-directory-name/bin"
is added to your $PATH environment.
+
. Release Packaging _basic recipe_
+
------
$ cd working-directory-name/hibari
$ make package
------
+
NOTE: A release package tarball "hibari-X.Y.Z-dev-ARCH-WORDSIZE.tgz"
and md5sum file "hibari-X.Y.Z-dev-ARCH-WORDSIZE-md5sum.txt" is written
to working-directory-name.

[[HibariAsciiDoc]]
=== Hibari Documentation

This section is the first step to download and to build your own
Hibari documentation.

. Building Hibari's "Guides" _basic recipe_
+
------
$ cd working-directory-name/hibari-doc/src/hibari
$ make clean -OR- make realclean
$ make
------
+
. Building Hibari's "Website" _basic recipe_
+
------
$ cd working-directory-name/hibari-doc/src/hibari/website
$ make clean -OR- make realclean
$ make
------
+
NOTE: HTML documentation is written in the "./public_html" directory.

Hibari's documentation is authored using AsciiDoc and a few auxillary
tools:

- ImageMagick
- docbook
- graphviz
- mscgen
- xmlto

Hibari's documentation is generated with asciidoc 8.6.3 and a manually
modified version of the a2x tool.

------
$ diff -u /usr/local/Cellar/asciidoc/8.6.3/bin/a2x{.orig,}
--- /usr/local/Cellar/asciidoc/8.6.3/bin/a2x.orig	2011-01-02 18:09:35.000000000 +0900
+++ /usr/local/Cellar/asciidoc/8.6.3/bin/a2x	2011-01-02 18:11:19.000000000 +0900
@@ -156,7 +156,10 @@
 def shell_copy(src, dst):
     verbose('copying "%s" to "%s"' % (src,dst))
     if not OPTIONS.dry_run:
-        shutil.copy(src, dst)
+        try:
+            shutil.copy(src, dst)
+        except shutil.Error:
+            return

 def shell_rm(path):
     if not os.path.exists(path):
------

[[ErlangOTP]]
=== Erlang/OTP System

This section is the first step to download, to build, and to install
your own Erlang/OTP system.

. Downloading _basic recipe_
  .. Please make sure to have the 'openssl-devel' package installed on
     your system.  OpenSSL is required by Hibari (and before
     configuring and building of your Erlang/OTP system).
  .. Get and install Git
+
  .. Download the source code for your Erlang/OTP system
+
------
$ cd working-directory-name
$ wget http://www.erlang.org/download/otp_src_R14B01.tar.gz
------
+
  .. Untar the source code for your Erlang/OTP system.
+
------
$ cd working-directory-name
$ tar -xzf otp_src_R14B01.tar.gz
------
+
. Building _basic recipe_
  .. Change to your working directory and configure Erlang/OTP
+
------
$ cd working-directory-name/otp_src_R14B01
$ ./configure --prefix=otp-installing-directory-name
------
+
  .. Build Erlang/OTP
+
------
$ cd working-directory-name/otp_src_R14B01
$ make
------
+
. Installing _basic recipe_
+
------
$ cd working-directory-name/otp_src_R14B01
$ sudo make install
------

CAUTION: Please make sure "otp-installing-directory-name/bin" is added
to your $PATH environment.


== Sample Application
_Under Construction_

// - _KK Dev to provide initial content._
// - _Likely deferred to future date._
